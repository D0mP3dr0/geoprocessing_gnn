# -*- coding: utf-8 -*-
"""PIPELINECOMPLETO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCRmvYplBLpqCQY30U343djicgFQDeOZ

<a href="https://colab.research.google.com/github/D0mP3dr0/geoprocessing_gnn/blob/main/geoprocessing_gnn.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

CRIACAO DO GRAFO INTRACAMADA
"""

# Verificar se o script est√° sendo executado no Colab ou localmente
import sys
import os

# Vari√°vel global para indicar o ambiente
IS_COLAB = 'google.colab' in sys.modules
IS_NOTEBOOK = False

# Configura√ß√£o para import de tqdm e display
try:
    from IPython.display import display, HTML
    IS_NOTEBOOK = True
    # Usar tqdm notebook version para barra de progresso quando estamos em um notebook
    from tqdm.notebook import tqdm
except ImportError:
    IS_NOTEBOOK = False
    # Fun√ß√£o alternativa para display quando n√£o estamos em um notebook
    def display(obj):
        if hasattr(obj, 'head') and callable(obj.head):
            print(obj.head())
        else:
            print(obj)
            
    HTML = lambda x: print(x)
    
    # Usar tqdm console version para barra de progresso quando em terminal
    from tqdm import tqdm

# Define a fun√ß√£o check_jupyter para verificar se estamos em ambiente Jupyter
def check_jupyter():
    try:
        from IPython import get_ipython
        if get_ipython() is None:
            return False
        return True
    except (ImportError, AttributeError):
        return False

# Configura√ß√£o dos caminhos
if IS_COLAB:
    # Se estiver no Colab, monte o Google Drive
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        
        # Caminhos de diret√≥rios no Google Drive
        BASE_DIR = "/content/drive/MyDrive/geoprocessamento_gnn"
    except ImportError:
        print("N√£o foi poss√≠vel montar o Google Drive. Verifique se o m√≥dulo google.colab est√° dispon√≠vel.")
        BASE_DIR = "."
else:
    # Se estiver executando localmente, usar caminhos locais
    # Voc√™ pode adaptar estes caminhos conforme necess√°rio
    print("Executando em ambiente local...")
    
    # Tentar obter o caminho do diret√≥rio de trabalho atual
    try:
        # Usar o caminho absoluto do arquivo atual como base
        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
        # Subir um n√≠vel para chegar no diret√≥rio raiz do projeto
        BASE_DIR = os.path.dirname(os.path.dirname(SCRIPT_DIR))
    except:
        # Fallback para o diret√≥rio atual
        BASE_DIR = os.getcwd()
    
    print(f"Diret√≥rio base: {BASE_DIR}")

# Definir os diret√≥rios de dados, sa√≠da e relat√≥rios
DATA_DIR = os.path.join(BASE_DIR, "DATA")  # Arquivos GPKG brutos
OUTPUT_DIR = os.path.join(BASE_DIR, "OUTPUT")  # Arquivos processados/editados
REPORT_DIR = os.path.join(BASE_DIR, "QUALITY_REPORT")  # Relat√≥rios de qualidade

# Criar os diret√≥rios se n√£o existirem
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

print(f"Diret√≥rio de dados brutos: {DATA_DIR}")
print(f"Diret√≥rio de sa√≠da: {OUTPUT_DIR}")
print(f"Diret√≥rio de relat√≥rios: {REPORT_DIR}")

"""üìì 1_Configuracao_Ambiente/"""

# Verificar o tipo de GPU dispon√≠vel
nvidia_smi_cmd = "nvidia-smi"  # Execute este comando no Colab

# Verificar a vers√£o do Python e ambiente atual
import sys
print(f"Vers√£o do Python: {sys.version}")
pip_version_cmd = "pip --version"  # Execute este comando no Colab

# Instala√ß√£o de bibliotecas essenciais para processamento geoespacial
# Execute estes comandos no Colab:
rasterio_install_cmd = "pip install rasterio"
geopandas_install_cmd = "pip install -U geopandas pyproj shapely fiona rtree pycrs mapclassify pyogrio"
osmnx_install_cmd = "pip install -U osmnx networkx folium keplergl"
raster_install_cmd = "pip install -U rasterstats xarray rioxarray netCDF4"
stats_install_cmd = "pip install -U scipy scikit-learn statsmodels plotly matplotlib seaborn"
jupyter_install_cmd = "pip install -U tqdm jupyterlab notebook ipywidgets"
geo_extras_install_cmd = "pip install -U pygeos contextily pysal momepy"
db_install_cmd = "pip install -U psycopg2-binary sqlalchemy geoalchemy2"

# Instala√ß√£o das bibliotecas RAPIDS para processamento acelerado por GPU
# O L4 √© compat√≠vel com RAPIDS, que acelera significativamente opera√ß√µes geoespaciais
rapids_install_cmd = "pip install -U cudf cuml cuspatial cupy"  # Execute este comando no Colab

# Bibliotecas espec√≠ficas para manipula√ß√£o de dados tabulares e leitura de arquivos
data_install_cmd = "pip install -U pandas numpy openpyxl xlrd xlwt xlsxwriter"
geo_formats_install_cmd = "pip install -U pyshp geojson topojson geobuf mapbox-vector-tile"
viz_install_cmd = "pip install -U geoviews holoviews datashader panel hvplot"
dist_install_cmd = "pip install -U dask distributed h5py hdf5 zarr"

# Verificar importa√ß√µes essenciais e configura√ß√µes b√°sicas
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import rasterio
import networkx as nx

print("Verifica√ß√£o de configura√ß√£o b√°sica conclu√≠da. Bibliotecas principais carregadas com sucesso.")

# Configurar ambiente para trabalhar com o L4 GPU
import os

# Definir vari√°veis de ambiente para otimiza√ß√£o de bibliotecas geoespaciais
os.environ['USE_PYGEOS'] = '0'  # Usar GeoPandas com GEOS (mais est√°vel)
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Utilizar GPU 0

# Configura√ß√µes para utilizar mem√≥ria de GPU de forma eficiente
os.environ['RAPIDS_NO_INITIALIZE'] = '1'  # Inicializa√ß√£o manual do RAPIDS

print("Vari√°veis de ambiente configuradas para otimiza√ß√£o com GPU L4")

# Configura√ß√£o para visualiza√ß√£o interativa
try:
    from IPython.display import display, HTML
    IS_NOTEBOOK = True
    # Usar tqdm notebook version
    from tqdm.notebook import tqdm
except ImportError:
    IS_NOTEBOOK = False
    # Fun√ß√£o alternativa para display quando n√£o estamos em um notebook
    def display(obj):
        if isinstance(obj, pd.DataFrame) or isinstance(obj, gpd.GeoDataFrame):
            print(obj.head())
        else:
            print(obj)
            
    HTML = lambda x: print(x)
    # Usar tqdm console version
    from tqdm import tqdm

# Aumentar limite de exibi√ß√£o para melhor visualiza√ß√£o de dados geoespaciais
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 100)
pd.set_option('display.width', 1000)

# Configurar matplotlib para visualiza√ß√µes de alta qualidade
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['figure.dpi'] = 100

print("Configura√ß√µes de visualiza√ß√£o otimizadas")

# Fun√ß√£o utilit√°ria para verificar consumo de mem√≥ria
def check_memory_usage():
    """Exibe o uso atual de mem√≥ria RAM e GPU."""
    print("Uso de mem√≥ria RAM:")
    free_h_cmd = "free -h"  # Execute este comando no Colab

    print("\nUso de mem√≥ria GPU:")
    gpu_memory_cmd = "nvidia-smi --query-gpu=memory.used,memory.total --format=csv"  # Execute este comando no Colab

check_memory_usage()

# Testar leitura b√°sica de dados geoespaciais
try:
    # Criar um GeoDataFrame simples para teste
    from shapely.geometry import Point

    # Criar alguns pontos de teste
    geometry = [Point(0, 0), Point(1, 1), Point(2, 2)]
    gdf = gpd.GeoDataFrame(geometry=geometry)

    # Plotar para verificar funcionamento
    gdf.plot()
    plt.title("Teste de Plotagem Geoespacial")
    plt.show()

    print("Teste de funcionalidade geoespacial conclu√≠do com sucesso!")
except Exception as e:
    print(f"Erro ao testar funcionalidade geoespacial: {e}")

# Resumo da configura√ß√£o do ambiente
print("="*80)
print("CONFIGURA√á√ÉO DO AMBIENTE CONCLU√çDA COM SUCESSO")
print("="*80)
print("\nBibliotecas instaladas e configuradas:")
print("- Processamento geoespacial: GeoPandas, Rasterio, Shapely, Fiona")
print("- An√°lise de redes: NetworkX, OSMnx")
print("- Processamento de dados: Pandas, NumPy, SciPy, Scikit-learn")
print("- Visualiza√ß√£o: Matplotlib, Folium, Plotly, Kepler.gl")
print("- Acelera√ß√£o GPU: RAPIDS (cuDF, cuSpatial)")
print("\nPr√≥ximos passos:")
print("1. Execute 1.2_Carregamento_Datasets.ipynb para carregar os dados")
print("2. Execute 1.3_Configuracao_Sistema_Coordenadas.ipynb para configurar os sistemas de refer√™ncia")
print("="*80)

"""1.2_Carregamento_Datasets.ipynb"""

# Carregamento de Datasets para Integra√ß√£o Geoespacial
# Este notebook carrega os datasets GPKG necess√°rios para o projeto

import os
import glob
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import json
from shapely.geometry import box

# Montar o Google Drive apenas se estiver no Colab
if IS_COLAB:
    try:
        from google.colab import drive
        drive.mount('/content/drive')
    except ImportError:
        print("N√£o foi poss√≠vel montar o Google Drive.")

# Defini√ß√£o do diret√≥rio onde est√£o os datasets
data_dir = DATA_DIR  # Usando a vari√°vel definida anteriormente

# Lista de arquivos GPKG dispon√≠veis
gpkg_files = glob.glob(os.path.join(data_dir, '**', '*.gpkg'), recursive=True)

print(f"Diret√≥rio de dados encontrado: {data_dir}")
print(f"Encontrados {len(gpkg_files)} arquivos GPKG:")

# Listar arquivos encontrados
if gpkg_files:
    for i, file_path in enumerate(gpkg_files, 1):
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Tamanho em MB
        print(f"{i}. {file_name} ({file_size:.2f} MB)")
else:
    print("Nenhum arquivo GPKG encontrado. Verifique o caminho ou extens√£o dos arquivos.")

def explore_gpkg(gpkg_path):
    """
    Explora um arquivo GeoPackage para obter informa√ß√µes sobre suas camadas.
    
    Args:
        gpkg_path: Caminho para o arquivo GeoPackage
        
    Returns:
        DataFrame com informa√ß√µes sobre as camadas
    """
    # Listar todas as camadas no arquivo
    layers = fiona.listlayers(gpkg_path)

    # Coletar informa√ß√µes sobre cada camada
    layer_info = []
    for layer in layers:
        try:
            # Abrir a camada para obter informa√ß√µes
            with fiona.open(gpkg_path, layer=layer) as src:
                # Obter a contagem de fei√ß√µes
                count = len(src)
                # Obter o tipo de geometria
                if count > 0:
                    geometry_type = src.schema['geometry']
                else:
                    geometry_type = "Desconhecido"
                # Obter o CRS
                crs = src.crs
                # Obter os campos de atributos
                fields = list(src.schema['properties'].keys())

                layer_info.append({
                    'Layer': layer,
                    'Feature Count': count,
                    'Geometry Type': geometry_type,
                    'CRS': crs,
                    'Fields': fields[:5] + ['...'] if len(fields) > 5 else fields
                })
        except Exception as e:
            print(f"Erro ao processar camada {layer}: {e}")
            layer_info.append({
                'Layer': layer,
                'Error': str(e)
            })

    return pd.DataFrame(layer_info)

# Importamos fiona aqui para listar as camadas
import fiona

# Explorando cada arquivo GPKG
for gpkg_file in gpkg_files:
    file_name = os.path.basename(gpkg_file)
    print(f"\n{'='*80}\nExplorando arquivo: {file_name}\n{'='*80}")

    layers_info = explore_gpkg(gpkg_file)
    display(layers_info)

# Fun√ß√£o para carregar e armazenar todos os datasets em um dicion√°rio
def load_all_datasets(gpkg_files):
    """
    Carrega todos os arquivos GPKG e suas camadas em um dicion√°rio.

    Args:
        gpkg_files: Lista de caminhos para arquivos GPKG

    Returns:
        Um dicion√°rio de GeoDataFrames organizados por nome de arquivo e camada
    """
    datasets = {}

    for gpkg_file in tqdm(gpkg_files, desc="Carregando arquivos GPKG"):
        file_name = os.path.basename(gpkg_file)
        datasets[file_name] = {}

        # Listar camadas
        layers = fiona.listlayers(gpkg_file)

        for layer in tqdm(layers, desc=f"Camadas em {file_name}", leave=False):
            try:
                # Carregar o GeoDataFrame
                gdf = gpd.read_file(gpkg_file, layer=layer)

                # Armazenar no dicion√°rio
                datasets[file_name][layer] = gdf

                print(f"Carregado: {file_name} - {layer} - {len(gdf)} registros")
            except Exception as e:
                print(f"Erro ao carregar camada {layer} de {file_name}: {e}")

    return datasets

# Carregando todos os datasets
datasets = load_all_datasets(gpkg_files)

# Salvando em uma vari√°vel global para acesso posterior
import builtins
builtins.datasets = datasets

print("\nCarregamento completo. Todos os datasets est√£o armazenados na vari√°vel global 'datasets'.")

# Habilitar widgets apenas se estiver no Colab
if IS_COLAB:
    try:
        from google.colab import output
        output.enable_custom_widget_manager()
        print("Support for third party widgets will remain active for the duration of the session. To disable support:")
        print("from google.colab import output")
        print("output.disable_custom_widget_manager()")
    except ImportError:
        print("Widgets do Colab n√£o dispon√≠veis.")

"""Support for third party widgets will remain active for the duration of the session. To disable support:"""

# Desabilitar widgets apenas se estiver no Colab
if IS_COLAB:
    try:
        from google.colab import output
        output.disable_custom_widget_manager()
    except ImportError:
        pass

# Fun√ß√£o para visualizar a extens√£o geogr√°fica de todos os datasets em um mapa
def plot_dataset_extents(datasets):
    """
    Visualiza a extens√£o geogr√°fica de todos os datasets carregados.

    Args:
        datasets: Dicion√°rio de datasets carregados
    """
    # Criar um gr√°fico vazio
    fig, ax = plt.subplots(1, 1, figsize=(15, 15))

    # Cores para diferentes datasets
    colors = plt.cm.tab20.colors
    color_idx = 0

    # Legenda
    legend_items = []

    # Iterar atrav√©s dos datasets
    for file_name, layers in datasets.items():
        for layer_name, gdf in layers.items():
            # Verificar se o GeoDataFrame tem geometria
            if gdf.geometry.is_empty.all():
                print(f"Geometria vazia em {file_name} - {layer_name}")
                continue

            try:
                # Obter a extens√£o do dataset
                minx, miny, maxx, maxy = gdf.total_bounds
                extent_box = box(minx, miny, maxx, maxy)

                # Criar um GeoDataFrame com a extens√£o
                extent_gdf = gpd.GeoDataFrame(geometry=[extent_box], crs=gdf.crs)

                # Plotar no mapa
                color = colors[color_idx % len(colors)]
                extent_gdf.boundary.plot(ax=ax, color=color, linewidth=2,
                                        label=f"{file_name} - {layer_name}")

                # Adicionar √† legenda
                legend_items.append(f"{file_name} - {layer_name}")

                # Incrementar √≠ndice de cor
                color_idx += 1
            except Exception as e:
                print(f"Erro ao plotar extens√£o de {file_name} - {layer_name}: {e}")

    # Configurar o gr√°fico
    ax.set_title("Extens√£o geogr√°fica dos datasets carregados", fontsize=16)
    ax.legend(legend_items, fontsize=10, loc='center left', bbox_to_anchor=(1, 0.5))
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    ax.grid(True)

    plt.tight_layout()
    plt.show()

# Visualizando a extens√£o dos datasets
plot_dataset_extents(datasets)

# Salvando o estado dos datasets para uso no pr√≥ximo notebook
import pickle

# Criando pasta de estado se n√£o existir
state_dir = os.path.join(DATA_DIR, 'state')
os.makedirs(state_dir, exist_ok=True)

# Caminho para o arquivo de estado
state_file = os.path.join(state_dir, 'datasets_state.pkl')

# Salvando o dicion√°rio de datasets
with open(state_file, 'wb') as f:
    pickle.dump(datasets, f)

print(f"Estado dos datasets salvo em: {state_file}")
print("Os datasets est√£o prontos para o pr√≥ximo notebook de configura√ß√£o do sistema de coordenadas.")

# Resumo do carregamento de dados
print("\n" + "="*80)
print("RESUMO DE CARREGAMENTO DE DADOS")
print("="*80)

# Contabilizar totais
total_layers = 0
total_features = 0

# Exibir estat√≠sticas por arquivo
for file_name, layers in datasets.items():
    layer_count = len(layers)
    total_layers += layer_count

    feature_counts = [len(gdf) for gdf in layers.values()]
    total_file_features = sum(feature_counts)
    total_features += total_file_features

    print(f"\nArquivo: {file_name}")
    print(f"  N√∫mero de camadas: {layer_count}")
    print(f"  Total de fei√ß√µes: {total_file_features:,}")

    # Listar cada camada com contagem
    for layer_name, gdf in layers.items():
        print(f"    - {layer_name}: {len(gdf):,} fei√ß√µes")

print("\n" + "="*80)
print(f"TOTAL GERAL: {total_layers} camadas, {total_features:,} fei√ß√µes")
print("="*80)

"""1.3_Configuracao_Sistema_Coordenadas.ipynb"""

# Configura√ß√£o do Sistema de Coordenadas para Integra√ß√£o Geoespacial
# Este notebook padroniza todos os datasets para o sistema SIRGAS 2000 23S (EPSG:31983)

import os
import pickle
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from shapely.geometry import box
import warnings

# Montar o Google Drive apenas se estiver no Colab
if IS_COLAB:
    try:
        from google.colab import drive
        drive.mount('/content/drive')
    except ImportError:
        print("N√£o foi poss√≠vel montar o Google Drive.")

# Diret√≥rio de dados
data_dir = DATA_DIR  # Usando a vari√°vel definida anteriormente
state_dir = os.path.join(DATA_DIR, 'state')
# Diret√≥rio de dados
data_dir = DATA_DIR  # Usando a vari√°vel definida anteriormente
state_dir = os.path.join(DATA_DIR, 'state')
state_file = os.path.join(state_dir, 'datasets_state.pkl')

# Verificando se o arquivo de estado existe
if not os.path.exists(state_file):
    raise FileNotFoundError(f"Arquivo de estado n√£o encontrado: {state_file}")

# Carregando o estado dos datasets do notebook anterior
with open(state_file, 'rb') as f:
    datasets = pickle.load(f)

# Verificando se o arquivo de estado existe
if not os.path.exists(state_file):
    raise FileNotFoundError(f"Arquivo de estado n√£o encontrado: {state_file}")

# Carregando o estado dos datasets do notebook anterior
with open(state_file, 'rb') as f:
    datasets = pickle.load(f)

# Definindo globalmente para uso posterior
import builtins
builtins.datasets = datasets

print(f"Datasets carregados com sucesso do arquivo: {state_file}")

# Defini√ß√£o do sistema de coordenadas alvo: SIRGAS 2000 / UTM zone 23S (EPSG:31983)
TARGET_CRS = "EPSG:31983"

# Informa√ß√µes sobre o sistema de coordenadas alvo
sirgas_info = {
    'EPSG': 31983,
    'Nome': 'SIRGAS 2000 / UTM zone 23S',
    'Proje√ß√£o': 'Transverse Mercator',
    'Datum': 'SIRGAS 2000',
    'Unidade': 'metros',
    'Zona UTM': '23S',
    'Meridiano Central': '-45',
    'Latitude de Origem': '0',
    'Falso Leste': '500000',
    'Falso Norte': '10000000',
    'Fator de Escala': '0.9996'
}

print("Sistema de coordenadas alvo:")
for key, value in sirgas_info.items():
    print(f"  {key}: {value}")

print("\nEste sistema de coordenadas √© ideal para o nosso projeto porque:")
print("1. Mant√©m a precis√£o das medi√ß√µes em metros para a √°rea de estudo")
print("2. √â o sistema padr√£o para projetos geoespaciais no Brasil, especialmente na zona 23S")
print("3. Permite calcular √°reas, dist√¢ncias e an√°lises espaciais com precis√£o")
print("4. Minimiza distor√ß√µes para a regi√£o de estudo")

# Fun√ß√£o para verificar e tabular os sistemas de coordenadas atuais de todos os datasets
def check_crs(datasets):
    """
    Verifica os sistemas de coordenadas de todos os datasets.

    Args:
        datasets: Dicion√°rio de datasets carregados

    Returns:
        DataFrame com informa√ß√µes sobre os sistemas de coordenadas
    """
    crs_info = []

    for file_name, layers in datasets.items():
        for layer_name, gdf in layers.items():
            try:
                # Obter informa√ß√µes do CRS
                if gdf.crs is None:
                    crs_code = None
                    crs_name = "N√£o definido"
                    is_projected = False
                    units = "Desconhecido"
                else:
                    crs_code = gdf.crs.to_epsg()
                    crs_name = gdf.crs.name if hasattr(gdf.crs, 'name') else str(gdf.crs)
                    is_projected = gdf.crs.is_projected
                    units = gdf.crs.axis_info[0].unit_name if hasattr(gdf.crs, 'axis_info') else "Desconhecido"

                needs_transformation = crs_code != 31983 and crs_code is not None

                crs_info.append({
                    'Arquivo': file_name,
                    'Camada': layer_name,
                    'EPSG': crs_code,
                    'CRS Nome': crs_name,
                    'Projetado': is_projected,
                    'Unidades': units,
                    'Requer Transforma√ß√£o': needs_transformation
                })
            except Exception as e:
                print(f"Erro ao verificar CRS de {file_name} - {layer_name}: {e}")
                crs_info.append({
                    'Arquivo': file_name,
                    'Camada': layer_name,
                    'EPSG': None,
                    'CRS Nome': f"ERRO: {str(e)}",
                    'Projetado': None,
                    'Unidades': None,
                    'Requer Transforma√ß√£o': None
                })

    return pd.DataFrame(crs_info)

# Analisando os sistemas de coordenadas atuais
crs_status = check_crs(datasets)
display(crs_status)

# Contabilizando quantos datasets precisam de transforma√ß√£o
requires_transformation = crs_status['Requer Transforma√ß√£o'].sum()
total_layers = len(crs_status)

print(f"\nStatus de transforma√ß√£o: {requires_transformation} de {total_layers} camadas precisam ser transformadas para SIRGAS 2000 / UTM zone 23S (EPSG:31983)")

# Fun√ß√£o para transformar datasets para o CRS alvo
def transform_datasets(datasets, target_crs=TARGET_CRS):
    """
    Transforma todos os datasets para o sistema de coordenadas alvo.

    Args:
        datasets: Dicion√°rio de datasets carregados
        target_crs: Sistema de coordenadas alvo

    Returns:
        Dicion√°rio de datasets transformados
    """
    transformed_datasets = {}
    transformation_report = []

    # Para cada arquivo
    for file_name, layers in tqdm(datasets.items(), desc="Transformando arquivos"):
        transformed_datasets[file_name] = {}

        # Para cada camada
        for layer_name, gdf in tqdm(layers.items(), desc=f"Camadas em {file_name}", leave=False):
            try:
                # Verificar se o CRS atual √© None
                if gdf.crs is None:
                    print(f"AVISO: {file_name} - {layer_name} n√£o possui CRS definido. Atribuindo o CRS alvo sem transforma√ß√£o.")
                    gdf.crs = target_crs
                    transformed_gdf = gdf
                    status = "Atribu√≠do CRS (sem transforma√ß√£o)"

                # Verificar se j√° est√° no CRS alvo
                elif gdf.crs.to_epsg() == 31983:
                    print(f"{file_name} - {layer_name} j√° est√° no CRS alvo. Nenhuma transforma√ß√£o necess√°ria.")
                    transformed_gdf = gdf
                    status = "J√° no CRS alvo"

                # Realizar a transforma√ß√£o
                else:
                    # Registrar informa√ß√µes antes da transforma√ß√£o
                    if 'geometry' in gdf.columns and len(gdf) > 0 and not gdf.geometry.is_empty.all():
                        pre_bounds = gdf.total_bounds
                    else:
                        pre_bounds = None

                    # Executar a transforma√ß√£o
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore", category=UserWarning)
                        transformed_gdf = gdf.to_crs(target_crs)

                    # Registrar informa√ß√µes ap√≥s a transforma√ß√£o
                    if pre_bounds is not None and len(transformed_gdf) > 0 and not transformed_gdf.geometry.is_empty.all():
                        post_bounds = transformed_gdf.total_bounds
                        status = "Transformado com sucesso"
                    else:
                        post_bounds = None
                        status = "Transformado (sem geometria para validar)"

                # Armazenar o GeoDataFrame transformado
                transformed_datasets[file_name][layer_name] = transformed_gdf

                # Registrar no relat√≥rio
                transformation_report.append({
                    'Arquivo': file_name,
                    'Camada': layer_name,
                    'CRS Original': str(gdf.crs),
                    'CRS Final': str(transformed_gdf.crs),
                    'Status': status,
                    'Registros': len(gdf)
                })

            except Exception as e:
                print(f"ERRO ao transformar {file_name} - {layer_name}: {e}")
                # Manter o dataset original em caso de erro
                transformed_datasets[file_name][layer_name] = gdf
                transformation_report.append({
                    'Arquivo': file_name,
                    'Camada': layer_name,
                    'CRS Original': str(gdf.crs) if hasattr(gdf, 'crs') else "Desconhecido",
                    'CRS Final': "ERRO",
                    'Status': f"Erro: {str(e)}",
                    'Registros': len(gdf) if hasattr(gdf, 'len') else "Desconhecido"
                })

    return transformed_datasets, pd.DataFrame(transformation_report)

# Executando a transforma√ß√£o
transformed_datasets, transformation_report = transform_datasets(datasets)

# Atualizando a vari√°vel global com os datasets transformados
builtins.datasets = transformed_datasets

# Exibindo o relat√≥rio de transforma√ß√£o
display(transformation_report)

# Valida√ß√£o final: verificando se todos os datasets est√£o no sistema de coordenadas correto
def validate_transformations(datasets, target_crs=TARGET_CRS):
    """
    Verifica se todos os datasets est√£o no sistema de coordenadas alvo.

    Args:
        datasets: Dicion√°rio de datasets transformados
        target_crs: Sistema de coordenadas alvo

    Returns:
        DataFrame com resultados da valida√ß√£o
    """
    validation_results = []
    target_epsg = 31983  # EPSG para SIRGAS 2000 / UTM zone 23S

    for file_name, layers in datasets.items():
        for layer_name, gdf in layers.items():
            try:
                current_epsg = gdf.crs.to_epsg()
                is_valid = current_epsg == target_epsg

                validation_results.append({
                    'Arquivo': file_name,
                    'Camada': layer_name,
                    'EPSG Atual': current_epsg,
                    'EPSG Alvo': target_epsg,
                    'Valida√ß√£o': "Sucesso" if is_valid else "Falha",
                    'CRS': str(gdf.crs)
                })
            except Exception as e:
                validation_results.append({
                    'Arquivo': file_name,
                    'Camada': layer_name,
                    'EPSG Atual': None,
                    'EPSG Alvo': target_epsg,
                    'Valida√ß√£o': f"Erro: {str(e)}",
                    'CRS': str(gdf.crs) if hasattr(gdf, 'crs') else "Desconhecido"
                })

    return pd.DataFrame(validation_results)

# Validando todas as transforma√ß√µes
validation_results = validate_transformations(transformed_datasets)
display(validation_results)

# Verificando sucesso geral
success_count = (validation_results['Valida√ß√£o'] == "Sucesso").sum()
total_count = len(validation_results)
success_rate = success_count / total_count * 100

print(f"\nValida√ß√£o conclu√≠da: {success_count} de {total_count} camadas ({success_rate:.2f}%) est√£o no sistema de coordenadas SIRGAS 2000 / UTM zone 23S (EPSG:31983)")

if success_count < total_count:
    print("\nATEN√á√ÉO: Algumas camadas n√£o foram transformadas corretamente. Verifique o relat√≥rio acima.")
else:
    print("\nTodos os datasets foram transformados com sucesso para o sistema de coordenadas alvo!")

# Salvando os datasets transformados
import pickle

# Criando pasta de estado se n√£o existir
state_dir = os.path.join(DATA_DIR, 'state')
os.makedirs(state_dir, exist_ok=True)

# Caminho para o arquivo de estado
transformed_state_file = os.path.join(state_dir, 'datasets_transformed_state.pkl')

# Salvando o dicion√°rio de datasets transformados
with open(transformed_state_file, 'wb') as f:
    pickle.dump(transformed_datasets, f)

print(f"Estado dos datasets transformados salvo em: {transformed_state_file}")

# Visualizando algumas fei√ß√µes para verificar visualmente a transforma√ß√£o
def plot_sample_geometries(datasets):
    """
    Plota amostras de geometrias para verifica√ß√£o visual.

    Args:
        datasets: Dicion√°rio de datasets transformados
    """
    # Criar um layout de 2x2 para visualiza√ß√£o
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()

    # Contador para controlar quantas camadas plotar
    plot_count = 0
    max_plots = 4

    # Cores para diferentes datasets
    colors = plt.cm.tab10.colors

    # Para cada arquivo e camada
    for file_name, layers in datasets.items():
        for layer_name, gdf in layers.items():
            if plot_count >= max_plots:
                break

            try:
                # Verificar se h√° geometrias para plotar
                if 'geometry' not in gdf.columns or len(gdf) == 0 or gdf.geometry.is_empty.all():
                    continue

                # Plotar apenas uma amostra (m√°ximo 100 fei√ß√µes)
                sample_size = min(100, len(gdf))
                ax = axes[plot_count]

                # Plotar a amostra
                gdf.sample(sample_size).plot(
                    ax=ax,
                    color=colors[plot_count % len(colors)],
                    alpha=0.7,
                    edgecolor='black',
                    linewidth=0.5
                )

                # Configurar o t√≠tulo e r√≥tulos
                ax.set_title(f"{file_name}\n{layer_name}", fontsize=12)
                ax.set_xlabel("Coordenada X (metros)")
                ax.set_ylabel("Coordenada Y (metros)")
                ax.grid(True)

                # Adicionar informa√ß√µes de CRS
                ax.text(0.5, -0.1, f"CRS: {gdf.crs.name if hasattr(gdf.crs, 'name') else str(gdf.crs)[:50]}...",
                       horizontalalignment='center', verticalalignment='center',
                       transform=ax.transAxes, fontsize=10)

                plot_count += 1

            except Exception as e:
                print(f"Erro ao plotar {file_name} - {layer_name}: {e}")
                continue

    if plot_count == 0:
        print("Nenhuma geometria dispon√≠vel para visualiza√ß√£o.")
        plt.close(fig)
        return

    # Ocultar eixos n√£o utilizados
    for i in range(plot_count, max_plots):
        axes[i].set_visible(False)

    plt.tight_layout()
    plt.show()

# Visualizando amostras das geometrias transformadas
plot_sample_geometries(transformed_datasets)

# Resumo e conclus√£o
print("="*80)
print("RESUMO DA CONFIGURA√á√ÉO DE SISTEMA DE COORDENADAS")
print("="*80)
print(f"\nSistema de coordenadas alvo: SIRGAS 2000 / UTM zone 23S (EPSG:31983)")
print(f"Total de arquivos processados: {len(transformed_datasets)}")
print(f"Total de camadas processadas: {total_count}")
print(f"Camadas transformadas com sucesso: {success_count} ({success_rate:.2f}%)")

print("\nBenef√≠cios da padroniza√ß√£o:")
print("1. Todas as an√°lises espaciais agora usar√£o o mesmo sistema de refer√™ncia")
print("2. C√°lculos de √°rea, dist√¢ncia e proximidade ser√£o precisos e consistentes")
print("3. Visualiza√ß√µes de mapas estar√£o corretamente alinhadas")
print("4. Opera√ß√µes de sobreposi√ß√£o espacial funcionar√£o corretamente")

print("\nPr√≥ximos passos:")
print("1. Prosseguir para a integra√ß√£o de Edif√≠cios e Uso do Solo")
print("2. Criar an√°lises demogr√°ficas usando os datasets transformados")
print("3. Realizar as opera√ß√µes hidrogr√°ficas com os dados padronizados")

print("\nOs datasets transformados est√£o dispon√≠veis na vari√°vel global 'datasets'")
print("="*80)


# Coment√°rio sobre a estrutura de notebooks de integra√ß√£o de edif√≠cios e uso do solo
"""
Descri√ß√£o dos notebooks de integra√ß√£o:
- Sobreposicao_Espacial.ipynb: Realiza an√°lise de sobreposi√ß√£o entre edif√≠cios e uso do solo
- Categorizacao_Funcional_Edificios.ipynb: Categoriza os edif√≠cios por fun√ß√£o
- Analise_Conformidade_Uso.ipynb: Analisa a conformidade dos edif√≠cios com o uso do solo
"""

"""2.1_Sobreposicao_Espacial.ipynb"""

# Sobreposi√ß√£o Espacial: Edif√≠cios x Uso do Solo
# Este notebook realiza a sobreposi√ß√£o espacial entre as camadas de edif√≠cios e uso do solo

import os
import shutil

# Criar o diret√≥rio de destino se n√£o existir
os.makedirs('/content/drive/MyDrive/geoprocessamento_gnn/OUTPUT/intermediate_results/', exist_ok=True)

# Copiar o arquivo da origem para o destino
shutil.copy(
    '/content/drive/MyDrive/geoprocessamento_gnn/DATA/state/datasets_transformed_state.pkl',
    '/content/drive/MyDrive/geoprocessamento_gnn/OUTPUT/intermediate_results/buildings_with_landuse.pkl'
)

print("Arquivo copiado com sucesso!")



import os
import pickle
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import numpy as np
import contextily as cx
from shapely.geometry import box
import warnings

# Ignorar avisos espec√≠ficos para opera√ß√µes espaciais
warnings.filterwarnings('ignore', category=UserWarning, message='.*CRS mismatch.*')

# Montando o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Defini√ß√£o dos diret√≥rios
data_dir = DATA_DIR  # Usando a vari√°vel definida anteriormente
results_dir = os.path.join(OUTPUT_DIR, 'intermediate_results')
os.makedirs(results_dir, exist_ok=True)
buildings_landuse_pkl = os.path.join(results_dir, 'buildings_with_landuse.pkl')

# Carregando os datasets transformados
with open(buildings_landuse_pkl, 'rb') as f:
    buildings_with_landuse = pickle.load(f)

print(f"Dados carregados de {buildings_landuse_pkl}")

# Verificar o tipo de dado carregado
if isinstance(buildings_with_landuse, dict):
    print(f"Dados carregados como dicion√°rio com {len(buildings_with_landuse)} chaves")
    print(f"Chaves dispon√≠veis: {list(buildings_with_landuse.keys())}")
    
    # Se for um dicion√°rio, extrair o GeoDataFrame de edif√≠cios
    found_gdf = False
    
    # Primeiro, procurar por uma chave espec√≠fica de edif√≠cio
    building_keys = ['buildings', 'edificios', 'edifica√ß√µes', 'building']
    for key in building_keys:
        if key in buildings_with_landuse:
            buildings_gdf = buildings_with_landuse[key]
            print(f"Usando a camada '{key}' com {len(buildings_gdf)} fei√ß√µes")
            found_gdf = True
            break
    
    # Se n√£o encontrou, verificar cada valor para encontrar um GeoDataFrame
    if not found_gdf:
        for key, value in buildings_with_landuse.items():
            if isinstance(value, gpd.GeoDataFrame):
                # Verificar se parece um GeoDataFrame de edif√≠cios
                cols = value.columns.tolist()
                building_related = any('build' in col.lower() for col in cols)
                
                if building_related or 'building' in str(key).lower():
                    buildings_gdf = value
                    print(f"Usando o GeoDataFrame '{key}' com {len(buildings_gdf)} fei√ß√µes")
                    found_gdf = True
                    break
        
        # Se ainda n√£o encontrou, usar o primeiro GeoDataFrame dispon√≠vel
        if not found_gdf:
            for key, value in buildings_with_landuse.items():
                if isinstance(value, gpd.GeoDataFrame) and not value.empty:
                    buildings_gdf = value
                    print(f"Usando o primeiro GeoDataFrame dispon√≠vel '{key}' com {len(buildings_gdf)} fei√ß√µes")
                    found_gdf = True
                    break
    
    if not found_gdf:
        print("N√£o foi poss√≠vel encontrar dados de edif√≠cios no dicion√°rio")
        # Criar um GeoDataFrame vazio com as colunas esperadas
        buildings_gdf = gpd.GeoDataFrame(columns=['geometry', 'building', 'categoria_funcional', 'subcategoria_funcional'])
        
elif isinstance(buildings_with_landuse, gpd.GeoDataFrame):
    print(f"Dados carregados como GeoDataFrame com {len(buildings_with_landuse)} fei√ß√µes")
    buildings_gdf = buildings_with_landuse
else:
    print(f"Formato de dados desconhecido: {type(buildings_with_landuse)}")
    # Criar um GeoDataFrame vazio com as colunas esperadas
    buildings_gdf = gpd.GeoDataFrame(columns=['geometry', 'building', 'categoria_funcional', 'subcategoria_funcional'])

# Criar dados de demonstra√ß√£o para testes
create_demo_data = False  # Altere para True se quiser usar dados de demonstra√ß√£o
if create_demo_data and (buildings_gdf.empty or 'geometry' not in buildings_gdf.columns):
    print("\nCriando dados de demonstra√ß√£o para testes...")
    from shapely.geometry import Point
    
    # Criar alguns pontos de exemplo
    geometries = [Point(x, y) for x, y in [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]]
    
    # Criar dados de exemplo
    demo_data = {
        'geometry': geometries,
        'building': ['residential', 'commercial', 'industrial', 'school', 'hospital'],
        'land_category': ['residential', 'commercial', 'industrial', 'institutional', 'healthcare']
    }
    
    # Criar GeoDataFrame
    demo_gdf = gpd.GeoDataFrame(demo_data, crs="EPSG:4326")
    
    # Usar dados de demonstra√ß√£o
    buildings_gdf = demo_gdf
    print("Usando dados de demonstra√ß√£o para permitir o funcionamento do pipeline.")
    print(f"Dados de demonstra√ß√£o criados com {len(buildings_gdf)} registros.")
    
    # Mostrar dados de demonstra√ß√£o
    print("\nDados de demonstra√ß√£o:")
    display(buildings_gdf)

# Explorar os dados dispon√≠veis para a categoriza√ß√£o
def explore_building_attributes(gdf):
    """
    Explora os atributos dispon√≠veis para categoriza√ß√£o dos edif√≠cios.

    Args:
        gdf: GeoDataFrame com os edif√≠cios e informa√ß√µes de uso do solo
    """
    # Verificar se o GeoDataFrame est√° vazio
    if gdf.empty:
        print("GeoDataFrame est√° vazio. N√£o h√° dados para explorar.")
        return
        
    # Verificar as colunas dispon√≠veis
    building_cols = [col for col in gdf.columns if 'build' in col.lower()]
    landuse_cols = [col for col in gdf.columns if 'land' in col.lower() or 'uso' in col.lower()]
    amenity_cols = [col for col in gdf.columns if 'amen' in col.lower()]
    function_cols = [col for col in gdf.columns if 'func' in col.lower() or 'class' in col.lower()]

    print("Colunas dispon√≠veis para categoriza√ß√£o:")
    print(f"- Colunas de edif√≠cio: {building_cols}")
    print(f"- Colunas de uso do solo: {landuse_cols}")
    print(f"- Colunas de amenidades: {amenity_cols}")
    print(f"- Colunas de fun√ß√£o/classe: {function_cols}")

    # Explorar valores √∫nicos em colunas principais
    for cols in [building_cols, landuse_cols, amenity_cols, function_cols]:
        for col in cols:
            if col in gdf.columns:
                values = gdf[col].dropna().unique()
                print(f"\nValores √∫nicos em '{col}' ({len(values)} valores):")
                if len(values) > 20:
                    print(values[:20], "... (mais valores)")
                else:
                    print(values)

# Explorando os atributos dispon√≠veis
if isinstance(buildings_gdf, gpd.GeoDataFrame) and not buildings_gdf.empty:
    explore_building_attributes(buildings_gdf)
else:
    print("N√£o h√° dados de edif√≠cios para explorar")

# Definir a hierarquia de categoriza√ß√£o funcional
def define_functional_categories():
    """
    Define a hierarquia e regras para categoriza√ß√£o funcional dos edif√≠cios.

    Returns:
        Dicion√°rio com defini√ß√µes de categorias funcionais e mapeamento de valores
    """
    # Defini√ß√£o hier√°rquica das categorias funcionais
    categories = {
        'residencial': {
            'description': 'Edif√≠cios residenciais unifamiliares e multifamiliares',
            'subcategories': {
                'unifamiliar': ['house', 'detached', 'bungalow', 'semidetached_house'],
                'multifamiliar': ['apartments', 'residential', 'terrace'],
                'misto': ['residential;commercial', 'mixed']
            },
            'landuse_match': ['residential', 'habitacional']
        },
        'comercial': {
            'description': 'Edif√≠cios comerciais e de servi√ßos',
            'subcategories': {
                'varejo': ['retail', 'shop', 'store', 'supermarket', 'mall'],
                'escritorios': ['commercial', 'office', 'offices', 'company'],
                'hospedagem': ['hotel', 'hostel', 'motel', 'guest_house'],
                'restauracao': ['restaurant', 'cafe', 'fast_food', 'food_court']
            },
            'landuse_match': ['commercial', 'retail', 'commercial']
        },
        'industrial': {
            'description': 'Edif√≠cios industriais e de armazenamento',
            'subcategories': {
                'fabrica': ['industrial', 'factory', 'manufacturing'],
                'armazenamento': ['warehouse', 'storage', 'depot', 'shed'],
                'logistica': ['logistics', 'distribution']
            },
            'landuse_match': ['industrial', 'industrial', 'manufacturing']
        },
        'institucional': {
            'description': 'Edif√≠cios institucionais e de servi√ßos p√∫blicos',
            'subcategories': {
                'educacao': ['school', 'university', 'college', 'kindergarten', 'educational'],
                'saude': ['hospital', 'clinic', 'healthcare', 'doctors', 'healthcare'],
                'governo': ['government', 'public', 'townhall', 'civic'],
                'religioso': ['church', 'mosque', 'temple', 'religious', 'place_of_worship'],
                'cultural': ['theatre', 'cinema', 'library', 'museum', 'arts_centre']
            },
            'landuse_match': ['institutional', 'education', 'healthcare', 'religious']
        },
        'infraestrutura': {
            'description': 'Infraestrutura e servi√ßos urbanos',
            'subcategories': {
                'transporte': ['transportation', 'train_station', 'bus_station', 'terminal'],
                'utilidades': ['utility', 'water_works', 'power', 'substation'],
                'seguranca': ['police', 'fire_station', 'military']
            },
            'landuse_match': ['transportation', 'infrastructure']
        }
    }

    # Mapeamentos espec√≠ficos de valores para otimizar a categoriza√ß√£o
    value_mappings = {
        # Mapeamento de valores de building para categorias
        'building_to_category': {
            'house': 'residencial.unifamiliar',
            'residential': 'residencial.multifamiliar',
            'apartments': 'residencial.multifamiliar',
            'commercial': 'comercial.escritorios',
            'retail': 'comercial.varejo',
            'industrial': 'industrial.fabrica',
            'warehouse': 'industrial.armazenamento',
            'school': 'institucional.educacao',
            'university': 'institucional.educacao',
            'hospital': 'institucional.saude',
            'church': 'institucional.religioso',
            'hotel': 'comercial.hospedagem',
            'public': 'institucional.governo',
            'office': 'comercial.escritorios',
            'shed': 'industrial.armazenamento'
        },

        # Mapeamento de tipos de uso do solo para categorias funcionais
        'landuse_to_category': {
            'residential': 'residencial',
            'commercial': 'comercial',
            'retail': 'comercial',
            'industrial': 'industrial',
            'institutional': 'institucional',
            'mixed': 'residencial.misto',
            'education': 'institucional.educacao',
            'healthcare': 'institucional.saude',
            'transportation': 'infraestrutura.transporte',
            'green': None,  # Uso do solo n√£o implica diretamente em categoria funcional
            'recreational': None,
            'agriculture': None,
            'water': None
        }
    }

    return {'categories': categories, 'mappings': value_mappings}

# Definindo as categorias funcionais
functional_categories = define_functional_categories()

# Exibindo a estrutura das categorias
for category, info in functional_categories['categories'].items():
    print(f"\n{category.upper()} - {info['description']}")
    for subcategory, values in info['subcategories'].items():
        print(f"  - {subcategory}: {', '.join(values[:3])}{'...' if len(values) > 3 else ''}")
    print(f"  Usos do solo correspondentes: {', '.join(info['landuse_match'])}")

# Fun√ß√£o para categorizar edif√≠cios com base em atributos e uso do solo
def categorize_buildings(gdf, categories_def):
    """
    Categoriza edif√≠cios com base em suas caracter√≠sticas e uso do solo.

    Args:
        gdf: GeoDataFrame com edif√≠cios e informa√ß√µes de uso do solo
        categories_def: Defini√ß√£o de categorias funcionais

    Returns:
        GeoDataFrame com novas colunas de categoriza√ß√£o
    """
    # Criar uma c√≥pia para n√£o modificar o original
    categorized_gdf = gdf.copy()

    # Extrair mapeamentos e categorias
    categories = categories_def['categories']
    mappings = categories_def['mappings']

    # Inicializar colunas para categoriza√ß√£o
    categorized_gdf['categoria_funcional'] = None
    categorized_gdf['subcategoria_funcional'] = None
    categorized_gdf['fonte_categoria'] = None

    # Identificar colunas dispon√≠veis para categoriza√ß√£o
    building_col = next((col for col in categorized_gdf.columns if col == 'building'), None)
    landuse_col = next((col for col in categorized_gdf.columns if col in ['landuse', 'land_category']), None)
    amenity_col = next((col for col in categorized_gdf.columns if col == 'amenity'), None)

    print(f"Usando colunas para categoriza√ß√£o: building={building_col}, landuse={landuse_col}, amenity={amenity_col}")

    # Contadores para estat√≠sticas
    categorization_stats = {
        'total': len(categorized_gdf),
        'from_building': 0,
        'from_amenity': 0,
        'from_landuse': 0,
        'uncategorized': 0
    }

    # Etapa 1: Categorizar com base no tipo de edif√≠cio
    if building_col:
        for idx, row in tqdm(categorized_gdf.iterrows(), total=len(categorized_gdf), desc="Categorizando por tipo de edif√≠cio"):
            building_value = row[building_col]

            if pd.notna(building_value) and building_value in mappings['building_to_category']:
                category_path = mappings['building_to_category'][building_value]
                if '.' in category_path:
                    main_cat, sub_cat = category_path.split('.')
                    categorized_gdf.at[idx, 'categoria_funcional'] = main_cat
                    categorized_gdf.at[idx, 'subcategoria_funcional'] = sub_cat
                else:
                    categorized_gdf.at[idx, 'categoria_funcional'] = category_path

                categorized_gdf.at[idx, 'fonte_categoria'] = 'building'
                categorization_stats['from_building'] += 1

    # Etapa 2: Complementar categoriza√ß√£o com base em amenidades
    if amenity_col:
        for idx, row in tqdm(categorized_gdf.iterrows(), total=len(categorized_gdf), desc="Categorizando por amenidades"):
            # S√≥ categorizar se ainda n√£o foi categorizado
            if pd.isna(row['categoria_funcional']) and pd.notna(row[amenity_col]):
                amenity_value = row[amenity_col]

                # Mapeamento simplificado de amenidades para categorias
                amenity_mapping = {
                    'school': 'institucional.educacao',
                    'university': 'institucional.educacao',
                    'library': 'institucional.cultural',
                    'hospital': 'institucional.saude',
                    'clinic': 'institucional.saude',
                    'doctors': 'institucional.saude',
                    'restaurant': 'comercial.restauracao',
                    'cafe': 'comercial.restauracao',
                    'fast_food': 'comercial.restauracao',
                    'bank': 'comercial.escritorios',
                    'marketplace': 'comercial.varejo',
                    'place_of_worship': 'institucional.religioso',
                    'police': 'infraestrutura.seguranca',
                    'fire_station': 'infraestrutura.seguranca',
                    'post_office': 'institucional.governo',
                    'town_hall': 'institucional.governo'
                }

                if amenity_value in amenity_mapping:
                    category_path = amenity_mapping[amenity_value]
                    if '.' in category_path:
                        main_cat, sub_cat = category_path.split('.')
                        categorized_gdf.at[idx, 'categoria_funcional'] = main_cat
                        categorized_gdf.at[idx, 'subcategoria_funcional'] = sub_cat
                    else:
                        categorized_gdf.at[idx, 'categoria_funcional'] = category_path

                    categorized_gdf.at[idx, 'fonte_categoria'] = 'amenity'
                    categorization_stats['from_amenity'] += 1

    # Etapa 3: Complementar categoriza√ß√£o com base no uso do solo
    if landuse_col:
        for idx, row in tqdm(categorized_gdf.iterrows(), total=len(categorized_gdf), desc="Categorizando por uso do solo"):
            # S√≥ categorizar se ainda n√£o foi categorizado
            if pd.isna(row['categoria_funcional']) and pd.notna(row[landuse_col]):
                landuse_value = row[landuse_col]

                # Verificar se o valor de uso do solo est√° no mapeamento
                if str(landuse_value).lower() in [k.lower() for k in mappings['landuse_to_category'].keys()]:
                    # Encontrar a chave correspondente (ignorando mai√∫sculas/min√∫sculas)
                    for key, value in mappings['landuse_to_category'].items():
                        if str(landuse_value).lower() == key.lower() and value is not None:
                            categorized_gdf.at[idx, 'categoria_funcional'] = value
                            categorized_gdf.at[idx, 'fonte_categoria'] = 'landuse'
                            categorization_stats['from_landuse'] += 1
                            break

    # Contar n√£o categorizados
    categorization_stats['uncategorized'] = categorization_stats['total'] - sum([
        categorization_stats['from_building'],
        categorization_stats['from_amenity'],
        categorization_stats['from_landuse']
    ])

    # Exibir estat√≠sticas
    print("\nEstat√≠sticas de categoriza√ß√£o:")
    print(f"- Total de edif√≠cios: {categorization_stats['total']}")
    print(f"- Categorizados por tipo de edif√≠cio: {categorization_stats['from_building']} ({categorization_stats['from_building']/categorization_stats['total']*100:.2f}%)")
    print(f"- Categorizados por amenidades: {categorization_stats['from_amenity']} ({categorization_stats['from_amenity']/categorization_stats['total']*100:.2f}%)")
    print(f"- Categorizados por uso do solo: {categorization_stats['from_landuse']} ({categorization_stats['from_landuse']/categorization_stats['total']*100:.2f}%)")
    print(f"- N√£o categorizados: {categorization_stats['uncategorized']} ({categorization_stats['uncategorized']/categorization_stats['total']*100:.2f}%)")

    return categorized_gdf, categorization_stats

# Categorizando os edif√≠cios
if isinstance(buildings_gdf, gpd.GeoDataFrame) and not buildings_gdf.empty:
    categorized_buildings, cat_stats = categorize_buildings(buildings_gdf, functional_categories)
    # Exibir amostra do resultado
    print("\nAmostra dos edif√≠cios categorizados:")
    if isinstance(categorized_buildings, gpd.GeoDataFrame):
        display(categorized_buildings.head())
    
    # Analisar os resultados da categoriza√ß√£o - APENAS se tivermos dados v√°lidos
    if isinstance(categorized_buildings, gpd.GeoDataFrame) and not categorized_buildings.empty and 'categoria_funcional' in categorized_buildings.columns:
        try:
            # Verificar se a fun√ß√£o existe
            if 'analyze_categorization_results' in globals():
                analyze_categorization_results(categorized_buildings, cat_stats)
            else:
                print("Fun√ß√£o analyze_categorization_results n√£o est√° definida. Pulando an√°lise visual de categoriza√ß√£o.")
                
            # Visualizar os edif√≠cios categorizados em um mapa
            try:
                if 'map_categorized_buildings' in globals():
                    fig, ax = map_categorized_buildings(categorized_buildings)
                else:
                    print("Fun√ß√£o map_categorized_buildings n√£o est√° definida. Pulando visualiza√ß√£o de mapa.")
            except Exception as e:
                print(f"Erro ao mapear edif√≠cios categorizados: {e}")
        except Exception as e:
            print(f"Erro ao analisar resultados da categoriza√ß√£o: {e}")
    else:
        print("Dados categorizados n√£o possuem colunas necess√°rias para an√°lise visual")
else:
    print("N√£o h√° dados de edif√≠cios v√°lidos para categoriza√ß√£o")
    categorized_buildings = gpd.GeoDataFrame()
    cat_stats = {}

# Analisar e visualizar os resultados da categoriza√ß√£o
def analyze_categorization_results(gdf, cat_stats):
    """
    Analisa e visualiza os resultados da categoriza√ß√£o funcional.

    Args:
        gdf: GeoDataFrame com edif√≠cios categorizados
        cat_stats: Estat√≠sticas da categoriza√ß√£o
    """
    # Distribui√ß√£o de categorias principais
    main_categories = gdf['categoria_funcional'].dropna().value_counts()

    # Gr√°fico de categorias principais
    plt.figure(figsize=(12, 6))
    main_categories.plot(kind='bar', color='skyblue')
    plt.title('Distribui√ß√£o de Categorias Funcionais Principais')
    plt.xlabel('Categoria')
    plt.ylabel('N√∫mero de Edif√≠cios')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Distribui√ß√£o de subcategorias
    subcategories = gdf.dropna(subset=['subcategoria_funcional'])
    subcategory_counts = subcategories.groupby(['categoria_funcional', 'subcategoria_funcional']).size().unstack(fill_value=0)

    # Gr√°fico de subcategorias (empilhado)
    if not subcategory_counts.empty:
        plt.figure(figsize=(14, 8))
        subcategory_counts.plot(kind='bar', stacked=True, cmap='viridis')
        plt.title('Distribui√ß√£o de Subcategorias Funcionais')
        plt.xlabel('Categoria Principal')
        plt.ylabel('N√∫mero de Edif√≠cios')
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Subcategoria', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()
    else:
        print("N√£o h√° dados de subcategorias para visualiza√ß√£o.")

    # Distribui√ß√£o por fonte de categoriza√ß√£o
    source_dist = gdf['fonte_categoria'].value_counts()

    # Gr√°fico de pizza da fonte de categoriza√ß√£o
    plt.figure(figsize=(10, 8))
    source_dist.plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette('pastel'), startangle=90)
    plt.title('Fonte de Categoriza√ß√£o Funcional')
    plt.ylabel('')
    plt.axis('equal')
    plt.tight_layout()
    plt.show()

    # An√°lise cruzada de categoria x uso do solo
    if 'land_category' in gdf.columns:
        cross_tab = pd.crosstab(
            gdf['categoria_funcional'],
            gdf['land_category'],
            margins=True,
            normalize='index'
        )

        print("An√°lise cruzada: Categorias funcionais x Uso do solo (normalizado por linha):")
        display(cross_tab)

        # Heatmap da rela√ß√£o
        plt.figure(figsize=(12, 10))
        sns.heatmap(cross_tab.iloc[:-1, :-1], annot=True, cmap='YlGnBu', fmt='.2f')
        plt.title('Distribui√ß√£o de Categorias Funcionais por Uso do Solo')
        plt.tight_layout()
        plt.show()

# Fun√ß√£o para criar um mapa dos edif√≠cios categorizados
def map_categorized_buildings(gdf, figsize=(15, 15), sample_size=1000):
    """
    Vers√£o b√°sica para criar um mapa dos edif√≠cios categorizados.
    
    Args:
        gdf: GeoDataFrame com edif√≠cios categorizados
        figsize: Tamanho da figura
        sample_size: Tamanho da amostra para visualiza√ß√£o
    """
    print("INFO: Fun√ß√£o map_categorized_buildings executando vers√£o simplificada.")
    
    try:
        import matplotlib.pyplot as plt
        import numpy as np
        
        # Criar mapa b√°sico
        fig, ax = plt.subplots(figsize=figsize)
        
        # Verificar se temos dados para plotar
        if not gdf.empty and 'geometry' in gdf.columns and 'categoria_funcional' in gdf.columns:
            # Filtrar apenas edif√≠cios categorizados
            categorized = gdf.dropna(subset=['categoria_funcional']).copy()
            
            # Se tiver muitos edif√≠cios, amostrar para melhor visualiza√ß√£o
            if len(categorized) > sample_size:
                categorized = categorized.sample(sample_size)
            
            # Preparar uma paleta de cores
            n_categories = categorized['categoria_funcional'].nunique()
            colors = plt.cm.tab20(np.linspace(0, 1, n_categories))
            
            # Plotar por categoria
            for i, (category, group) in enumerate(categorized.groupby('categoria_funcional')):
                color = colors[i % len(colors)]
                group.plot(ax=ax, color=color, label=category, alpha=0.7, markersize=20)
            
            # Configurar gr√°fico
            ax.set_title('Categoriza√ß√£o Funcional de Edif√≠cios', fontsize=16)
            ax.legend(title='Categoria Funcional', loc='upper left', bbox_to_anchor=(1, 1))
            plt.tight_layout()
        else:
            ax.text(0.5, 0.5, "Dados insuficientes para visualiza√ß√£o", 
                   ha='center', va='center', fontsize=14)
        
        return fig, ax
    except Exception as e:
        print(f"Erro ao criar mapa: {e}")
        # Criar uma figura vazia em caso de erro
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.text(0.5, 0.5, f"Erro ao criar visualiza√ß√£o: {str(e)}", 
               ha='center', va='center', fontsize=14)
        return fig, ax

# Note: Map visualization disabled - only create maps when there's valid data
# To visualize buildings on a map, use:
# fig, ax = map_categorized_buildings(categorized_buildings) # Only when buildings_gdf has valid data

# Adicionar a classifica√ß√£o funcional ao GeoDataFrame original
def finalize_categorization(gdf, existing_class_col=None):
    """
    Finaliza a categoriza√ß√£o criando uma coluna de classifica√ß√£o final.

    Args:
        gdf: GeoDataFrame com categoriza√ß√£o preliminar
        existing_class_col: Nome da coluna de classifica√ß√£o existente, se houver

    Returns:
        GeoDataFrame com classifica√ß√£o final
    """
    # Criar c√≥pia para n√£o modificar o original
    final_gdf = gdf.copy()

    # Criar coluna de classifica√ß√£o final
    final_gdf['building_class_enhanced'] = None

    # Verificar se j√° existe uma coluna de classifica√ß√£o
    if existing_class_col and existing_class_col in final_gdf.columns:
        print(f"Incorporando classifica√ß√£o existente da coluna '{existing_class_col}'")
        # Copiar classifica√ß√£o existente como base
        final_gdf['building_class_enhanced'] = final_gdf[existing_class_col]

    # Verificar se temos a coluna categoria_funcional
    if 'categoria_funcional' in final_gdf.columns:
        # Prioridade: categoriza√ß√£o nova > classifica√ß√£o existente
        for idx, row in tqdm(final_gdf.iterrows(), total=len(final_gdf), desc="Finalizando categoriza√ß√£o"):
            # Se temos categoria funcional, us√°-la
            if pd.notna(row['categoria_funcional']):
                # Se temos subcategoria, inclu√≠-la na classifica√ß√£o
                if 'subcategoria_funcional' in final_gdf.columns and pd.notna(row['subcategoria_funcional']):
                    classification = f"{row['categoria_funcional']}_{row['subcategoria_funcional']}"
                else:
                    classification = row['categoria_funcional']

                final_gdf.at[idx, 'building_class_enhanced'] = classification

    # Para edif√≠cios sem classifica√ß√£o final, tentar usar outros atributos
    unclassified = final_gdf['building_class_enhanced'].isna()
    unclassified_count = unclassified.sum()
    total_count = len(final_gdf)
    
    # Evitar divis√£o por zero
    if unclassified_count > 0:
        unclassified_percent = unclassified_count / total_count * 100
        print(f"Edif√≠cios ainda sem classifica√ß√£o: {unclassified_count} ({unclassified_percent:.2f}%)")
    else:
        print(f"Todos os edif√≠cios foram classificados com sucesso!")

    if unclassified.any():
        # Tentar usar 'building' para os n√£o classificados
        if 'building' in final_gdf.columns:
            for idx in final_gdf[unclassified].index:
                if pd.notna(final_gdf.at[idx, 'building']):
                    final_gdf.at[idx, 'building_class_enhanced'] = f"unclassified_{final_gdf.at[idx, 'building']}"

        # Atualizar contagem de n√£o classificados
        unclassified = final_gdf['building_class_enhanced'].isna()
        unclassified_count = unclassified.sum()
        
        # Evitar divis√£o por zero novamente
        if unclassified_count > 0:
            unclassified_percent = unclassified_count / total_count * 100
            print(f"Edif√≠cios ainda sem classifica√ß√£o ap√≥s uso de 'building': {unclassified_count} ({unclassified_percent:.2f}%)")
        
        # Para os restantes, usar 'other' como classifica√ß√£o
        final_gdf.loc[unclassified, 'building_class_enhanced'] = 'other'

    # Exibir estat√≠sticas da classifica√ß√£o final
    if 'building_class_enhanced' in final_gdf.columns:
        final_stats = final_gdf['building_class_enhanced'].value_counts().head(10)
        print("\nTop 10 classifica√ß√µes finais:")
        for class_name, count in final_stats.items():
            print(f"- {class_name}: {count} ({count/len(final_gdf)*100:.2f}%)")

    return final_gdf

# Identificar coluna de classifica√ß√£o existente, se houver
existing_class_cols = [col for col in categorized_buildings.columns if 'class' in col.lower() and col != 'categoria_funcional']
existing_class_col = existing_class_cols[0] if existing_class_cols else None

# Finalizar a categoriza√ß√£o
final_categorized_buildings = finalize_categorization(categorized_buildings, existing_class_col)

# Validar a categoriza√ß√£o final
def validate_categorization(gdf, n_samples=5):
    """
    Valida a categoriza√ß√£o exibindo amostras aleat√≥rias de cada classe.
    
    Args:
        gdf: GeoDataFrame com categoriza√ß√£o
        n_samples: N√∫mero de amostras para exibir
        
    Returns:
        dict: M√©tricas de categoriza√ß√£o
    """
    if gdf is None:
        print("GeoDataFrame √© None. N√£o √© poss√≠vel validar.")
        return {'total': 0, 'categorized': 0, 'percent_categorized': 0}
        
    if len(gdf) == 0:
        print("GeoDataFrame vazio. N√£o √© poss√≠vel validar.")
        return {'total': 0, 'categorized': 0, 'percent_categorized': 0}
    
    # Criar uma c√≥pia para n√£o modificar o original
    gdf_copy = gdf.copy()
    
    # Verificar e adicionar as colunas necess√°rias individualmente
    required_columns = ['categoria_funcional', 'subcategoria_funcional', 'building_class_enhanced']
    missing_columns = []
    
    for col in required_columns:
        if col not in gdf_copy.columns:
            missing_columns.append(col)
            gdf_copy[col] = pd.NA
    
    if missing_columns:
        print(f"AVISO: As seguintes colunas n√£o existem no DataFrame e foram adicionadas: {missing_columns}")
    
    # Calcular m√©tricas de categoriza√ß√£o
    total = len(gdf_copy)
    categorized = 0
    percent_categorized = 0
    
    if 'building_class_enhanced' in gdf_copy.columns:
        categorized = (~gdf_copy['building_class_enhanced'].isna()).sum()
        percent_categorized = (categorized / total) * 100 if total > 0 else 0
    
    print(f"Total de edif√≠cios: {total}")
    print(f"Edif√≠cios categorizados: {categorized} ({percent_categorized:.2f}%)")
    
    # Selecionar colunas para exibir na valida√ß√£o
    display_columns = ['geometry']
    
    # Adicionar as colunas de categoriza√ß√£o se existirem
    if 'building_class_enhanced' in gdf_copy.columns:
        display_columns.append('building_class_enhanced')
    if 'categoria_funcional' in gdf_copy.columns:
        display_columns.append('categoria_funcional')
    if 'subcategoria_funcional' in gdf_copy.columns:
        display_columns.append('subcategoria_funcional')
    
    # Adicionar outros atributos que podem ajudar na valida√ß√£o
    for col in ['name', 'building', 'amenity', 'shop', 'aeroway', 'tourism']:
        if col in gdf_copy.columns:
            display_columns.append(col)
    
    # Mostrar amostras de cada categoria para valida√ß√£o
    if 'building_class_enhanced' in gdf_copy.columns:
        # Filtrar para obter apenas valores n√£o nulos
        non_null_mask = gdf_copy['building_class_enhanced'].notna()
        if non_null_mask.any():
            categories = gdf_copy.loc[non_null_mask, 'building_class_enhanced'].unique()
            if len(categories) > 0:
                print(f"\nExibindo at√© {n_samples} amostras de cada categoria ({len(categories)} categorias no total)")
                
                for cat in categories:
                    subset = gdf_copy[gdf_copy['building_class_enhanced'] == cat]
                    print(f"\nCategoria: {cat} ({len(subset)} edif√≠cios)")
                    
                    if len(subset) > 0:
                        # Tentar obter amostras aleat√≥rias com tratamento de erro
                        try:
                            # Garantir que n√£o tentamos obter mais amostras do que existem
                            sample_n = min(n_samples, len(subset))
                            if sample_n > 0:
                                sample = subset.sample(sample_n)
                                # Exibir apenas as colunas selecionadas
                                columns_to_display = [col for col in display_columns if col in sample.columns]
                                print(sample[columns_to_display])
                            else:
                                print("N√£o h√° edif√≠cios suficientes para amostragem.")
                        except Exception as e:
                            print(f"Erro ao exibir amostra: {e}")
            else:
                print("Nenhuma categoria encontrada nos dados.")
        else:
            print("Nenhum edif√≠cio foi categorizado (todas as entradas s√£o nulas).")
    else:
        print("Coluna 'building_class_enhanced' n√£o dispon√≠vel para mostrar categorias.")
    
    # Retornar m√©tricas
    return {
        'total': total,
        'categorized': categorized,
        'percent_categorized': percent_categorized
    }

# Salvar o resultado final para uso no pr√≥ximo notebook
def save_final_categorization(gdf, filename='buildings_categorized.gpkg'):
    """
    Salva a categoriza√ß√£o final para uso posterior.

    Args:
        gdf: GeoDataFrame com a categoriza√ß√£o final
        filename: Nome do arquivo para salvamento
    """
    # Criar pasta de resultados se n√£o existir
    results_dir = os.path.join(OUTPUT_DIR, 'results')
    os.makedirs(results_dir, exist_ok=True)

    # Caminho completo para o arquivo
    result_path = os.path.join(results_dir, filename)

    # Salvar como GeoPackage
    gdf.to_file(result_path, driver='GPKG')

    # Salvar tamb√©m como pickle para preservar tipos de dados
    pickle_path = os.path.join(results_dir, 'buildings_categorized.pkl')
    with open(pickle_path, 'wb') as f:
        pickle.dump(gdf, f)

    print(f"Categoriza√ß√£o final salva em:\n- {result_path}\n- {pickle_path}")

    return result_path, pickle_path

# Salvar a categoriza√ß√£o final
gpkg_path, pickle_path = save_final_categorization(final_categorized_buildings)

# Resumo e conclus√£o
print("="*80)
print("RESUMO DA CATEGORIZA√á√ÉO FUNCIONAL DE EDIF√çCIOS")
print("="*80)

# Estat√≠sticas gerais
total_buildings = len(final_categorized_buildings)

# Verificar se as colunas necess√°rias existem antes de acess√°-las
if 'categoria_funcional' in final_categorized_buildings.columns:
    categorized_count = final_categorized_buildings['categoria_funcional'].notna().sum()
    categories_count = final_categorized_buildings['categoria_funcional'].nunique()
else:
    categorized_count = 0
    categories_count = 0
    # Adicionar a coluna se n√£o existir
    final_categorized_buildings['categoria_funcional'] = pd.NA

if 'subcategoria_funcional' in final_categorized_buildings.columns:
    subcategories_count = final_categorized_buildings['subcategoria_funcional'].nunique()
else:
    subcategories_count = 0
    # Adicionar a coluna se n√£o existir
    final_categorized_buildings['subcategoria_funcional'] = pd.NA

print(f"Total de edif√≠cios processados: {total_buildings}")
print("\nN√£o foi poss√≠vel gerar estat√≠sticas finais - dados insuficientes ou inv√°lidos")

print("="*80)

# Explorar os dados de zoneamento
if isinstance(final_categorized_buildings, gpd.GeoDataFrame) and not final_categorized_buildings.empty:
    try:
        primary_landuse_col = explore_landuse_data(final_categorized_buildings)
    except NameError:
        print("Fun√ß√£o explore_landuse_data n√£o est√° definida. Pulando an√°lise de zoneamento.")
        primary_landuse_col = None
    except Exception as e:
        print(f"Erro ao explorar dados de zoneamento: {e}")
        primary_landuse_col = None
else:
    primary_landuse_col = None
    print("N√£o h√° dados para explorar o zoneamento")

# Prosseguir apenas se tivermos uma coluna de uso do solo v√°lida
if primary_landuse_col:
    try:
        # Definir regras de conformidade para avalia√ß√£o
        conformity_rules = define_conformity_rules()
        
        # Analisar conformidade entre uso real e zoneamento
        try:
            conformity_result = analyze_conformity(final_categorized_buildings, primary_landuse_col, conformity_rules)
            if isinstance(conformity_result, tuple) and len(conformity_result) >= 1:
                conformity_gdf = conformity_result[0]
                conformity_stats = conformity_result[1] if len(conformity_result) > 1 else {}
                level_stats = conformity_result[2] if len(conformity_result) > 2 else {}
            else:
                conformity_gdf = conformity_result
                conformity_stats = {}
                level_stats = {}
        except Exception as e:
            print(f"Erro ao analisar conformidade: {e}")
            conformity_gdf = gpd.GeoDataFrame()
            conformity_stats = {}
            level_stats = {}
        
        # Verificar se a an√°lise de conformidade foi bem-sucedida
        if isinstance(conformity_gdf, gpd.GeoDataFrame) and not conformity_gdf.empty and 'conformidade_uso' in conformity_gdf.columns:
            # Visualizar resultados em mapa
            try:
                map_conformity(conformity_gdf)
                
                # An√°lise de padr√µes de conformidade
                analyze_conformity_patterns(conformity_gdf, primary_landuse_col)
                
                # Calcular √≠ndices de conformidade
                conformity_indices = calculate_conformity_indices(conformity_gdf)
                
                # Identificar anomalias e padr√µes espaciais
                identify_anomalies(conformity_gdf, primary_landuse_col)
                
                # Salvar resultados
                gpkg_path, pickle_path, indices_path = save_conformity_analysis(conformity_gdf)
                
                print(f"\nA an√°lise de conformidade foi conclu√≠da com sucesso!")
                if gpkg_path:
                    print(f"Resultados salvos em:")
                    print(f"- GeoPackage: {os.path.basename(gpkg_path)}")
                    print(f"- Pickle: {os.path.basename(pickle_path)}")
                    print(f"- √çndices: {os.path.basename(indices_path)}")
            except Exception as e:
                print(f"Erro durante a an√°lise de conformidade: {e}")
        else:
            print("A an√°lise de conformidade n√£o produziu resultados v√°lidos")
    except NameError as e:
        print(f"Fun√ß√£o n√£o definida: {e}. Pulando an√°lise de conformidade.")
    except Exception as e:
        print(f"Erro ao realizar an√°lise de conformidade: {e}")
else:
    print("N√£o foi poss√≠vel realizar a an√°lise de conformidade devido √† falta de dados de zoneamento")

# Se n√£o encontrou dados v√°lidos, criar um GeoDataFrame de exemplo para testes
if (not 'buildings_gdf' in locals() or 
    not isinstance(buildings_gdf, gpd.GeoDataFrame) or 
    buildings_gdf.empty or
    'geometry' not in buildings_gdf.columns):
    
    print("\nCriando GeoDataFrame de exemplo para testes...")
    from shapely.geometry import Point
    
    # Criar alguns pontos de exemplo
    geometries = [Point(x, y) for x, y in [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]]
    
    # Criar dados de exemplo
    data = {
        'geometry': geometries,
        'building': ['residential', 'commercial', 'industrial', 'school', 'hospital'],
        'land_category': ['residential', 'commercial', 'industrial', 'institutional', 'healthcare']
    }
    
    # Criar GeoDataFrame
    example_gdf = gpd.GeoDataFrame(data, crs="EPSG:4326")
    
    # Decidir se deve usar o GeoDataFrame de exemplo
    use_example = False
    if use_example:
        buildings_gdf = example_gdf
        print("Usando dados de exemplo para demonstra√ß√£o. Defina use_example = False para desativar.")
    else:
        print("Dados de exemplo dispon√≠veis, mas n√£o est√£o sendo usados. Defina use_example = True para ativar.")
    
    # Mostrar o GeoDataFrame de exemplo
    print("\nExemplo de GeoDataFrame que seria usado:")
    display(example_gdf)

# Explorando os atributos dispon√≠veis

# Adicionar implementa√ß√µes m√≠nimas para fun√ß√µes ausentes
# Estas implementa√ß√µes apenas evitam erros e n√£o executam a funcionalidade real

def explore_landuse_data(gdf):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o explore_landuse_data n√£o implementada completamente.")
    # Tentar encontrar colunas relacionadas a uso do solo
    landuse_cols = [col for col in gdf.columns if 'land' in col.lower() or 'uso' in col.lower()]
    print(f"Colunas de uso do solo encontradas: {landuse_cols}")
    return landuse_cols[0] if landuse_cols else None

def define_conformity_rules():
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o define_conformity_rules n√£o implementada completamente.")
    return {"regras_vazias": True}  # Retorna dicion√°rio vazio de regras

def analyze_conformity(gdf, landuse_col, rules):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o analyze_conformity n√£o implementada completamente.")
    return gdf.copy(), {}, {}  # Retorna GeoDataFrame sem modifica√ß√µes e dicion√°rios vazios

def map_conformity(gdf):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o map_conformity n√£o implementada completamente.")
    # N√£o faz nada, apenas evita erros

def analyze_conformity_patterns(gdf, landuse_col):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o analyze_conformity_patterns n√£o implementada completamente.")
    # N√£o faz nada, apenas evita erros

def calculate_conformity_indices(gdf):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o calculate_conformity_indices n√£o implementada completamente.")
    return {}  # Retorna dicion√°rio vazio

def identify_anomalies(gdf, landuse_col):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o identify_anomalies n√£o implementada completamente.")
    # N√£o faz nada, apenas evita erros

def save_conformity_analysis(gdf):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o save_conformity_analysis n√£o implementada completamente.")
    gpkg_path = None
    pickle_path = None
    indices_path = None
    return gpkg_path, pickle_path, indices_path

def analyze_categorization_results(gdf, stats):
    """Vers√£o tempor√°ria para evitar erros."""
    print("INFO: Fun√ß√£o analyze_categorization_results n√£o implementada completamente.")
    # Vers√£o b√°sica - apenas mostra contagem de categorias se dispon√≠vel
    if 'categoria_funcional' in gdf.columns:
        categories = gdf['categoria_funcional'].value_counts()
        print("\nDistribui√ß√£o de categorias:")
        print(categories)
    # N√£o faz nada mais, apenas evita erros

# Importa√ß√µes necess√°rias para evitar erros
try:
    import seaborn as sns
except ImportError:
    print("Biblioteca seaborn n√£o est√° dispon√≠vel. Algumas visualiza√ß√µes ser√£o limitadas.")
    # Criar um substituto simples para sns.color_palette
    class MockSNS:
        def color_palette(self, *args, **kwargs):
            return [(0.1, 0.2, 0.5), (0.2, 0.4, 0.6), (0.3, 0.6, 0.7), (0.4, 0.8, 0.8)]
    sns = MockSNS()

# Verificar se tqdm est√° dispon√≠vel para barras de progresso
try:
    from tqdm.notebook import tqdm
except ImportError:
    try:
        from tqdm import tqdm
    except ImportError:
        # Criar uma vers√£o simples de tqdm se n√£o estiver dispon√≠vel
        def tqdm(iterable, **kwargs):
            return iterable