{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cad13e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c03dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Resumo e conclus√£o\n",
    "\n",
    "# prompt: me de o comando para carregar o drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "üìì 1_Configuracao_Ambiente/\n",
    "# Verificar o tipo de GPU dispon√≠vel\n",
    "!nvidia-smi\n",
    "# Verificar a vers√£o do Python e ambiente atual\n",
    "import sys\n",
    "print(f\"Vers√£o do Python: {sys.version}\")\n",
    "!pip --version\n",
    "# Instala√ß√£o de bibliotecas essenciais para processamento geoespacial\n",
    "!pip install -U geopandas rasterio pyproj shapely fiona rtree pycrs mapclassify pyogrio\n",
    "!pip install -U osmnx networkx folium keplergl\n",
    "!pip install -U rasterstats xarray rioxarray netCDF4 \n",
    "!pip install -U scipy scikit-learn statsmodels plotly matplotlib seaborn\n",
    "!pip install -U tqdm jupyterlab notebook ipywidgets\n",
    "!pip install -U pygeos contextily pysal momepy\n",
    "!pip install -U psycopg2-binary sqlalchemy geoalchemy2\n",
    "# Instala√ß√£o das bibliotecas RAPIDS para processamento acelerado por GPU\n",
    "# O L4 √© compat√≠vel com RAPIDS, que acelera significativamente opera√ß√µes geoespaciais\n",
    "!pip install -U cudf cuml cuspatial cupy\n",
    "# Bibliotecas espec√≠ficas para manipula√ß√£o de dados tabulares e leitura de arquivos\n",
    "!pip install -U pandas numpy openpyxl xlrd xlwt xlsxwriter\n",
    "!pip install -U pyshp geojson topojson geobuf mapbox-vector-tile\n",
    "!pip install -U geoviews holoviews datashader panel hvplot\n",
    "!pip install -U dask distributed h5py hdf5 zarr\n",
    "# Verificar importa√ß√µes essenciais e configura√ß√µes b√°sicas\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import networkx as nx\n",
    "\n",
    "print(\"Verifica√ß√£o de configura√ß√£o b√°sica conclu√≠da. Bibliotecas principais carregadas com sucesso.\")\n",
    "# Configurar ambiente para trabalhar com o L4 GPU\n",
    "import os\n",
    "\n",
    "# Definir vari√°veis de ambiente para otimiza√ß√£o de bibliotecas geoespaciais\n",
    "os.environ['USE_PYGEOS'] = '0'  # Usar GeoPandas com GEOS (mais est√°vel)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Utilizar GPU 0\n",
    "\n",
    "# Configura√ß√µes para utilizar mem√≥ria de GPU de forma eficiente\n",
    "os.environ['RAPIDS_NO_INITIALIZE'] = '1'  # Inicializa√ß√£o manual do RAPIDS\n",
    "\n",
    "print(\"Vari√°veis de ambiente configuradas para otimiza√ß√£o com GPU L4\")\n",
    "# Configura√ß√£o para visualiza√ß√£o interativa\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Aumentar limite de exibi√ß√£o para melhor visualiza√ß√£o de dados geoespaciais\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Configurar matplotlib para visualiza√ß√µes de alta qualidade\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Configura√ß√µes de visualiza√ß√£o otimizadas\")\n",
    "# Fun√ß√£o utilit√°ria para verificar consumo de mem√≥ria\n",
    "def check_memory_usage():\n",
    "    \"\"\"Exibe o uso atual de mem√≥ria RAM e GPU.\"\"\"\n",
    "    print(\"Uso de mem√≥ria RAM:\")\n",
    "    !free -h\n",
    "    \n",
    "    print(\"\\nUso de mem√≥ria GPU:\")\n",
    "    !nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
    "    \n",
    "check_memory_usage()\n",
    "# Testar leitura b√°sica de dados geoespaciais\n",
    "try:\n",
    "    # Criar um GeoDataFrame simples para teste\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    # Criar alguns pontos de teste\n",
    "    geometry = [Point(0, 0), Point(1, 1), Point(2, 2)]\n",
    "    gdf = gpd.GeoDataFrame(geometry=geometry)\n",
    "    \n",
    "    # Plotar para verificar funcionamento\n",
    "    gdf.plot()\n",
    "    plt.title(\"Teste de Plotagem Geoespacial\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Teste de funcionalidade geoespacial conclu√≠do com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao testar funcionalidade geoespacial: {e}\")\n",
    "# Resumo da configura√ß√£o do ambiente\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURA√á√ÉO DO AMBIENTE CONCLU√çDA COM SUCESSO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBibliotecas instaladas e configuradas:\")\n",
    "print(\"- Processamento geoespacial: GeoPandas, Rasterio, Shapely, Fiona\")\n",
    "print(\"- An√°lise de redes: NetworkX, OSMnx\")\n",
    "print(\"- Processamento de dados: Pandas, NumPy, SciPy, Scikit-learn\")\n",
    "print(\"- Visualiza√ß√£o: Matplotlib, Folium, Plotly, Kepler.gl\")\n",
    "print(\"- Acelera√ß√£o GPU: RAPIDS (cuDF, cuSpatial)\")\n",
    "print(\"\\nPr√≥ximos passos:\")\n",
    "print(\"1. Execute 1.2_Carregamento_Datasets.ipynb para carregar os dados\")\n",
    "print(\"2. Execute 1.3_Configuracao_Sistema_Coordenadas.ipynb para configurar os sistemas de refer√™ncia\")\n",
    "print(\"=\"*80)\n",
    "1.2_Carregamento_Datasets.ipynb\n",
    "# Carregamento de Datasets para Integra√ß√£o Geoespacial\n",
    "# Este notebook carrega os datasets GPKG necess√°rios para o projeto\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Montando o Google Drive para acessar os dados\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Defini√ß√£o do diret√≥rio onde est√£o os datasets\n",
    "data_dir = '/content/drive/MyDrive/geoprocessamento_gnn/DATA'\n",
    "\n",
    "# Verificando se o diret√≥rio existe\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"O diret√≥rio {data_dir} n√£o foi encontrado. Verifique o caminho.\")\n",
    "    \n",
    "print(f\"Diret√≥rio de dados encontrado: {data_dir}\")\n",
    "# Listando todos os arquivos GPKG no diret√≥rio de dados\n",
    "gpkg_files = glob.glob(os.path.join(data_dir, \"*.gpkg\"))\n",
    "print(f\"Encontrados {len(gpkg_files)} arquivos GPKG:\")\n",
    "for i, file in enumerate(gpkg_files):\n",
    "    print(f\"{i+1}. {os.path.basename(file)}\")\n",
    "    \n",
    "if len(gpkg_files) == 0:\n",
    "    print(\"Nenhum arquivo GPKG encontrado. Verifique o caminho ou extens√£o dos arquivos.\")\n",
    "# Fun√ß√£o para explorar as camadas dentro de um arquivo GPKG\n",
    "def explore_gpkg(gpkg_path):\n",
    "    \"\"\"\n",
    "    Explora as camadas dispon√≠veis em um arquivo GeoPackage.\n",
    "    \n",
    "    Args:\n",
    "        gpkg_path: Caminho para o arquivo GPKG\n",
    "    \n",
    "    Returns:\n",
    "        Um DataFrame com informa√ß√µes sobre as camadas\n",
    "    \"\"\"\n",
    "    # Listar todas as camadas no arquivo\n",
    "    layers = fiona.listlayers(gpkg_path)\n",
    "    \n",
    "    # Coletar informa√ß√µes sobre cada camada\n",
    "    layer_info = []\n",
    "    for layer in layers:\n",
    "        try:\n",
    "            # Abrir a camada para obter informa√ß√µes\n",
    "            with fiona.open(gpkg_path, layer=layer) as src:\n",
    "                # Obter a contagem de fei√ß√µes\n",
    "                count = len(src)\n",
    "                # Obter o tipo de geometria\n",
    "                if count > 0:\n",
    "                    geometry_type = src.schema['geometry']\n",
    "                else:\n",
    "                    geometry_type = \"Desconhecido\"\n",
    "                # Obter o CRS\n",
    "                crs = src.crs\n",
    "                # Obter os campos de atributos\n",
    "                fields = list(src.schema['properties'].keys())\n",
    "                \n",
    "                layer_info.append({\n",
    "                    'Layer': layer,\n",
    "                    'Feature Count': count,\n",
    "                    'Geometry Type': geometry_type,\n",
    "                    'CRS': crs,\n",
    "                    'Fields': fields[:5] + ['...'] if len(fields) > 5 else fields\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar camada {layer}: {e}\")\n",
    "            layer_info.append({\n",
    "                'Layer': layer,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(layer_info)\n",
    "\n",
    "# Importamos fiona aqui para listar as camadas\n",
    "import fiona\n",
    "\n",
    "# Explorando cada arquivo GPKG\n",
    "for gpkg_file in gpkg_files:\n",
    "    file_name = os.path.basename(gpkg_file)\n",
    "    print(f\"\\n{'='*80}\\nExplorando arquivo: {file_name}\\n{'='*80}\")\n",
    "    \n",
    "    layers_info = explore_gpkg(gpkg_file)\n",
    "    display(layers_info)\n",
    "# Fun√ß√£o para carregar e armazenar todos os datasets em um dicion√°rio\n",
    "def load_all_datasets(gpkg_files):\n",
    "    \"\"\"\n",
    "    Carrega todos os arquivos GPKG e suas camadas em um dicion√°rio.\n",
    "    \n",
    "    Args:\n",
    "        gpkg_files: Lista de caminhos para arquivos GPKG\n",
    "    \n",
    "    Returns:\n",
    "        Um dicion√°rio de GeoDataFrames organizados por nome de arquivo e camada\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    for gpkg_file in tqdm(gpkg_files, desc=\"Carregando arquivos GPKG\"):\n",
    "        file_name = os.path.basename(gpkg_file)\n",
    "        datasets[file_name] = {}\n",
    "        \n",
    "        # Listar camadas\n",
    "        layers = fiona.listlayers(gpkg_file)\n",
    "        \n",
    "        for layer in tqdm(layers, desc=f\"Camadas em {file_name}\", leave=False):\n",
    "            try:\n",
    "                # Carregar o GeoDataFrame\n",
    "                gdf = gpd.read_file(gpkg_file, layer=layer)\n",
    "                \n",
    "                # Armazenar no dicion√°rio\n",
    "                datasets[file_name][layer] = gdf\n",
    "                \n",
    "                print(f\"Carregado: {file_name} - {layer} - {len(gdf)} registros\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar camada {layer} de {file_name}: {e}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Carregando todos os datasets\n",
    "datasets = load_all_datasets(gpkg_files)\n",
    "\n",
    "# Salvando em uma vari√°vel global para acesso posterior\n",
    "import builtins\n",
    "builtins.datasets = datasets\n",
    "\n",
    "print(\"\\nCarregamento completo. Todos os datasets est√£o armazenados na vari√°vel global 'datasets'.\")\n",
    "# Fun√ß√£o para visualizar a extens√£o geogr√°fica de todos os datasets em um mapa\n",
    "def plot_dataset_extents(datasets):\n",
    "    \"\"\"\n",
    "    Visualiza a extens√£o geogr√°fica de todos os datasets carregados.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dicion√°rio de datasets carregados\n",
    "    \"\"\"\n",
    "    # Criar um gr√°fico vazio\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    \n",
    "    # Cores para diferentes datasets\n",
    "    colors = plt.cm.tab20.colors\n",
    "    color_idx = 0\n",
    "    \n",
    "    # Legenda\n",
    "    legend_items = []\n",
    "    \n",
    "    # Iterar atrav√©s dos datasets\n",
    "    for file_name, layers in datasets.items():\n",
    "        for layer_name, gdf in layers.items():\n",
    "            # Verificar se o GeoDataFrame tem geometria\n",
    "            if gdf.geometry.is_empty.all():\n",
    "                print(f\"Geometria vazia em {file_name} - {layer_name}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Obter a extens√£o do dataset\n",
    "                minx, miny, maxx, maxy = gdf.total_bounds\n",
    "                extent_box = box(minx, miny, maxx, maxy)\n",
    "                \n",
    "                # Criar um GeoDataFrame com a extens√£o\n",
    "                extent_gdf = gpd.GeoDataFrame(geometry=[extent_box], crs=gdf.crs)\n",
    "                \n",
    "                # Plotar no mapa\n",
    "                color = colors[color_idx % len(colors)]\n",
    "                extent_gdf.boundary.plot(ax=ax, color=color, linewidth=2, \n",
    "                                        label=f\"{file_name} - {layer_name}\")\n",
    "                \n",
    "                # Adicionar √† legenda\n",
    "                legend_items.append(f\"{file_name} - {layer_name}\")\n",
    "                \n",
    "                # Incrementar √≠ndice de cor\n",
    "                color_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao plotar extens√£o de {file_name} - {layer_name}: {e}\")\n",
    "    \n",
    "    # Configurar o gr√°fico\n",
    "    ax.set_title(\"Extens√£o geogr√°fica dos datasets carregados\", fontsize=16)\n",
    "    ax.legend(legend_items, fontsize=10, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizando a extens√£o dos datasets\n",
    "plot_dataset_extents(datasets)\n",
    "# Salvando o estado dos datasets para uso no pr√≥ximo notebook\n",
    "import pickle\n",
    "\n",
    "# Criando pasta de estado se n√£o existir\n",
    "state_dir = os.path.join(data_dir, 'state')\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "\n",
    "# Caminho para o arquivo de estado\n",
    "state_file = os.path.join(state_dir, 'datasets_state.pkl')\n",
    "\n",
    "# Salvando o dicion√°rio de datasets\n",
    "with open(state_file, 'wb') as f:\n",
    "    pickle.dump(datasets, f)\n",
    "\n",
    "print(f\"Estado dos datasets salvo em: {state_file}\")\n",
    "print(\"Os datasets est√£o prontos para o pr√≥ximo notebook de configura√ß√£o do sistema de coordenadas.\")\n",
    "# Resumo do carregamento de dados\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO DE CARREGAMENTO DE DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contabilizar totais\n",
    "total_layers = 0\n",
    "total_features = 0\n",
    "\n",
    "# Exibir estat√≠sticas por arquivo\n",
    "for file_name, layers in datasets.items():\n",
    "    layer_count = len(layers)\n",
    "    total_layers += layer_count\n",
    "    \n",
    "    feature_counts = [len(gdf) for gdf in layers.values()]\n",
    "    total_file_features = sum(feature_counts)\n",
    "    total_features += total_file_features\n",
    "    \n",
    "    print(f\"\\nArquivo: {file_name}\")\n",
    "    print(f\"  N√∫mero de camadas: {layer_count}\")\n",
    "    print(f\"  Total de fei√ß√µes: {total_file_features:,}\")\n",
    "    \n",
    "    # Listar cada camada com contagem\n",
    "    for layer_name, gdf in layers.items():\n",
    "        print(f\"    - {layer_name}: {len(gdf):,} fei√ß√µes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL GERAL: {total_layers} camadas, {total_features:,} fei√ß√µes\")\n",
    "print(\"=\"*80)\n",
    "1.3_Configuracao_Sistema_Coordenadas.ipynb\n",
    "# Configura√ß√£o do Sistema de Coordenadas para Integra√ß√£o Geoespacial\n",
    "# Este notebook padroniza todos os datasets para o sistema SIRGAS 2000 23S (EPSG:31983)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from shapely.geometry import box\n",
    "import warnings\n",
    "\n",
    "# Montando o Google Drive para acessar os dados\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Diret√≥rio de dados\n",
    "data_dir = '/content/drive/MyDrive/geoprocessamento_gnn/DATA'\n",
    "state_dir = os.path.join(data_dir, 'state')\n",
    "state_file = os.path.join(state_dir, 'datasets_state.pkl')\n",
    "\n",
    "# Verificando se o arquivo de estado existe\n",
    "if not os.path.exists(state_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de estado n√£o encontrado: {state_file}\")\n",
    "\n",
    "# Carregando o estado dos datasets do notebook anterior\n",
    "with open(state_file, 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "\n",
    "# Definindo globalmente para uso posterior\n",
    "import builtins\n",
    "builtins.datasets = datasets\n",
    "\n",
    "print(f\"Datasets carregados com sucesso do arquivo: {state_file}\")\n",
    "# Defini√ß√£o do sistema de coordenadas alvo: SIRGAS 2000 / UTM zone 23S (EPSG:31983)\n",
    "TARGET_CRS = \"EPSG:31983\"\n",
    "\n",
    "# Informa√ß√µes sobre o sistema de coordenadas alvo\n",
    "sirgas_info = {\n",
    "    'EPSG': 31983,\n",
    "    'Nome': 'SIRGAS 2000 / UTM zone 23S',\n",
    "    'Proje√ß√£o': 'Transverse Mercator',\n",
    "    'Datum': 'SIRGAS 2000',\n",
    "    'Unidade': 'metros',\n",
    "    'Zona UTM': '23S',\n",
    "    'Meridiano Central': '-45',\n",
    "    'Latitude de Origem': '0',\n",
    "    'Falso Leste': '500000',\n",
    "    'Falso Norte': '10000000',\n",
    "    'Fator de Escala': '0.9996'\n",
    "}\n",
    "\n",
    "print(\"Sistema de coordenadas alvo:\")\n",
    "for key, value in sirgas_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nEste sistema de coordenadas √© ideal para o nosso projeto porque:\")\n",
    "print(\"1. Mant√©m a precis√£o das medi√ß√µes em metros para a √°rea de estudo\")\n",
    "print(\"2. √â o sistema padr√£o para projetos geoespaciais no Brasil, especialmente na zona 23S\")\n",
    "print(\"3. Permite calcular √°reas, dist√¢ncias e an√°lises espaciais com precis√£o\")\n",
    "print(\"4. Minimiza distor√ß√µes para a regi√£o de estudo\")\n",
    "# Fun√ß√£o para verificar e tabular os sistemas de coordenadas atuais de todos os datasets\n",
    "def check_crs(datasets):\n",
    "    \"\"\"\n",
    "    Verifica os sistemas de coordenadas de todos os datasets.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dicion√°rio de datasets carregados\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com informa√ß√µes sobre os sistemas de coordenadas\n",
    "    \"\"\"\n",
    "    crs_info = []\n",
    "    \n",
    "    for file_name, layers in datasets.items():\n",
    "        for layer_name, gdf in layers.items():\n",
    "            try:\n",
    "                # Obter informa√ß√µes do CRS\n",
    "                if gdf.crs is None:\n",
    "                    crs_code = None\n",
    "                    crs_name = \"N√£o definido\"\n",
    "                    is_projected = False\n",
    "                    units = \"Desconhecido\"\n",
    "                else:\n",
    "                    crs_code = gdf.crs.to_epsg()\n",
    "                    crs_name = gdf.crs.name if hasattr(gdf.crs, 'name') else str(gdf.crs)\n",
    "                    is_projected = gdf.crs.is_projected\n",
    "                    units = gdf.crs.axis_info[0].unit_name if hasattr(gdf.crs, 'axis_info') else \"Desconhecido\"\n",
    "                \n",
    "                needs_transformation = crs_code != 31983 and crs_code is not None\n",
    "                \n",
    "                crs_info.append({\n",
    "                    'Arquivo': file_name,\n",
    "                    'Camada': layer_name,\n",
    "                    'EPSG': crs_code,\n",
    "                    'CRS Nome': crs_name,\n",
    "                    'Projetado': is_projected,\n",
    "                    'Unidades': units,\n",
    "                    'Requer Transforma√ß√£o': needs_transformation\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao verificar CRS de {file_name} - {layer_name}: {e}\")\n",
    "                crs_info.append({\n",
    "                    'Arquivo': file_name,\n",
    "                    'Camada': layer_name,\n",
    "                    'EPSG': None,\n",
    "                    'CRS Nome': f\"ERRO: {str(e)}\",\n",
    "                    'Projetado': None,\n",
    "                    'Unidades': None,\n",
    "                    'Requer Transforma√ß√£o': None\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(crs_info)\n",
    "\n",
    "# Analisando os sistemas de coordenadas atuais\n",
    "crs_status = check_crs(datasets)\n",
    "display(crs_status)\n",
    "\n",
    "# Contabilizando quantos datasets precisam de transforma√ß√£o\n",
    "requires_transformation = crs_status['Requer Transforma√ß√£o'].sum()\n",
    "total_layers = len(crs_status)\n",
    "\n",
    "print(f\"\\nStatus de transforma√ß√£o: {requires_transformation} de {total_layers} camadas precisam ser transformadas para SIRGAS 2000 / UTM zone 23S (EPSG:31983)\")\n",
    "# Fun√ß√£o para transformar datasets para o CRS alvo\n",
    "def transform_datasets(datasets, target_crs=TARGET_CRS):\n",
    "    \"\"\"\n",
    "    Transforma todos os datasets para o sistema de coordenadas alvo.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dicion√°rio de datasets carregados\n",
    "        target_crs: Sistema de coordenadas alvo\n",
    "    \n",
    "    Returns:\n",
    "        Dicion√°rio de datasets transformados\n",
    "    \"\"\"\n",
    "    transformed_datasets = {}\n",
    "    transformation_report = []\n",
    "    \n",
    "    # Para cada arquivo\n",
    "    for file_name, layers in tqdm(datasets.items(), desc=\"Transformando arquivos\"):\n",
    "        transformed_datasets[file_name] = {}\n",
    "        \n",
    "        # Para cada camada\n",
    "        for layer_name, gdf in tqdm(layers.items(), desc=f\"Camadas em {file_name}\", leave=False):\n",
    "            try:\n",
    "                # Verificar se o CRS atual √© None\n",
    "                if gdf.crs is None:\n",
    "                    print(f\"AVISO: {file_name} - {layer_name} n√£o possui CRS definido. Atribuindo o CRS alvo sem transforma√ß√£o.\")\n",
    "                    gdf.crs = target_crs\n",
    "                    transformed_gdf = gdf\n",
    "                    status = \"Atribu√≠do CRS (sem transforma√ß√£o)\"\n",
    "                \n",
    "                # Verificar se j√° est√° no CRS alvo\n",
    "                elif gdf.crs.to_epsg() == 31983:\n",
    "                    print(f\"{file_name} - {layer_name} j√° est√° no CRS alvo. Nenhuma transforma√ß√£o necess√°ria.\")\n",
    "                    transformed_gdf = gdf\n",
    "                    status = \"J√° no CRS alvo\"\n",
    "                \n",
    "                # Realizar a transforma√ß√£o\n",
    "                else:\n",
    "                    # Registrar informa√ß√µes antes da transforma√ß√£o\n",
    "                    if 'geometry' in gdf.columns and len(gdf) > 0 and not gdf.geometry.is_empty.all():\n",
    "                        pre_bounds = gdf.total_bounds\n",
    "                    else:\n",
    "                        pre_bounds = None\n",
    "                    \n",
    "                    # Executar a transforma√ß√£o\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                        transformed_gdf = gdf.to_crs(target_crs)\n",
    "                    \n",
    "                    # Registrar informa√ß√µes ap√≥s a transforma√ß√£o\n",
    "                    if pre_bounds is not None and len(transformed_gdf) > 0 and not transformed_gdf.geometry.is_empty.all():\n",
    "                        post_bounds = transformed_gdf.total_bounds\n",
    "                        status = \"Transformado com sucesso\"\n",
    "                    else:\n",
    "                        post_bounds = None\n",
    "                        status = \"Transformado (sem geometria para validar)\"\n",
    "                \n",
    "                # Armazenar o GeoDataFrame transformado\n",
    "                transformed_datasets[file_name][layer_name] = transformed_gdf\n",
    "                \n",
    "                # Registrar no relat√≥rio\n",
    "                transformation_report.append({\n",
    "                    'Arquivo': file_name,\n",
    "                    'Camada': layer_name,\n",
    "                    'CRS Original': str(gdf.crs),\n",
    "                    'CRS Final': str(transformed_gdf.crs),\n",
    "                    'Status': status,\n",
    "                    'Registros': len(gdf)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERRO ao transformar {file_name} - {layer_name}: {e}\")\n",
    "                # Manter o dataset original em caso de erro\n",
    "                transformed_datasets[file_name][layer_name] = gdf\n",
    "                transformation_report.append({\n",
    "                    'Arquivo': file_name,\n",
    "                    'Camada': layer_name,\n",
    "                    'CRS Original': str(gdf.crs) if hasattr(gdf, 'crs') else \"Desconhecido\",\n",
    "                    'CRS Final': \"ERRO\",\n",
    "                    'Status': f\"Erro: {str(e)}\",\n",
    "                    'Registros': len(gdf) if hasattr(gdf, 'len') else \"Desconhecido\" \n",
    "                })\n",
    "    \n",
    "    return transformed_datasets, pd.DataFrame(transformation_report)\n",
    "\n",
    "# Executando a transforma√ß√£o\n",
    "transformed_datasets, transformation_report = transform_datasets(datasets)\n",
    "\n",
    "# Atualizando a vari√°vel global com os datasets transformados\n",
    "builtins.datasets = transformed_datasets\n",
    "\n",
    "# Exibindo o relat√≥rio de transforma√ß√£o\n",
    "display(transformation_report)\n",
    "# Valida√ß√£o final: verificando se todos os datasets est√£o no sistema de coordenadas correto\n",
    "def validate_transformations(datasets, target_crs=TARGET_CRS):\n",
    "    \"\"\"\n",
    "    Verifica se todos os datasets est√£o no sistema de coordenadas alvo.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dicion√°rio de datasets transformados\n",
    "        target_crs: Sistema de coordenadas alvo\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com resultados da valida√ß√£o\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    target_epsg = 31983  # EPSG para SIRGAS 2000 / UTM zone 23S\n",
    "    \n",
    "    for file_name, layers in datasets.items():\n",
    "        for layer_name, gdf in layers.items():\n",
    "            try:\n",
    "                current_epsg = gdf.crs.to_epsg()\n",
    "                is_valid = current_epsg == target_epsg\n",
    "                \n",
    "                validation_results.append({\n",
    "                    'Arquivo': file_name,\n",
    "                    'Camada': layer_name,\n",
    "                    'EPSG Atual': current_epsg,\n",
    "                    'EPSG Alvo': target_epsg,\n",
    "                    'Valida√ß√£o': \"Sucesso\" if is_valid else \"Falha\",\n",
    "                    'CRS': str(gdf.crs)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                validation_results.append({\n",
    "                    'Arquivo': file_name,\n",
    "                    'Camada': layer_name,\n",
    "                    'EPSG Atual': None,\n",
    "                    'EPSG Alvo': target_epsg,\n",
    "                    'Valida√ß√£o': f\"Erro: {str(e)}\",\n",
    "                    'CRS': str(gdf.crs) if hasattr(gdf, 'crs') else \"Desconhecido\"\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(validation_results)\n",
    "\n",
    "# Validando todas as transforma√ß√µes\n",
    "validation_results = validate_transformations(transformed_datasets)\n",
    "display(validation_results)\n",
    "\n",
    "# Verificando sucesso geral\n",
    "success_count = (validation_results['Valida√ß√£o'] == \"Sucesso\").sum()\n",
    "total_count = len(validation_results)\n",
    "success_rate = success_count / total_count * 100\n",
    "\n",
    "print(f\"\\nValida√ß√£o conclu√≠da: {success_count} de {total_count} camadas ({success_rate:.2f}%) est√£o no sistema de coordenadas SIRGAS 2000 / UTM zone 23S (EPSG:31983)\")\n",
    "\n",
    "if success_count < total_count:\n",
    "    print(\"\\nATEN√á√ÉO: Algumas camadas n√£o foram transformadas corretamente. Verifique o relat√≥rio acima.\")\n",
    "else:\n",
    "    print(\"\\nTodos os datasets foram transformados com sucesso para o sistema de coordenadas alvo!\")\n",
    "# Salvando os datasets transformados\n",
    "import pickle\n",
    "\n",
    "# Criando pasta de estado se n√£o existir\n",
    "state_dir = os.path.join(data_dir, 'state')\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "\n",
    "# Caminho para o arquivo de estado\n",
    "transformed_state_file = os.path.join(state_dir, 'datasets_transformed_state.pkl')\n",
    "\n",
    "# Salvando o dicion√°rio de datasets transformados\n",
    "with open(transformed_state_file, 'wb') as f:\n",
    "    pickle.dump(transformed_datasets, f)\n",
    "\n",
    "print(f\"Estado dos datasets transformados salvo em: {transformed_state_file}\")\n",
    "# Visualizando algumas fei√ß√µes para verificar visualmente a transforma√ß√£o\n",
    "def plot_sample_geometries(datasets):\n",
    "    \"\"\"\n",
    "    Plota amostras de geometrias para verifica√ß√£o visual.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dicion√°rio de datasets transformados\n",
    "    \"\"\"\n",
    "    # Criar um layout de 2x2 para visualiza√ß√£o\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Contador para controlar quantas camadas plotar\n",
    "    plot_count = 0\n",
    "    max_plots = 4\n",
    "    \n",
    "    # Cores para diferentes datasets\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    # Para cada arquivo e camada\n",
    "    for file_name, layers in datasets.items():\n",
    "        for layer_name, gdf in layers.items():\n",
    "            if plot_count >= max_plots:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Verificar se h√° geometrias para plotar\n",
    "                if 'geometry' not in gdf.columns or len(gdf) == 0 or gdf.geometry.is_empty.all():\n",
    "                    continue\n",
    "                    \n",
    "                # Plotar apenas uma amostra (m√°ximo 100 fei√ß√µes)\n",
    "                sample_size = min(100, len(gdf))\n",
    "                ax = axes[plot_count]\n",
    "                \n",
    "                # Plotar a amostra\n",
    "                gdf.sample(sample_size).plot(\n",
    "                    ax=ax, \n",
    "                    color=colors[plot_count % len(colors)],\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=0.5\n",
    "                )\n",
    "                \n",
    "                # Configurar o t√≠tulo e r√≥tulos\n",
    "                ax.set_title(f\"{file_name}\\n{layer_name}\", fontsize=12)\n",
    "                ax.set_xlabel(\"Coordenada X (metros)\")\n",
    "                ax.set_ylabel(\"Coordenada Y (metros)\")\n",
    "                ax.grid(True)\n",
    "                \n",
    "                # Adicionar informa√ß√µes de CRS\n",
    "                ax.text(0.5, -0.1, f\"CRS: {gdf.crs.name if hasattr(gdf.crs, 'name') else str(gdf.crs)[:50]}...\",\n",
    "                       horizontalalignment='center', verticalalignment='center',\n",
    "                       transform=ax.transAxes, fontsize=10)\n",
    "                \n",
    "                plot_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao plotar {file_name} - {layer_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if plot_count == 0:\n",
    "        print(\"Nenhuma geometria dispon√≠vel para visualiza√ß√£o.\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "    \n",
    "    # Ocultar eixos n√£o utilizados\n",
    "    for i in range(plot_count, max_plots):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizando amostras das geometrias transformadas\n",
    "plot_sample_geometries(transformed_datasets)\n",
    "# Resumo e conclus√£o\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMO DA CONFIGURA√á√ÉO DE SISTEMA DE COORDENADAS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSistema de coordenadas alvo: SIRGAS 2000 / UTM zone 23S (EPSG:31983)\")\n",
    "print(f\"Total de arquivos processados: {len(transformed_datasets)}\")\n",
    "print(f\"Total de camadas processadas: {total_count}\")\n",
    "print(f\"Camadas transformadas com sucesso: {success_count} ({success_rate:.2f}%)\")\n",
    "\n",
    "print(\"\\nBenef√≠cios da padroniza√ß√£o:\")\n",
    "print(\"1. Todas as an√°lises espaciais agora usar√£o o mesmo sistema de refer√™ncia\")\n",
    "print(\"2. C√°lculos de √°rea, dist√¢ncia e proximidade ser√£o precisos e consistentes\")\n",
    "print(\"3. Visualiza√ß√µes de mapas estar√£o corretamente alinhadas\")\n",
    "print(\"4. Opera√ß√µes de sobreposi√ß√£o espacial funcionar√£o corretamente\")\n",
    "\n",
    "print(\"\\nPr√≥ximos passos:\")\n",
    "print(\"1. Prosseguir para a integra√ß√£o de Edif√≠cios e Uso do Solo\")\n",
    "print(\"2. Criar an√°lises demogr√°ficas usando os datasets transformados\")\n",
    "print(\"3. Realizar as opera√ß√µes hidrogr√°ficas com os dados padronizados\")\n",
    "\n",
    "print(\"\\nOs datasets transformados est√£o dispon√≠veis na vari√°vel global 'datasets'\")\n",
    "print(\"=\"*80)\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
