# -*- coding: utf-8 -*-
"""mba.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GYnvFLQJk2NCMMKrG4zJu5J7VpNpH0Rj
"""

# prompt: me de o comando para carregar o google drive

from google.colab import drive
drive.mount('/content/drive')

!pip install -U numpy geopandas fiona folium geopandas h3 contextily basemap

!apt-get install libgeos-dev
!pip install basemap

# Instalar a versão estável do PyTorch
!pip install torch==1.13.1 torchvision==0.14.1 -f https://download.pytorch.org/whl/cpu

# Agora instalar o PyTorch Geometric e suas extensões
!pip install torch-geometric
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-1.13.1+cpu.html

import pandas as pd

# Caminho dos arquivos
caminho_entrada = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/csv_licenciamento_c55c1dea01bd184e27df233da8ac28a2.csv'
caminho_saida = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_sorocaba.csv'

# Ler o arquivo CSV original
df = pd.read_csv(caminho_entrada, encoding='utf-8')

# Listar as colunas a serem excluídas
colunas_para_excluir = [
    'NumFistel', 'NumServico', 'NumAto', 'CodDebitoTFI', '_id',
    'NumFistelAssociado', 'NumRede', 'NumEstacao',
    'DataLicenciamento', 'DataPrimeiroLicenciamento', 'DataValidade',
    'CodTipoAntena', 'CodEquipamentoAntena', 'CodEquipamentoTransmissor',
    'CodTipoClasseEstacao', 'DesignacaoEmissao', 'ClassInfraFisica',
    'CompartilhamentoInfraFisica', 'NomeEntidadeAssociado',
    'FrenteCostaAntena', 'AnguloMeiaPotenciaAntena'
]

# Verificar se todas as colunas existem no DataFrame
colunas_existentes = [col for col in colunas_para_excluir if col in df.columns]
colunas_nao_encontradas = set(colunas_para_excluir) - set(colunas_existentes)

if colunas_nao_encontradas:
    print(f"Aviso: As seguintes colunas não foram encontradas no dataset: {colunas_nao_encontradas}")

# Remover as colunas
df_filtrado = df.drop(columns=colunas_existentes)

# Salvar o novo CSV
df_filtrado.to_csv(caminho_saida, index=False, encoding='utf-8')

print(f"Arquivo salvo com sucesso como {caminho_saida}")
print(f"Colunas mantidas: {list(df_filtrado.columns)}")
print(f"Número de ERBs no dataset: {len(df_filtrado)}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.basemap import Basemap
import folium
from folium.plugins import HeatMap, MarkerCluster
from sklearn.cluster import DBSCAN
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Configurações para visualização
plt.style.use('ggplot')
sns.set(style="whitegrid")
pd.set_option('display.max_columns', None)
plt.rcParams['figure.figsize'] = (12, 8)

# Carregamento dos dados
file_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_sorocaba.csv'
df = pd.read_csv(file_path, encoding='utf-8')

def analise_completa_erbs(df):
    """Função para realizar análise completa do dataset de ERBs."""

    print("=" * 80)
    print("ANÁLISE EXPLORATÓRIA COMPLETA - ESTAÇÕES RÁDIO BASE PARA SISTEMA DE EVACUAÇÃO")
    print("=" * 80)

    # 1. VISÃO GERAL DOS DADOS
    print("\n1. VISÃO GERAL DOS DADOS")
    print("-" * 50)
    print(f"Total de estações: {df.shape[0]}")
    print(f"Variáveis disponíveis: {df.shape[1]}")
    print("\nPrimeiras linhas do dataset:")
    print(df.head())

    # Verificar tipos de dados
    print("\nTipos de dados:")
    print(df.dtypes)

    # 2. ANÁLISE DE VALORES AUSENTES
    print("\n\n2. ANÁLISE DE VALORES AUSENTES")
    print("-" * 50)
    missing_values = df.isnull().sum()
    missing_percent = (missing_values / len(df)) * 100
    missing_data = pd.DataFrame({
        'Valores ausentes': missing_values,
        'Percentual (%)': missing_percent.round(2)
    })
    print(missing_data[missing_data['Valores ausentes'] > 0].sort_values('Percentual (%)', ascending=False))

    # 3. ESTATÍSTICAS DESCRITIVAS
    print("\n\n3. ESTATÍSTICAS DESCRITIVAS")
    print("-" * 50)

    # Tratamento de dados numéricos (converter colunas que deveriam ser numéricas)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # Tratar potência que pode estar como string
    if 'PotenciaTransmissorWatts' in df.columns and df['PotenciaTransmissorWatts'].dtype == 'object':
        try:
            df['PotenciaTransmissorWatts'] = df['PotenciaTransmissorWatts'].str.replace(',', '.').astype(float)
            numeric_cols.append('PotenciaTransmissorWatts')
        except:
            print("Não foi possível converter PotenciaTransmissorWatts para float")

    # Estatísticas para colunas numéricas
    print("\nEstatísticas de variáveis numéricas relevantes:")
    numeric_stats = df[numeric_cols].describe().T
    # Adicionar mediana e variância
    numeric_stats['median'] = df[numeric_cols].median()
    numeric_stats['variance'] = df[numeric_cols].var()
    print(numeric_stats)

    # 4. ANÁLISE GEOGRÁFICA
    print("\n\n4. ANÁLISE GEOGRÁFICA")
    print("-" * 50)

    # Verificar amplitude das coordenadas
    if 'Latitude' in df.columns and 'Longitude' in df.columns:
        print(f"Amplitude de latitude: {df['Latitude'].min()} a {df['Latitude'].max()}")
        print(f"Amplitude de longitude: {df['Longitude'].min()} a {df['Longitude'].max()}")

        # Contagem de ERBs por município
        if 'Municipio.NomeMunicipio' in df.columns:
            print("\nDistribuição de ERBs por município:")
            municipio_counts = df['Municipio.NomeMunicipio'].value_counts()
            print(municipio_counts.head(10))

    # 5. ANÁLISE TECNOLÓGICA
    print("\n\n5. ANÁLISE TECNOLÓGICA")
    print("-" * 50)

    # Tecnologias utilizadas
    if 'Tecnologia' in df.columns:
        print("\nDistribuição de tecnologias:")
        tech_counts = df['Tecnologia'].value_counts()
        print(tech_counts)

    if 'tipoTecnologia' in df.columns:
        print("\nDistribuição de tipos de tecnologia:")
        tech_type_counts = df['tipoTecnologia'].value_counts()
        print(tech_type_counts)

    # 6. ANÁLISE DE FAIXAS DE FREQUÊNCIA
    print("\n\n6. ANÁLISE DE FAIXAS DE FREQUÊNCIA")
    print("-" * 50)

    if 'FreqTxMHz' in df.columns:
        print(f"\nFaixa de frequência de transmissão: {df['FreqTxMHz'].min()} MHz a {df['FreqTxMHz'].max()} MHz")

    if 'FreqRxMHz' in df.columns:
        print(f"Faixa de frequência de recepção: {df['FreqRxMHz'].min()} MHz a {df['FreqRxMHz'].max()} MHz")

    # 7. ANÁLISE POR OPERADORA
    print("\n\n7. ANÁLISE POR OPERADORA")
    print("-" * 50)

    if 'NomeEntidade' in df.columns:
        print("\nDistribuição de ERBs por operadora:")
        operator_counts = df['NomeEntidade'].value_counts()
        print(operator_counts.head(10))

    # 8. RETORNO DE VISUALIZAÇÕES PARA ANÁLISE COMPLETA
    print("\n\n8. CRIANDO VISUALIZAÇÕES")
    print("-" * 50)
    print("Gerando gráficos para análise avançada...")

    # Criar uma figura com vários subplots
    fig = plt.figure(figsize=(20, 20))

    # 8.1 Distribuição geográfica (mapa)
    if 'Latitude' in df.columns and 'Longitude' in df.columns:
        # Remover valores nulos ou inválidos
        geo_df = df.dropna(subset=['Latitude', 'Longitude'])
        geo_df = geo_df[(geo_df['Latitude'] != 0) & (geo_df['Longitude'] != 0)]

        try:
            # Criar mapa folium
            m = folium.Map(
                location=[geo_df['Latitude'].mean(), geo_df['Longitude'].mean()],
                zoom_start=10
            )

            # Adicionar cluster de marcadores
            marker_cluster = MarkerCluster().add_to(m)

            # Adicionar marcadores para cada ERB
            for idx, row in geo_df.iterrows():
                popup_text = f"""
                <b>Estação:</b> {row.get('NomeEntidade', 'N/A')}<br>
                <b>Tecnologia:</b> {row.get('Tecnologia', 'N/A')}<br>
                <b>Freq. Tx:</b> {row.get('FreqTxMHz', 'N/A')} MHz<br>
                <b>Altura:</b> {row.get('AlturaAntena', 'N/A')} m<br>
                """

                folium.Marker(
                    location=[row['Latitude'], row['Longitude']],
                    popup=folium.Popup(popup_text, max_width=300),
                    tooltip=f"{row.get('Tecnologia', 'ERB')}"
                ).add_to(marker_cluster)

            # Salvar mapa como HTML
            mapa_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mapa_erbs_sorocaba.html'
            m.save(mapa_file)
            print(f"Mapa interativo salvo em: {mapa_file}")

        except Exception as e:
            print(f"Erro ao gerar mapa: {e}")

    # Gráficos estáticos
    plt.figure(figsize=(18, 14))

    # 8.2 Histograma de alturas de antenas
    if 'AlturaAntena' in df.columns:
        plt.subplot(2, 2, 1)
        sns.histplot(df['AlturaAntena'].dropna(), bins=30, kde=True)
        plt.title('Distribuição de Altura das Antenas')
        plt.xlabel('Altura (m)')
        plt.ylabel('Frequência')

    # 8.3 Gráfico de barras de tecnologias
    if 'Tecnologia' in df.columns:
        plt.subplot(2, 2, 2)
        tech_counts = df['Tecnologia'].value_counts()
        sns.barplot(x=tech_counts.index, y=tech_counts.values)
        plt.title('Distribuição de Tecnologias')
        plt.xlabel('Tecnologia')
        plt.ylabel('Quantidade')
        plt.xticks(rotation=45)

    # 8.4 Gráfico de dispersão Ganho x Potência
    if 'GanhoAntena' in df.columns and 'PotenciaTransmissorWatts' in df.columns:
        plt.subplot(2, 2, 3)
        sns.scatterplot(data=df, x='GanhoAntena', y='PotenciaTransmissorWatts',
                        hue='Tecnologia' if 'Tecnologia' in df.columns else None,
                        alpha=0.7)
        plt.title('Relação entre Ganho e Potência')
        plt.xlabel('Ganho da Antena (dBi)')
        plt.ylabel('Potência do Transmissor (W)')

    # 8.5 Box plot de frequências por tecnologia
    if 'FreqTxMHz' in df.columns and 'Tecnologia' in df.columns:
        plt.subplot(2, 2, 4)
        sns.boxplot(x='Tecnologia', y='FreqTxMHz', data=df)
        plt.title('Distribuição de Frequências por Tecnologia')
        plt.xlabel('Tecnologia')
        plt.ylabel('Frequência (MHz)')
        plt.xticks(rotation=45)

    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/MBA/analise_erbs_sorocaba.png', dpi=300)
    print("Gráficos de análise salvos em: /content/drive/MyDrive/GrafosGeoespaciais/MBA/analise_erbs_sorocaba.png")

    # 9. ANÁLISE DE CLUSTERS GEOGRÁFICOS COM DBSCAN
    print("\n\n9. ANÁLISE DE CLUSTERS GEOGRÁFICOS")
    print("-" * 50)

    if 'Latitude' in df.columns and 'Longitude' in df.columns:
        # Remover valores nulos ou inválidos
        geo_df = df.dropna(subset=['Latitude', 'Longitude'])
        geo_df = geo_df[(geo_df['Latitude'] != 0) & (geo_df['Longitude'] != 0)]

        try:
            # Preparar dados para DBSCAN
            coords = geo_df[['Latitude', 'Longitude']].values

            # Converter graus para radianos
            earth_radius = 6371  # km
            coords_rad = np.radians(coords)

            # Calcular distância em km para eps (aprox. 1km)
            eps_rad = 1 / earth_radius

            # Aplicar DBSCAN
            db = DBSCAN(eps=eps_rad, min_samples=5, algorithm='ball_tree', metric='haversine')
            geo_df['cluster'] = db.fit_predict(coords_rad)

            # Contar clusters
            n_clusters = len(set(geo_df['cluster'])) - (1 if -1 in geo_df['cluster'] else 0)
            print(f"Número de clusters identificados: {n_clusters}")
            print(f"Estações isoladas (não pertencentes a clusters): {list(geo_df['cluster']).count(-1)}")

            # Salvar dados com clusters
            geo_df.to_csv('/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_sorocaba_clusters.csv', index=False)
            print("Dados com clusters geográficos salvos em: /content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_sorocaba_clusters.csv")

        except Exception as e:
            print(f"Erro na análise de clusters: {e}")

    # 10. CONCLUSÕES E RECOMENDAÇÕES PARA O PROJETO DE EVACUAÇÃO
    print("\n\n10. CONCLUSÕES E RECOMENDAÇÕES")
    print("-" * 50)
    print("""
    Com base na análise dos dados, recomendamos:

    1. COBERTURA GEOGRÁFICA:
       - As ERBs apresentam concentração em determinadas regiões, criando pontos de densidade
         que devem ser considerados ao modelar a rede de evacuação.

    2. TECNOLOGIA E ALCANCE:
       - Diversificar as mensagens por tecnologia, considerando alcances diferentes de ERBs
         2G/3G/4G/5G para garantir cobertura abrangente durante evacuações.

    3. MODELAGEM GNN:
       - Utilizar as variáveis de potência, ganho, altura e azimute para calcular raios
         de cobertura realistas na construção dos nós do grafo.
       - Estabelecer pesos nas arestas baseados em densidade populacional estimada.

    4. PREPARAÇÃO DOS DADOS:
       - Completar valores ausentes nas variáveis críticas (altura, ganho, potência)
         com médias por tipo de tecnologia.
       - Remover outliers extremos que podem comprometer a modelagem.

    5. OTIMIZAÇÃO DO SISTEMA DE EVACUAÇÃO:
       - Nos clusters identificados, selecionar ERBs estratégicas para disseminação
         inicial de alertas, maximizando a cobertura com número mínimo de mensagens.
    """)

    return {
        "total_estacoes": df.shape[0],
        "arquivo_mapa": '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mapa_erbs_sorocaba.html',
        "arquivo_graficos": '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analise_erbs_sorocaba.png',
        "arquivo_clusters": '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_sorocaba_clusters.csv'
    }

# Executar análise completa
resultados = analise_completa_erbs(df)

"""Análise e Enriquecimento dos Dados ERB de Sorocaba
Parâmetros Relevantes para Estimativa de Cobertura
Analisando o arquivo erb_sorocaba.csv, identifico que você possui dados valiosos sobre as Estações Rádio Base de Sorocaba que podem ser enriquecidos com cálculos de cobertura e alcance. Estes dados são fundamentais para sua proposta de integração com grafos geoespaciais. Os parâmetros mais relevantes incluem:
Dados Disponíveis para Cálculo de Cobertura

PotenciaTransmissorWatts: Determina a força do sinal emitido
GanhoAntena: Amplificação direcional do sinal (em dBi)
FreqTxMHz: Frequência de transmissão que influencia diretamente no alcance
AlturaAntena: Afeta significativamente a área de cobertura
Azimute: Direção principal de propagação do sinal
AnguloElevacao: Inclinação vertical da antena
Polarizacao: Orientação da onda eletromagnética

Enriquecimento dos Dados com Cálculos de Cobertura
Para calcular o alcance e cobertura sem obstáculos, você pode adicionar as seguintes informações derivadas:
1. Potência Efetivamente Irradiada (EIRP)
A EIRP combina a potência do transmissor com o ganho da antena:
EIRP (dBm) = 10 × log₁₀(PotenciaTransmissorWatts × 1000) + GanhoAntena
Este valor fornece a potência efetiva considerando a direcionalidade da antena.
2. Raio de Cobertura Teórico (Modelo de Espaço Livre)
Para cada ERB, você pode estimar o raio de cobertura em espaço livre usando o modelo de Friis:
Raio (km) = 10^((EIRP - Sensibilidade - 32.44 - 20 × log₁₀(FreqTxMHz)) / 20)
Onde:

Sensibilidade é o valor mínimo para recepção (tipicamente -100 dBm para celulares)
32.44 é um fator constante para harmonização de unidades
O termo com frequência representa a atenuação no espaço livre

3. Área de Cobertura Setorial
Considerando o azimute e o padrão de radiação típico para antenas setoriais:

Setores de 120° (típico em áreas urbanas)
Área de Cobertura (km²) = (π × Raio² × Ângulo_Setor) / 360

4. Índice de Cobertura Populacional
Ao correlacionar com dados censitários por setor:

População Coberta = Σ (População_Setor × Percentual_Área_Coberta)
Índice de Cobertura = População_Coberta / População_Total

Informações Adicionais Relevantes para o Projeto
1. Classificação de ERBs por Importância Estratégica
Sugiro criar uma classificação baseada em:

Capacidade de Cobertura: ERBs que cobrem maior população
Redundância na Rede: ERBs com sobreposição de cobertura
Criticidade Infraestrutural: Proximidade a instalações essenciais (hospitais, centros de comando)
Robustez a Falhas: Presença de sistemas de energia de emergência

2. Atributos de Topografia e Ambiente Urbano
Integrar dados que afetam a propagação real:

Modelo Digital de Elevação (MDE): Para cálculo de visada direta
Densidade Construtiva: Áreas com edifícios altos têm maior atenuação
Classificação de Uso do Solo: Diferentes superfícies afetam a propagação
Vegetação: Áreas densamente arborizadas impactam o sinal

3. Métricas de Conectividade para o Grafo
Para cada ERB, calcular:

Grau de Centralidade: Baseado na sobreposição com outras células
Criticidade de Fluxo: Impacto da desativação na cobertura geral
Índice de Resiliência: Capacidade de manter cobertura em caso de falhas
Pontuação de Prioridade em Evacuação: Combinação ponderada dos fatores acima

Abordagem Metodológica sem Código
Como você solicitou trabalhar sem código, recomendo esta abordagem usando ferramentas de análise de dados:

Extração e Uniformização de Dados:

Criar uma planilha derivada com valores padronizados
Converter unidades divergentes (dBm, Watts, etc.)
Normalizar coordenadas geográficas para um sistema consistente


Cálculos de Propagação:

Usar fórmulas em planilha para os cálculos de EIRP e raio
Criar tabelas de referência para fatores de correção típicos
Documentar premissas e limitações dos modelos utilizados


Integração com PITCIC:

Classificar ERBs conforme categorias do framework (infraestrutura crítica)
Mapear zonas de cobertura como camada adicional no grafo
Estabelecer regras de conectividade entre ERBs e demais elementos


Visualização e Validação:

Solicitar exportação para formato tabular compatível com QGIS
Validar estimativas de cobertura com dados reais, se disponíveis
Documentar discrepâncias e ajustar modelos iterativamente



Considerações para Modelagem com GNN
Para preparar estes dados para seu modelo de GNN, considere:

Representação dos Nós: Cada ERB como nó, com vetor de características incluindo todos os parâmetros calculados
Definição de Arestas: Conexões baseadas em sobreposição de cobertura ou proximidade espacial
Atributos das Arestas: Força da conectividade, redundância, robustez compartilhada
Camadas do Grafo: Integração da camada ERB com demais camadas do PITCIC

Este processamento textual dos dados permitirá criar uma base sólida para a implementação posterior do modelo GNN, mantendo o rigor metodológico necessário para sua pesquisa de MBA.
Gostaria que eu aprofundasse algum aspecto específico desta análise?

# Guia para Cálculo e Visualização da Cobertura de ERBs em Sorocaba

Para realizar os cálculos de cobertura das ERBs e gerar as visualizações solicitadas por operadora (Claro, Oi, Vivo e Tim), siga este guia passo a passo. Vou detalhar o processo completo usando ferramentas de análise de dados e GIS que são compatíveis com seu projeto.

## Etapa 1: Filtrar e Preparar os Dados

Primeiro, você precisará filtrar o arquivo erb_sorocaba.csv para incluir apenas as quatro operadoras de interesse:

1. Abra o arquivo em uma ferramenta como Excel, Google Sheets ou utilize o pandas em Python.
2. Filtre a coluna "NomeEntidade" para manter apenas: Claro, Oi, Vivo e Tim.
3. Verifique a consistência dos dados nas colunas necessárias para os cálculos:
   - PotenciaTransmissorWatts (converta para valor numérico se necessário)
   - GanhoAntena (em dBi)
   - FreqTxMHz
   - Azimute
   - Latitude e Longitude

## Etapa 2: Calcular a Potência Efetivamente Irradiada (EIRP)

Para cada linha do seu conjunto de dados filtrado:

1. Converta a potência do transmissor para um valor numérico (se estiver como string)
2. Aplique a fórmula:
   ```
   EIRP (dBm) = 10 × log₁₀(PotenciaTransmissorWatts × 1000) + GanhoAntena
   ```
3. Adicione uma nova coluna chamada "EIRP_dBm" com os resultados

Por exemplo, uma ERB com potência de 20 Watts e ganho de antena de 16 dBi teria:
- EIRP = 10 × log₁₀(20 × 1000) + 16 = 10 × log₁₀(20000) + 16 = 10 × 4.301 + 16 = 43.01 + 16 = 59.01 dBm

## Etapa 3: Calcular o Raio de Cobertura Teórico

Com os valores de EIRP calculados:

1. Defina o valor de sensibilidade (tipicamente -100 dBm para dispositivos móveis)
2. Aplique a fórmula de Friis para cada linha:
   ```
   Raio (km) = 10^((EIRP - Sensibilidade - 32.44 - 20 × log₁₀(FreqTxMHz)) / 20)
   ```
3. Adicione uma nova coluna "Raio_Cobertura_km"

Continuando o exemplo anterior, se a frequência for 850 MHz:
- Raio = 10^((59.01 - (-100) - 32.44 - 20 × log₁₀(850)) / 20)
- Raio = 10^((59.01 + 100 - 32.44 - 20 × 2.929) / 20)
- Raio = 10^((159.01 - 32.44 - 58.58) / 20)
- Raio = 10^(67.99 / 20)
- Raio = 10^3.3995
- Raio ≈ 2.51 km

## Etapa 4: Calcular a Área de Cobertura Setorial

Para cada ERB:

1. Determine o ângulo de setor para cada antena (tipicamente 120° em áreas urbanas)
2. Aplique a fórmula:
   ```
   Área de Cobertura (km²) = (π × Raio² × Ângulo_Setor) / 360
   ```
3. Adicione uma nova coluna "Area_Cobertura_km2"

Para nosso exemplo com raio de 2.51 km e setor de 120°:
- Área = (3.14159 × 2.51² × 120) / 360
- Área = (3.14159 × 6.3001 × 120) / 360
- Área = (3.14159 × 6.3001 × 0.333)
- Área ≈ 6.59 km²

## Etapa 5: Criação de Visualizações por Operadora

### Preparando os Dados para Visualização em GIS (QGIS)

1. Exporte os dados processados para formato CSV
2. Crie arquivos separados para cada operadora (Claro, Oi, Vivo, Tim)
3. Importe no QGIS como camadas de pontos, usando Latitude e Longitude

### Mapas de Posicionamento das ERBs

1. No QGIS, carregue uma camada base do OpenStreetMap para Sorocaba
2. Simbolize cada operadora com uma cor distinta:
   - Claro: vermelho
   - Oi: amarelo
   - Vivo: roxo
   - Tim: azul
3. Ajuste o tamanho dos pontos proporcionalmente à potência (EIRP)
4. Crie uma legenda clara identificando cada operadora
5. Exporte como "Mapa_Posicionamento_ERBs_Sorocaba.png"

### Visualização de Cobertura por Operadora

Para cada operadora:

1. Use a ferramenta "Buffer" do QGIS para criar círculos ao redor de cada ponto, usando o valor da coluna "Raio_Cobertura_km"
2. Para visualizações mais precisas, ajuste o buffer conforme o azimute e o ângulo de setor:
   - Use ferramentas como "Wedge Buffers" ou scripts personalizados
   - Crie um setor de 120° orientado conforme o azimute de cada ERB
3. Dissolva os buffers sobrepostos para melhor visualização
4. Defina um esquema de cores semi-transparente para visualizar sobreposições
5. Exporte como "Mapa_Cobertura_[Operadora].png"

### Mapa de Calor de Cobertura

1. Crie um raster de densidade usando a ferramenta "Heatmap" (Mapa de calor) do QGIS
2. Configure o raio de influência com base no "Raio_Cobertura_km"
3. Use um gradiente de cores que vai do azul (baixa cobertura) ao vermelho (alta cobertura)
4. Crie mapas separados por operadora e um mapa combinado
5. Exporte como "Mapa_Calor_Cobertura_[Operadora].png" e "Mapa_Calor_Cobertura_Total.png"

### Visualização de Potência (EIRP)

1. Simbolize os pontos das ERBs usando uma rampa de cores baseada no valor de "EIRP_dBm"
2. Use uma escala de cores contínua do amarelo (menor potência) ao vermelho (maior potência)
3. Adicione rótulos com os valores exatos de EIRP para as ERBs principais
4. Crie mapas separados por operadora e um mapa comparativo
5. Exporte como "Mapa_Potencia_EIRP_[Operadora].png"

## Etapa 6: Análise Comparativa entre Operadoras

Para complementar suas visualizações, crie gráficos comparativos:

1. Número total de ERBs por operadora (gráfico de barras)
2. Distribuição de potência EIRP por operadora (gráfico de boxplot)
3. Área de cobertura total por operadora (gráfico de pizza)
4. Porcentagem de cobertura territorial de Sorocaba por operadora (gráfico de barras)

## Etapa 7: Integração com seu Modelo de GNN

Estes dados enriquecidos servirão como base para seu modelo de GNN:

1. Cada ERB será um nó no grafo, com atributos incluindo:
   - Coordenadas geográficas (latitude, longitude)
   - Operadora
   - EIRP calculado
   - Raio de cobertura
   - Área de cobertura
   - Azimute

2. As arestas entre nós podem representar:
   - Sobreposição de cobertura
   - Proximidade espacial
   - Complementaridade de serviço (ERBs de diferentes operadoras que cobrem a mesma área)

Esta estrutura de grafo enriquecida com os atributos calculados fornecerá uma base sólida para suas análises de GNN focadas na avaliação de riscos, otimização de cobertura e planejamento de contingências em cenários QBRN.

## Considerações Adicionais

1. **Limitações do Modelo**: O modelo de Friis assume propagação em espaço livre. Em áreas urbanas, considere aplicar fatores de correção para:
   - Atenuação por edificações (3-20 dB, dependendo da densidade)
   - Atenuação por vegetação (1-4 dB)
   - Efeitos de sombra causados por relevo

2. **Validação de Resultados**: Se possível, compare seus resultados teóricos com:
   - Mapas de cobertura oficiais das operadoras
   - Medições reais de sinal em pontos específicos
   - Modelos de propagação mais sofisticados como Okumura-Hata para áreas urbanas

3. **Interpretação dos Resultados**: Ao analisar os mapas de calor e cobertura, preste atenção especial a:
   - Áreas com múltipla cobertura (resistentes a falhas individuais)
   - Áreas com cobertura única (pontos de vulnerabilidade)
   - Zonas de sombra (sem cobertura adequada)

Esta metodologia fornecerá uma visão abrangente da cobertura de ERBs em Sorocaba, essencial para seu projeto de integração com o framework PITCIC e a modelagem com GNN.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point, Polygon, LineString, MultiPolygon
import folium
from folium.plugins import HeatMap, MarkerCluster
import math
import contextily as ctx
from matplotlib.colors import LinearSegmentedColormap, Normalize
from matplotlib.patches import Patch
from matplotlib.lines import Line2D
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import matplotlib.font_manager as fm
from matplotlib.ticker import MaxNLocator
import matplotlib.patheffects as PathEffects
from datetime import datetime
import os
from google.colab import drive
import warnings
from scipy.spatial import Voronoi, voronoi_plot_2d
from sklearn.preprocessing import MinMaxScaler
from scipy.interpolate import griddata
import networkx as nx
import h3
import rasterio
from rasterio.features import rasterize
from rasterio.transform import from_bounds

# Ignorar avisos
warnings.filterwarnings('ignore')

# Montar o Google Drive (caso esteja usando o Colab)
drive.mount('/content/drive', force_remount=True)

# Configurações visuais
plt.rcParams.update({
    'font.family': 'DejaVu Sans',
    'font.size': 11,
    'axes.titlesize': 16,
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 18,
    'figure.figsize': (12, 8),
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.2
})

# Diretório de saída
output_dir = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/visualizacoes'
os.makedirs(output_dir, exist_ok=True)

# Constantes
SENSIBILIDADE_RECEPTOR = -100  # dBm
ANGULO_SETOR = 120  # graus

# Dicionário de cores para operadoras
cores_operadoras = {
    'CLARO': '#E02020',
    'OI': '#FFD700',
    'VIVO': '#9932CC',
    'TIM': '#0000CD'
}

# Funções de cálculo de sinal e cobertura
def calcular_eirp(potencia_watts, ganho_antena):
    try:
        potencia = float(potencia_watts)
        if potencia <= 0:
            return np.nan
    except (ValueError, TypeError):
        return np.nan
    return 10 * np.log10(potencia * 1000) + ganho_antena

def calcular_raio_cobertura_aprimorado(eirp, freq_mhz, tipo_area='urbana'):
    if np.isnan(eirp) or np.isnan(freq_mhz) or freq_mhz <= 0:
        return np.nan
    atenuacao = {'urbana_densa': 22, 'urbana': 16, 'suburbana': 12, 'rural': 8}
    raio_base = 10 ** ((eirp - SENSIBILIDADE_RECEPTOR - 32.44 - 20 * np.log10(freq_mhz)) / 20)
    raio_ajustado = raio_base * 0.7 / (atenuacao.get(tipo_area, 16) / 10)
    limite_freq = min(7, 15000 / freq_mhz) if freq_mhz > 0 else 5
    return min(raio_ajustado, limite_freq)

def calcular_area_cobertura(raio, angulo=ANGULO_SETOR):
    if np.isnan(raio):
        return np.nan
    return (np.pi * raio**2 * angulo) / 360

def criar_setor_preciso(lat, lon, raio, azimute, angulo=ANGULO_SETOR, resolucao=30):
    if np.isnan(raio) or np.isnan(azimute) or raio <= 0:
        return None
    azimute_rad = np.radians((450 - float(azimute)) % 360)
    metade_angulo = np.radians(angulo / 2)
    pontos = [(lon, lat)]
    for i in range(resolucao + 1):
        angulo_atual = azimute_rad - metade_angulo + (i * 2 * metade_angulo / resolucao)
        for j in [0.8, 0.9, 0.95, 1.0]:
            dist = raio * j
            dx = dist * np.cos(angulo_atual) / 111.32
            dy = dist * np.sin(angulo_atual) / (111.32 * np.cos(np.radians(lat)))
            pontos.append((lon + dx, lat + dy))
    pontos.append((lon, lat))
    try:
        return Polygon(pontos)
    except:
        return None

def adicionar_elementos_cartograficos(ax, titulo, fonte="Dados: Anatel (ERBs), OpenStreetMap (Base)"):
    ax.set_title(titulo, fontweight='bold', pad=20)
    scalebar = AnchoredSizeBar(ax.transData, 5000, '5 km', 'lower right', pad=0.5,
                               color='black', frameon=True, size_vertical=100)
    ax.add_artist(scalebar)
    x, y, arrow_length = 0.06, 0.12, 0.08
    ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),
                arrowprops=dict(facecolor='black', width=2, headwidth=8),
                ha='center', va='center', fontsize=14,
                xycoords='figure fraction',
                fontweight='bold',
                bbox=dict(boxstyle="circle,pad=0.3", fc="white", ec="black", alpha=0.8))
    ax.annotate(f"{fonte}\nGerado em: {datetime.now().strftime('%d/%m/%Y')}",
                xy=(0.01, 0.01), xycoords='figure fraction', fontsize=8,
                ha='left', va='bottom',
                bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    ax.grid(True, linestyle='--', alpha=0.3, color='gray')
    ax.title.set_path_effects([PathEffects.withStroke(linewidth=1.5, foreground='white')])
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')

def criar_legenda_personalizada(ax, cores, titulo="Legenda"):
    elementos_legenda = [Patch(facecolor=cor, edgecolor='k', alpha=0.7, label=operadora)
                          for operadora, cor in cores.items()]
    legenda = ax.legend(handles=elementos_legenda, title=titulo, loc='upper right',
                        frameon=True, framealpha=0.8, edgecolor='k')
    legenda.get_frame().set_linewidth(0.8)
    plt.setp(legenda.get_title(), fontweight='bold')

# Função para carregar e processar dados
def carregar_e_processar_dados(caminho_arquivo):
    print("Carregando dados de ERBs...")
    dados_erb = pd.read_csv(caminho_arquivo)
    operadoras_alvo = ['CLARO', 'OI', 'VIVO', 'TIM', 'CLARO S.A.', 'OI MÓVEL S.A.', 'TELEFÔNICA BRASIL S.A.', 'TIM S.A.']
    print(f"Operadoras encontradas: {', '.join(sorted(dados_erb['NomeEntidade'].unique()[:10]))}")
    dados_filtrados = dados_erb[dados_erb['NomeEntidade'].str.contains('|'.join(operadoras_alvo), case=False, na=False)]
    mapeamento_operadoras = {
        'CLARO': 'CLARO',
        'CLARO S.A.': 'CLARO',
        'OI': 'OI',
        'OI MÓVEL S.A.': 'OI',
        'VIVO': 'VIVO',
        'TELEFÔNICA BRASIL S.A.': 'VIVO',
        'TIM': 'TIM',
        'TIM S.A.': 'TIM'
    }
    dados_filtrados.loc[:, 'Operadora'] = dados_filtrados['NomeEntidade'].apply(
        lambda x: next((v for k, v in mapeamento_operadoras.items() if k in x.upper()), 'OUTRA')
    )
    dados_filtrados = dados_filtrados[dados_filtrados['Operadora'].isin(['CLARO', 'OI', 'VIVO', 'TIM'])]
    sorocaba_bbox = [-23.60, -23.30, -47.65, -47.25]  # [lat_min, lat_max, lon_min, lon_max]
    dados_filtrados = dados_filtrados[
        (dados_filtrados['Latitude'] >= sorocaba_bbox[0]) &
        (dados_filtrados['Latitude'] <= sorocaba_bbox[1]) &
        (dados_filtrados['Longitude'] >= sorocaba_bbox[2]) &
        (dados_filtrados['Longitude'] <= sorocaba_bbox[3])
    ]
    dados_filtrados = dados_filtrados.drop_duplicates(subset=['Latitude', 'Longitude', 'Operadora'])
    max_erbs_por_operadora = {'CLARO': 120, 'TIM': 110, 'VIVO': 100, 'OI': 80}
    amostras = []
    for operadora, max_erbs in max_erbs_por_operadora.items():
        subset = dados_filtrados[dados_filtrados['Operadora'] == operadora]
        if len(subset) > max_erbs:
            subset = subset.sample(max_erbs, random_state=42)
        amostras.append(subset)
    dados_filtrados = pd.concat(amostras).reset_index(drop=True)
    print(f"Dados filtrados: {len(dados_filtrados)} ERBs encontradas.")
    print("Convertendo tipos de dados...")
    dados_filtrados['PotenciaTransmissorWatts'] = pd.to_numeric(dados_filtrados['PotenciaTransmissorWatts'], errors='coerce')
    dados_filtrados['FreqTxMHz'] = pd.to_numeric(dados_filtrados['FreqTxMHz'], errors='coerce')
    dados_filtrados['GanhoAntena'] = pd.to_numeric(dados_filtrados['GanhoAntena'], errors='coerce')
    dados_filtrados['Azimute'] = pd.to_numeric(dados_filtrados['Azimute'], errors='coerce')
    print("Preenchendo valores ausentes...")
    potencia_media = dados_filtrados.groupby('Operadora')['PotenciaTransmissorWatts'].median()
    for operadora, potencia in potencia_media.items():
        if pd.isna(potencia) or potencia <= 0:
            potencia = 20.0
        mask = (dados_filtrados['Operadora'] == operadora) & ((pd.isna(dados_filtrados['PotenciaTransmissorWatts'])) | (dados_filtrados['PotenciaTransmissorWatts'] <= 0))
        dados_filtrados.loc[mask, 'PotenciaTransmissorWatts'] = potencia
    ganho_medio = dados_filtrados['GanhoAntena'].median()
    if pd.isna(ganho_medio) or ganho_medio <= 0:
        ganho_medio = 16.0
    dados_filtrados.loc[pd.isna(dados_filtrados['GanhoAntena']) | (dados_filtrados['GanhoAntena'] <= 0), 'GanhoAntena'] = ganho_medio
    freq_media = dados_filtrados.groupby('Operadora')['FreqTxMHz'].median()
    for operadora, freq in freq_media.items():
        if pd.isna(freq) or freq <= 0:
            freq = 850.0
        mask = (dados_filtrados['Operadora'] == operadora) & ((pd.isna(dados_filtrados['FreqTxMHz'])) | (dados_filtrados['FreqTxMHz'] <= 0))
        dados_filtrados.loc[mask, 'FreqTxMHz'] = freq
    azimutes_padrao = [0, 120, 240]
    for i, row in dados_filtrados.iterrows():
        if pd.isna(row['Azimute']):
            dados_filtrados.loc[i, 'Azimute'] = azimutes_padrao[i % len(azimutes_padrao)]
    print("Calculando tipo de área...")
    geometria = [Point(xy) for xy in zip(dados_filtrados['Longitude'], dados_filtrados['Latitude'])]
    gdf_erb = gpd.GeoDataFrame(dados_filtrados, geometry=geometria, crs="EPSG:4326")
    def calcular_densidade_erb(ponto, gdf, raio=0.01):
        buffer = ponto.buffer(raio)
        return len(gdf[gdf.geometry.intersects(buffer)])
    gdf_erb['densidade_erb'] = gdf_erb.geometry.apply(lambda p: calcular_densidade_erb(p, gdf_erb))
    gdf_erb['tipo_area'] = pd.cut(gdf_erb['densidade_erb'], bins=[0, 3, 6, 10, float('inf')], labels=['rural', 'suburbana', 'urbana', 'urbana_densa'])
    gdf_erb['tipo_area'] = gdf_erb['tipo_area'].fillna('urbana')
    print("Calculando EIRP...")
    gdf_erb['EIRP_dBm'] = gdf_erb.apply(lambda row: calcular_eirp(row['PotenciaTransmissorWatts'], row['GanhoAntena']), axis=1)
    print("Calculando raio de cobertura...")
    gdf_erb['Raio_Cobertura_km'] = gdf_erb.apply(lambda row: calcular_raio_cobertura_aprimorado(row['EIRP_dBm'], row['FreqTxMHz'], row['tipo_area']), axis=1)
    print("Calculando área de cobertura...")
    gdf_erb['Area_Cobertura_km2'] = gdf_erb.apply(lambda row: calcular_area_cobertura(row['Raio_Cobertura_km']), axis=1)
    print("Calculando setores de cobertura...")
    gdf_erb['setor_geometria'] = gdf_erb.apply(lambda row: criar_setor_preciso(row['Latitude'], row['Longitude'], row['Raio_Cobertura_km'], row['Azimute']), axis=1)
    gdf_setores = gpd.GeoDataFrame(
        gdf_erb[['Operadora', 'EIRP_dBm', 'Raio_Cobertura_km', 'Area_Cobertura_km2', 'tipo_area']],
        geometry=gdf_erb['setor_geometria'],
        crs="EPSG:4326"
    ).dropna(subset=['geometry'])
    print(f"Processamento concluído. {len(gdf_erb)} ERBs e {len(gdf_setores)} setores de cobertura.")
    return gdf_erb, gdf_setores

# --- Funções de Visualização (com reprojeção para EPSG:3857) ---

def criar_mapa_posicionamento(gdf_erb, caminho_saida):
    print("Criando mapa de posicionamento das ERBs...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    min_eirp = gdf_erb_3857['EIRP_dBm'].min()
    max_eirp = gdf_erb_3857['EIRP_dBm'].max()
    tamanhos = ((gdf_erb_3857['EIRP_dBm'] - min_eirp) / (max_eirp - min_eirp) * 120 + 30)
    for operadora, cor in cores_operadoras.items():
        subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        if subset.empty:
            continue
        subset_tamanhos = tamanhos.loc[subset.index]
        ax.scatter(subset.geometry.x, subset.geometry.y,
                   s=subset_tamanhos * 1.5, color=cor, alpha=0.2, edgecolor='none')
        ax.scatter(subset.geometry.x, subset.geometry.y,
                   s=subset_tamanhos, color=cor, alpha=0.8, edgecolor='white', linewidth=0.5, label=operadora)
    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)
    adicionar_elementos_cartograficos(ax, 'Posicionamento das Estações Rádio Base (ERBs) por Operadora em Sorocaba')
    criar_legenda_personalizada(ax, cores_operadoras, "Operadoras")
    ax.annotate('Tamanho dos pontos proporcional à potência irradiada (EIRP)',
                xy=(0.5, 0.02), xycoords='figure fraction', ha='center',
                fontsize=10, bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de posicionamento salvo em {caminho_saida}")

def criar_mapa_cobertura_por_operadora(gdf_erb, gdf_setores, caminho_saida):
    print("Criando mapa de cobertura por operadora...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
    fig, axes = plt.subplots(2, 2, figsize=(20, 15))
    axes = axes.flatten()
    operadoras = ['CLARO', 'OI', 'VIVO', 'TIM']
    for i, operadora in enumerate(operadoras):
        ax = axes[i]
        subset_erb = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        subset_setores = gdf_setores_3857[gdf_setores_3857['Operadora'] == operadora]
        if subset_setores.empty:
            ax.set_title(f'Sem dados para {operadora}', fontsize=16)
            continue
        cor_base = cores_operadoras[operadora]
        cor_setores = f"{cor_base}66"
        subset_setores.plot(ax=ax, color=cor_setores, edgecolor=cor_base, linewidth=0.3, alpha=0.6)
        subset_erb.plot(ax=ax, color=cor_base, markersize=50, marker='o',
                        edgecolor='white', linewidth=0.7, alpha=0.9)
        ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        adicionar_elementos_cartograficos(ax, f'Cobertura da Operadora {operadora} em Sorocaba')
        n_erbs = len(subset_erb)
        cobertura_media = subset_erb['Raio_Cobertura_km'].mean()
        densidade_cobertura = n_erbs / 325
        info_text = (f"Total de ERBs: {n_erbs}\n"
                     f"Raio médio: {cobertura_media:.2f} km\n"
                     f"Densidade: {densidade_cobertura:.2f} ERBs/km²")
        ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                    fontsize=11, ha='left', va='top',
                    bbox=dict(boxstyle="round,pad=0.5", fc="white", ec=cor_base, alpha=0.8))
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.1, hspace=0.15)
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de cobertura por operadora salvo em {caminho_saida}")

def criar_mapa_sobreposicao(gdf_erb, gdf_setores, caminho_saida):
    print("Criando mapa de sobreposição de cobertura (versão otimizada)...")
    try:
        # Reprojetar para Web Mercator
        gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
        gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
        bbox = gdf_erb_3857.total_bounds
        x_min, y_min, x_max, y_max = bbox
        grid_size = 500
        # Definir transformação afim para o raster
        transform = from_bounds(x_min, y_min, x_max, y_max, grid_size, grid_size)
        # Inicializar array de contagem com zeros
        contagem_sobreposicao = np.zeros((grid_size, grid_size), dtype=np.uint8)
        # Para cada operadora, rasterize seus polígonos de cobertura e some os resultados
        for operadora in cores_operadoras.keys():
            subset = gdf_setores_3857[gdf_setores_3857['Operadora'] == operadora]
            if not subset.empty:
                shapes = [(geom, 1) for geom in subset.geometry if geom.is_valid]
                if shapes:
                    mask = rasterize(
                        shapes=shapes,
                        out_shape=(grid_size, grid_size),
                        transform=transform,
                        all_touched=True,
                        fill=0,
                        dtype=np.uint8
                    )
                    contagem_sobreposicao += mask
        # Plotar o resultado
        fig, ax = plt.subplots(figsize=(16, 12))
        im = ax.imshow(contagem_sobreposicao, extent=[x_min, x_max, y_min, y_max],
                       cmap=plt.cm.viridis, origin='lower', alpha=0.7, interpolation='bilinear')
        # Opcional: plotar os limites dos setores para cada operadora
        for operadora in cores_operadoras.keys():
            subset = gdf_setores_3857[gdf_setores_3857['Operadora'] == operadora]
            if not subset.empty:
                subset.boundary.plot(ax=ax, color=cores_operadoras[operadora], linewidth=1, alpha=0.8)
        # Plotar as ERBs
        for operadora, cor in cores_operadoras.items():
            subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
            if not subset.empty:
                subset.plot(ax=ax, color=cor, markersize=20, edgecolor='white', linewidth=0.5)
        ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        adicionar_elementos_cartograficos(ax, 'Sobreposição de Cobertura entre Operadoras em Sorocaba (Otimizado)',
                                         "Operadoras sobrepostas: CLARO, OI, TIM, VIVO")
        cbar = plt.colorbar(im, ax=ax, shrink=0.7)
        cbar.set_label('Número de Operadoras com Cobertura', fontsize=12)
        cbar.set_ticks(range(5))
        cbar.set_ticklabels(['Sem cobertura', '1 operadora', '2 operadoras', '3 operadoras', '4 operadoras'])
        criar_legenda_personalizada(ax, cores_operadoras, "Operadoras")
        plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"Mapa de sobreposição salvo em {caminho_saida}")
    except Exception as e:
        print(f"Erro ao criar mapa de sobreposição (otimizado): {e}")
        plt.close('all')

def criar_mapa_calor_potencia(gdf_erb, caminho_saida):
    print("Criando mapa de calor de potência...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    bbox = gdf_erb_3857.total_bounds
    x_min, y_min, x_max, y_max = bbox
    margin = 0.01
    x_min -= margin; y_min -= margin; x_max += margin; y_max += margin
    grid_size = 500
    xi = np.linspace(x_min, x_max, grid_size)
    yi = np.linspace(y_min, y_max, grid_size)
    xi, yi = np.meshgrid(xi, yi)
    pontos = np.array([(p.x, p.y) for p in gdf_erb_3857.geometry])
    valores = gdf_erb_3857['EIRP_dBm'].values
    grid_potencia = griddata(pontos, valores, (xi, yi), method='cubic', fill_value=np.min(valores))
    vmin = np.percentile(grid_potencia, 5)
    vmax = np.percentile(grid_potencia, 95)
    cmap = LinearSegmentedColormap.from_list('potencia',
                ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1',
                 '#6baed6', '#4292c6', '#2171b5', '#08519c', '#08306b'], N=256)
    im = ax.imshow(grid_potencia, extent=[x_min, x_max, y_min, y_max],
                   origin='lower', cmap=cmap, alpha=0.8, vmin=vmin, vmax=vmax, aspect='auto')
    contornos = ax.contour(xi, yi, grid_potencia, levels=5, colors='white', alpha=0.6, linewidths=0.8)
    plt.clabel(contornos, inline=1, fontsize=8, fmt='%.1f dBm')
    scatter = ax.scatter(gdf_erb_3857.geometry.x, gdf_erb_3857.geometry.y,
                         c=gdf_erb_3857['EIRP_dBm'], cmap=cmap, s=50, edgecolor='white',
                         linewidth=0.5, alpha=0.9, vmin=vmin, vmax=vmax)
    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
    adicionar_elementos_cartograficos(ax, 'Potência Efetivamente Irradiada (EIRP) das ERBs em Sorocaba')
    cbar = plt.colorbar(im, ax=ax, shrink=0.7)
    cbar.set_label('EIRP (dBm)', fontsize=12)
    eirp_min = gdf_erb_3857['EIRP_dBm'].min()
    eirp_max = gdf_erb_3857['EIRP_dBm'].max()
    eirp_media = gdf_erb_3857['EIRP_dBm'].mean()
    info_text = (f"EIRP Média: {eirp_media:.1f} dBm\n"
                 f"EIRP Mínima: {eirp_min:.1f} dBm\n"
                 f"EIRP Máxima: {eirp_max:.1f} dBm")
    ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                fontsize=11, ha='left', va='top',
                bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de calor de potência salvo em {caminho_saida}")

import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx
import numpy as np
import pandas as pd
from shapely.geometry import Polygon
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

def compute_hex_centers_in_batches(x_min, x_max, y_min, y_max, hex_size, batch_size=100):
    y_step = hex_size * 0.866  # cos(30°)
    y_range = np.arange(y_min, y_max, y_step)
    x_range = np.arange(x_min, x_max, hex_size)

    hex_centers = []
    for i in range(0, len(x_range), batch_size):
        x_batch = x_range[i:i+batch_size]
        X, Y = np.meshgrid(x_batch, y_range)
        offsets = np.where(np.arange(len(y_range)) % 2 == 1, hex_size / 2, 0)
        X_offset = X + offsets[:, np.newaxis]
        hex_centers.extend(zip(X_offset.flatten(), Y.flatten()))
    return hex_centers

def criar_mapa_vulnerabilidade(gdf_erb, gdf_setores, caminho_saida):
    print("Criando mapa de vulnerabilidade de cobertura...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)

    fig, ax = plt.subplots(figsize=(16, 12))
    bbox = gdf_erb_3857.total_bounds
    x_min, y_min, x_max, y_max = bbox
    hex_size = 0.005  # ajuste conforme escala

    hex_centers = compute_hex_centers_in_batches(x_min, x_max, y_min, y_max, hex_size, batch_size=100)

    def create_hexagon(center, size):
        angles = np.linspace(0, 2 * np.pi, 7)[:-1]
        x_coords = center[0] + size * np.cos(angles)
        y_coords = center[1] + size * np.sin(angles) * 0.866
        return Polygon(zip(x_coords, y_coords))

    hex_polygons = [create_hexagon(center, hex_size / 1.5) for center in hex_centers]
    gdf_hex = gpd.GeoDataFrame(geometry=hex_polygons, crs=gdf_erb_3857.crs)

    operadoras = gdf_erb_3857['Operadora'].unique()
    sindex_operadoras = {}
    subset_operadoras = {}

    # Pré-indexação por operadora
    for op in operadoras:
        subset = gdf_setores_3857[gdf_setores_3857['Operadora'] == op]
        sindex_operadoras[op] = subset.sindex
        subset_operadoras[op] = subset

    cobertura_hex = []
    hex_geoms = gdf_hex.geometry.values

    for hex_geom in hex_geoms:
        presentes = set()
        for op in operadoras:
            idxs = list(sindex_operadoras[op].intersection(hex_geom.bounds))
            if not idxs:
                continue
            candidatos = subset_operadoras[op].iloc[idxs]
            if candidatos.intersects(hex_geom).any():
                presentes.add(op)
        cobertura_hex.append(len(presentes))

    gdf_hex['num_operadoras'] = cobertura_hex
    gdf_hex['vulnerabilidade'] = pd.cut(
        gdf_hex['num_operadoras'],
        bins=[-1, 0, 1, 2, 5],
        labels=['Sem cobertura', 'Vulnerável', 'Redundância baixa', 'Cobertura ideal']
    )

    cores_vulnerabilidade = {
        'Sem cobertura': '#d73027',
        'Vulnerável': '#fc8d59',
        'Redundância baixa': '#fee090',
        'Cobertura ideal': '#1a9850'
    }
    gdf_hex['cor'] = gdf_hex['vulnerabilidade'].map(cores_vulnerabilidade)

    # Plot das áreas
    for categoria, cor in cores_vulnerabilidade.items():
        subset = gdf_hex[gdf_hex['vulnerabilidade'] == categoria]
        if not subset.empty:
            subset.plot(ax=ax, color=cor, edgecolor='white', linewidth=0.3, alpha=0.7, label=categoria)

    # Delimita áreas críticas
    areas_criticas = gdf_hex[gdf_hex['num_operadoras'] <= 1]
    if not areas_criticas.empty:
        areas_criticas.plot(ax=ax, facecolor='none', edgecolor='#d73027', linewidth=0.8, alpha=0.9)

    # Plot das ERBs por operadora
    cores_operadoras = {
        op: cor for op, cor in zip(operadoras, plt.cm.tab10.colors[:len(operadoras)])
    }
    for operadora, cor in cores_operadoras.items():
        subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        if not subset.empty:
            subset.plot(ax=ax, color=cor, markersize=20, alpha=0.7, edgecolor='white', linewidth=0.5)

    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)

    # Elementos cartográficos
    ax.set_title('Análise de Vulnerabilidade de Cobertura em Sorocaba', fontsize=16)
    elementos_legenda = [Patch(facecolor=cor, edgecolor='white', alpha=0.7, label=cat)
                         for cat, cor in cores_vulnerabilidade.items()]
    for operadora, cor in cores_operadoras.items():
        elementos_legenda.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=cor,
                                        markersize=8, label=f'ERB {operadora}'))

    ax.legend(handles=elementos_legenda, title="Vulnerabilidade e ERBs",
              loc='upper right', frameon=True, framealpha=0.8, edgecolor='k')

    # Estatísticas do mapa
    total_areas = len(gdf_hex)
    areas_sem_cobertura = len(gdf_hex[gdf_hex['num_operadoras'] == 0])
    areas_vulneraveis = len(gdf_hex[gdf_hex['num_operadoras'] == 1])
    areas_redundantes = len(gdf_hex[gdf_hex['num_operadoras'] >= 2])
    info_text = (
        f"Áreas sem cobertura: {areas_sem_cobertura} ({areas_sem_cobertura/total_areas*100:.1f}%)\n"
        f"Áreas vulneráveis: {areas_vulneraveis} ({areas_vulneraveis/total_areas*100:.1f}%)\n"
        f"Áreas com redundância: {areas_redundantes} ({areas_redundantes/total_areas*100:.1f}%)"
    )
    ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                fontsize=11, ha='left', va='top',
                bbox=dict(boxstyle="round,pad=0.5", fc="white", ec='black', alpha=0.8))

    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de vulnerabilidade salvo em {caminho_saida}")



def criar_mapa_voronoi_setores(gdf_erb, caminho_saida):
    print("Criando mapa de setores de Voronoi...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    pontos = np.array([(p.x, p.y) for p in gdf_erb_3857.geometry])
    if len(pontos) < 4:
        print("Dados insuficientes para criar diagrama de Voronoi")
        plt.close(fig)
        return
    try:
        vor = Voronoi(pontos)
        bounds = gdf_erb_3857.total_bounds
        boundary = Polygon([(bounds[0], bounds[1]), (bounds[2], bounds[1]),
                            (bounds[2], bounds[3]), (bounds[0], bounds[3])])
        regions = []
        for region_idx, region in enumerate(vor.regions):
            if not region or -1 in region:
                continue
            polygon = [vor.vertices[i] for i in region]
            if len(polygon) < 3:
                continue
            poly = Polygon(polygon)
            if poly.is_valid:
                poly = poly.intersection(boundary)
                if not poly.is_empty:
                    regions.append((region_idx, poly))
        region_to_erb = {}
        for i, region_idx in enumerate(vor.point_region):
            region_to_erb[region_idx] = i
        voronoi_regions = []
        for region_idx, poly in regions:
            if region_idx in region_to_erb:
                erb_idx = region_to_erb[region_idx]
                if erb_idx < len(gdf_erb_3857):
                    voronoi_regions.append({
                        'geometry': poly,
                        'operadora': gdf_erb_3857.iloc[erb_idx]['Operadora'],
                        'eirp': gdf_erb_3857.iloc[erb_idx]['EIRP_dBm'],
                        'erb_idx': erb_idx
                    })
        if not voronoi_regions:
            print("Nenhuma região de Voronoi válida gerada")
            plt.close(fig)
            return
        gdf_voronoi = gpd.GeoDataFrame(voronoi_regions, crs=gdf_erb_3857.crs)
        for operadora, cor in cores_operadoras.items():
            subset = gdf_voronoi[gdf_voronoi['operadora'] == operadora]
            if not subset.empty:
                subset.plot(ax=ax, color=cor, edgecolor='white', linewidth=0.8, alpha=0.4)
        for linha in vor.ridge_vertices:
            if -1 not in linha:
                x, y = vor.vertices[linha, 0], vor.vertices[linha, 1]
                ax.plot(x, y, 'k-', linewidth=0.5, alpha=0.3)
        for operadora, cor in cores_operadoras.items():
            subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
            if not subset.empty:
                subset.plot(ax=ax, color=cor, markersize=30, marker='o',
                            edgecolor='white', linewidth=0.7, alpha=0.9)
        ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        adicionar_elementos_cartograficos(ax, 'Análise de Setores de Voronoi por Operadora em Sorocaba')
        criar_legenda_personalizada(ax, cores_operadoras, "Operadoras")
        ax.annotate("Células de Voronoi representam áreas onde cada ponto está mais próximo\n"
                    "de uma determinada ERB do que de qualquer outra ERB.",
                    xy=(0.5, 0.03), xycoords='figure fraction', ha='center',
                    fontsize=10, bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
        estatisticas = gdf_voronoi.groupby('operadora')['geometry'].agg(lambda x: sum(geom.area for geom in x))
        estatisticas = (estatisticas / estatisticas.sum() * 100).round(1)
        info_text = "Distribuição espacial por operadora:\n"
        for op, valor in estatisticas.items():
            info_text += f"{op}: {valor:.1f}%\n"
        ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                    fontsize=10, ha='left', va='top',
                    bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
        plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"Mapa de setores de Voronoi salvo em {caminho_saida}")
    except Exception as e:
        print(f"Erro ao criar diagrama de Voronoi: {e}")
        plt.close(fig)

def criar_mapa_3d_cobertura(gdf_erb, gdf_setores, caminho_saida):
    print("Criando visualização 3D da cobertura...")
    from mpl_toolkits.mplot3d import Axes3D
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
    fig = plt.figure(figsize=(16, 12))
    ax = fig.add_subplot(111, projection='3d')
    xmin, ymin, xmax, ymax = gdf_erb_3857.total_bounds
    n_bins = 50
    x_edges = np.linspace(xmin, xmax, n_bins)
    y_edges = np.linspace(ymin, ymax, n_bins)
    densidade_cobertura = np.zeros((n_bins-1, n_bins-1))
    potencia_media = np.zeros((n_bins-1, n_bins-1))
    for i in range(n_bins-1):
        for j in range(n_bins-1):
            x_min_cell, x_max_cell = x_edges[i], x_edges[i+1]
            y_min_cell, y_max_cell = y_edges[j], y_edges[j+1]
            cell_poly = Polygon([(x_min_cell, y_min_cell),
                                 (x_max_cell, y_min_cell),
                                 (x_max_cell, y_max_cell),
                                 (x_min_cell, y_max_cell)])
            setores_na_celula = gdf_setores_3857[gdf_setores_3857.geometry.intersects(cell_poly)]
            densidade_cobertura[j, i] = len(setores_na_celula)
            if len(setores_na_celula) > 0:
                potencia_media[j, i] = setores_na_celula['EIRP_dBm'].mean()
            else:
                potencia_media[j, i] = np.nan
    densidade_max = np.percentile(densidade_cobertura[densidade_cobertura > 0], 95)
    densidade_norm = np.clip(densidade_cobertura / densidade_max, 0, 1)
    densidade_norm = np.where(densidade_norm > 0, densidade_norm, np.nan)
    X, Y = np.meshgrid((x_edges[:-1] + x_edges[1:]) / 2,
                       (y_edges[:-1] + y_edges[1:]) / 2)
    Z = densidade_norm * 5  # escala para visualização
    vmin = np.nanpercentile(potencia_media, 5)
    vmax = np.nanpercentile(potencia_media, 95)
    norm = Normalize(vmin=vmin, vmax=vmax)
    surf = ax.plot_surface(X, Y, Z,
                           facecolors=plt.cm.viridis(norm(potencia_media)),
                           rstride=1, cstride=1, alpha=0.8,
                           shade=True, edgecolor='none')
    for operadora, cor in cores_operadoras.items():
        subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        if not subset.empty:
            ax.scatter(subset.geometry.x, subset.geometry.y,
                       np.ones(len(subset)) * 5.2,
                       color=cor, s=50, edgecolor='white',
                       linewidth=0.5, alpha=0.9, label=operadora)
    ax.set_title('Visualização 3D da Cobertura de ERBs em Sorocaba', fontsize=16, pad=20)
    ax.set_xlabel('Longitude', fontsize=12, labelpad=10)
    ax.set_ylabel('Latitude', fontsize=12, labelpad=10)
    ax.set_zlabel('Intensidade de Cobertura', fontsize=12, labelpad=10)
    ax.grid(False)
    ax.view_init(elev=35, azim=220)
    m = plt.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis)
    m.set_array([])
    cbar = fig.colorbar(m, ax=ax, shrink=0.5, pad=0.1)
    cbar.set_label('Potência Média (EIRP dBm)', fontsize=10)
    ax.legend(title="Operadoras", loc="upper right", frameon=True, framealpha=0.8)
    ax.text2D(0.02, 0.02,
              "Altura representa intensidade de cobertura.\n"
              "Cor representa potência média (EIRP) das ERBs.",
              transform=ax.transAxes, fontsize=10,
              bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    plt.figtext(0.5, 0.01,
                f"Dados: Anatel (ERBs) | Processado em: {datetime.now().strftime('%d/%m/%Y')}",
                ha='center', fontsize=8)
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Visualização 3D salva em {caminho_saida}")

def gerar_mapa_interativo(gdf_erb, gdf_setores, caminho_saida):
    print("Gerando mapa interativo...")
    gdf_erb_wgs84 = gdf_erb.to_crs(epsg=4326)
    gdf_setores_wgs84 = gdf_setores.to_crs(epsg=4326)
    centro = [gdf_erb_wgs84.geometry.y.mean(), gdf_erb_wgs84.geometry.x.mean()]
    mapa = folium.Map(location=centro, zoom_start=13, tiles='CartoDB positron')
    folium.LayerControl().add_to(mapa)
    operadoras = gdf_erb['Operadora'].unique()
    grupos_operadoras = {op: folium.FeatureGroup(name=f"ERBs {op}") for op in operadoras}
    grupos_setores = {op: folium.FeatureGroup(name=f"Cobertura {op}") for op in operadoras}
    for operadora in operadoras:
        cor = cores_operadoras[operadora]
        setores_op = gdf_setores_wgs84[gdf_setores_wgs84['Operadora'] == operadora]
        for idx, row in setores_op.iterrows():
            geojson = folium.GeoJson(row.geometry.__geo_interface__,
                                     style_function=lambda x, cor=cor: {
                                         'fillColor': cor,
                                         'color': cor,
                                         'weight': 1,
                                         'fillOpacity': 0.3
                                     })
            folium.Popup(
                f"<b>Operadora:</b> {operadora}<br>"
                f"<b>EIRP:</b> {row['EIRP_dBm']:.1f} dBm<br>"
                f"<b>Raio:</b> {row['Raio_Cobertura_km']:.2f} km<br>"
                f"<b>Área:</b> {row['Area_Cobertura_km2']:.2f} km²"
            ).add_to(geojson)
            geojson.add_to(grupos_setores[operadora])
    for operadora in operadoras:
        cor = cores_operadoras[operadora]
        erbs_op = gdf_erb_wgs84[gdf_erb_wgs84['Operadora'] == operadora]
        cluster = MarkerCluster(name=f"ERBs {operadora}")
        for idx, row in erbs_op.iterrows():
            icone = folium.Icon(color='white', icon_color=cor, icon='antenna', prefix='fa')
            marcador = folium.Marker(
                location=[row.geometry.y, row.geometry.x],
                icon=icone,
                popup=folium.Popup(
                    f"<b>Operadora:</b> {operadora}<br>"
                    f"<b>EIRP:</b> {row['EIRP_dBm']:.1f} dBm<br>"
                    f"<b>Tecnologia:</b> {row['Tecnologia']}<br>"
                    f"<b>Frequência:</b> {row['FreqTxMHz']:.1f} MHz<br>"
                    f"<b>Raio estimado:</b> {row['Raio_Cobertura_km']:.2f} km",
                    max_width=300
                )
            )
            marcador.add_to(cluster)
        cluster.add_to(grupos_operadoras[operadora])
    pontos_calor = []
    for idx, row in gdf_erb_wgs84.iterrows():
        peso = (row['EIRP_dBm'] - gdf_erb['EIRP_dBm'].min()) / (gdf_erb['EIRP_dBm'].max() - gdf_erb['EIRP_dBm'].min())
        peso = max(0.1, min(1.0, peso))
        centro = [row.geometry.y, row.geometry.x]
        raio_km = row['Raio_Cobertura_km']
        pontos_calor.append(centro + [peso * 1.5])
        azimute = row['Azimute']
        if not pd.isna(azimute):
            azimute_rad = np.radians((450 - float(azimute)) % 360)
            angulo_setor = np.radians(ANGULO_SETOR)
            for i in range(10):
                angulo = azimute_rad - angulo_setor / 2 + i * (angulo_setor / 9)
                for dist_fator in [0.3, 0.6, 0.9]:
                    dist = raio_km * dist_fator
                    dx = dist * np.cos(angulo) / 111.32
                    dy = dist * np.sin(angulo) / (111.32 * np.cos(np.radians(centro[0])))
                    peso_ajustado = peso * (1 - 0.7 * dist_fator)
                    pontos_calor.append([centro[0] + dy, centro[1] + dx, peso_ajustado])
    mapa_calor = HeatMap(
        pontos_calor,
        name="Mapa de Calor de Cobertura",
        min_opacity=0.4,
        max_zoom=13,
        radius=25,
        blur=15,
        gradient={
            0.0: 'blue',
            0.25: 'cyan',
            0.5: 'lime',
            0.75: 'yellow',
            1.0: 'red'
        }
    )
    mapa_calor.add_to(mapa)
    for operadora in operadoras:
        grupos_setores[operadora].add_to(mapa)
        grupos_operadoras[operadora].add_to(mapa)
    titulo_html = '''
    <div style="position: fixed; top: 10px; left: 50%; transform: translateX(-50%);
         z-index: 9999; background-color: white; padding: 10px; border-radius: 5px;
         box-shadow: 0 0 10px rgba(0,0,0,0.2); font-family: Arial, sans-serif;
         font-size: 16px; font-weight: bold;">
         Mapa Interativo de Cobertura de ERBs em Sorocaba
    </div>
    '''
    mapa.get_root().html.add_child(folium.Element(titulo_html))
    info_html = f'''
    <div style="position: fixed; bottom: 10px; left: 10px; z-index: 9999; background-color: white;
         padding: 8px; border-radius: 5px; box-shadow: 0 0 5px rgba(0,0,0,0.2);
         font-family: Arial, sans-serif; font-size: 12px;">
         <b>Dados:</b> Anatel (ERBs) | <b>Gerado em:</b> {datetime.now().strftime('%d/%m/%Y')}
    </div>
    '''
    mapa.get_root().html.add_child(folium.Element(info_html))
    mapa.save(caminho_saida)
    print(f"Mapa interativo salvo em {caminho_saida}")

def gerar_visualizacoes(gdf_erb, gdf_setores, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    caminho_mapa_posicionamento = os.path.join(output_dir, "01_mapa_posicionamento_erbs.png")
    caminho_mapa_cobertura = os.path.join(output_dir, "02_mapa_cobertura_por_operadora.png")
    caminho_mapa_sobreposicao = os.path.join(output_dir, "03_mapa_sobreposicao_cobertura.png")
    caminho_mapa_calor_potencia = os.path.join(output_dir, "04_mapa_calor_potencia.png")
    caminho_mapa_vulnerabilidade = os.path.join(output_dir, "05_mapa_vulnerabilidade.png")
    caminho_dashboard = os.path.join(output_dir, "06_dashboard_comparativo.png")
    caminho_mapa_voronoi = os.path.join(output_dir, "07_mapa_setores_voronoi.png")
    caminho_visualizacao_3d = os.path.join(output_dir, "08_visualizacao_3d_cobertura.png")
    caminho_mapa_interativo = os.path.join(output_dir, "09_mapa_interativo.html")

    try:
        criar_mapa_posicionamento(gdf_erb, caminho_mapa_posicionamento)
    except Exception as e:
        print(f"Erro ao criar mapa de posicionamento: {e}")

    try:
        criar_mapa_cobertura_por_operadora(gdf_erb, gdf_setores, caminho_mapa_cobertura)
    except Exception as e:
        print(f"Erro ao criar mapa de cobertura por operadora: {e}")

    try:
        criar_mapa_sobreposicao(gdf_erb, gdf_setores, caminho_mapa_sobreposicao)
    except Exception as e:
        print(f"Erro ao criar mapa de sobreposição: {e}")

    try:
        criar_mapa_calor_potencia(gdf_erb, caminho_mapa_calor_potencia)
    except Exception as e:
        print(f"Erro ao criar mapa de calor de potência: {e}")

    try:
        criar_mapa_vulnerabilidade(gdf_erb, gdf_setores, caminho_mapa_vulnerabilidade)
    except Exception as e:
        print(f"Erro ao criar mapa de vulnerabilidade: {e}")

    try:
        criar_dashboard_operadoras(gdf_erb, gdf_setores, caminho_dashboard)
    except Exception as e:
        print(f"Erro ao criar dashboard: {e}")

    try:
        criar_mapa_voronoi_setores(gdf_erb, caminho_mapa_voronoi)
    except Exception as e:
        print(f"Erro ao criar mapa de setores Voronoi: {e}")

    try:
        criar_mapa_3d_cobertura(gdf_erb, gdf_setores, caminho_visualizacao_3d)
    except Exception as e:
        print(f"Erro ao criar visualização 3D: {e}")

    try:
        gerar_mapa_interativo(gdf_erb, gdf_setores, caminho_mapa_interativo)
    except Exception as e:
        print(f"Erro ao gerar mapa interativo: {e}")

    try:
        gdf_erb.to_file(os.path.join(output_dir, "erb_sorocaba_processado.geojson"), driver="GeoJSON")
    except Exception as e:
        print(f"Erro ao salvar ERBs processadas: {e}")

    try:
        gdf_setores_simples = gdf_setores.copy()
        gdf_setores_simples['geometry'] = gdf_setores_simples.geometry.buffer(0).simplify(0.0001)
        gdf_setores_simples.to_file(os.path.join(output_dir, "erb_sorocaba_setores_simplificado.geojson"), driver="GeoJSON")
    except Exception as e:
        print(f"Erro ao salvar setores: {e}")

    print(f"Todas as visualizações foram tentadas. Verifique a pasta: {output_dir}")

def main():
    try:
        caminho_arquivo = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_sorocaba.csv'
        print(f"Iniciando processamento do arquivo: {caminho_arquivo}")
        gdf_erb, gdf_setores = carregar_e_processar_dados(caminho_arquivo)
        gerar_visualizacoes(gdf_erb, gdf_setores, output_dir)
        print("Processamento concluído com sucesso!")
    except Exception as e:
        print(f"Erro crítico durante o processamento: {e}")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point, Polygon, LineString, MultiPolygon
import folium
from folium.plugins import HeatMap, MarkerCluster
import math
import contextily as ctx
from matplotlib.colors import LinearSegmentedColormap, Normalize
from matplotlib.patches import Patch
from matplotlib.lines import Line2D
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import matplotlib.font_manager as fm
from matplotlib.ticker import MaxNLocator
import matplotlib.patheffects as PathEffects
from datetime import datetime
import os
from google.colab import drive
import warnings
from scipy.spatial import Voronoi, voronoi_plot_2d
from sklearn.preprocessing import MinMaxScaler
from scipy.interpolate import griddata
import networkx as nx
import h3
import rasterio
from rasterio.features import rasterize
from rasterio.transform import from_bounds

# Ignorar avisos
warnings.filterwarnings('ignore')

# Montar o Google Drive (se estiver usando o Colab)
drive.mount('/content/drive', force_remount=True)

# Configurações visuais
plt.rcParams.update({
    'font.family': 'DejaVu Sans',
    'font.size': 11,
    'axes.titlesize': 16,
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 18,
    'figure.figsize': (12, 8),
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.2
})

# Diretório de saída
output_dir = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/visualizacoes'
os.makedirs(output_dir, exist_ok=True)

# Constantes
SENSIBILIDADE_RECEPTOR = -100  # dBm
ANGULO_SETOR = 120  # graus

# Parâmetros de resolução para operações de grade (ajuste conforme seu ambiente)
GRID_SIZE_SOBREPOSICAO = 300  # para mapa de sobreposição (antes era 500)
GRID_SIZE_CALOR = 300         # para mapa de calor (antes era 500)

# Dicionário de cores para operadoras
cores_operadoras = {
    'CLARO': '#E02020',
    'OI': '#FFD700',
    'VIVO': '#9932CC',
    'TIM': '#0000CD'
}

# --- Funções de Cálculo de Sinal e Cobertura ---

def calcular_eirp(potencia_watts, ganho_antena):
    try:
        potencia = float(potencia_watts)
        if potencia <= 0:
            return np.nan
    except (ValueError, TypeError):
        return np.nan
    return 10 * np.log10(potencia * 1000) + ganho_antena

def calcular_raio_cobertura_aprimorado(eirp, freq_mhz, tipo_area='urbana'):
    if np.isnan(eirp) or np.isnan(freq_mhz) or freq_mhz <= 0:
        return np.nan
    atenuacao = {'urbana_densa': 22, 'urbana': 16, 'suburbana': 12, 'rural': 8}
    raio_base = 10 ** ((eirp - SENSIBILIDADE_RECEPTOR - 32.44 - 20 * np.log10(freq_mhz)) / 20)
    raio_ajustado = raio_base * 0.7 / (atenuacao.get(tipo_area, 16) / 10)
    limite_freq = min(7, 15000 / freq_mhz) if freq_mhz > 0 else 5
    return min(raio_ajustado, limite_freq)

def calcular_area_cobertura(raio, angulo=ANGULO_SETOR):
    if np.isnan(raio):
        return np.nan
    return (np.pi * raio**2 * angulo) / 360

def criar_setor_preciso(lat, lon, raio, azimute, angulo=ANGULO_SETOR, resolucao=30):
    if np.isnan(raio) or np.isnan(azimute) or raio <= 0:
        return None
    azimute_rad = np.radians((450 - float(azimute)) % 360)
    metade_angulo = np.radians(angulo / 2)
    pontos = [(lon, lat)]
    for i in range(resolucao + 1):
        angulo_atual = azimute_rad - metade_angulo + (i * 2 * metade_angulo / resolucao)
        for j in [0.8, 0.9, 0.95, 1.0]:
            dist = raio * j
            dx = dist * np.cos(angulo_atual) / 111.32
            dy = dist * np.sin(angulo_atual) / (111.32 * np.cos(np.radians(lat)))
            pontos.append((lon + dx, lat + dy))
    pontos.append((lon, lat))
    try:
        return Polygon(pontos)
    except:
        return None

# --- Funções de Elementos Cartográficos ---

def adicionar_elementos_cartograficos(ax, titulo, fonte="Dados: Anatel (ERBs), OpenStreetMap (Base)"):
    ax.set_title(titulo, fontweight='bold', pad=20)
    scalebar = AnchoredSizeBar(ax.transData, 5000, '5 km', 'lower right', pad=0.5,
                               color='black', frameon=True, size_vertical=100)
    ax.add_artist(scalebar)
    x, y, arrow_length = 0.06, 0.12, 0.08
    ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),
                arrowprops=dict(facecolor='black', width=2, headwidth=8),
                ha='center', va='center', fontsize=14,
                xycoords='figure fraction',
                fontweight='bold',
                bbox=dict(boxstyle="circle,pad=0.3", fc="white", ec="black", alpha=0.8))
    ax.annotate(f"{fonte}\nGerado em: {datetime.now().strftime('%d/%m/%Y')}",
                xy=(0.01, 0.01), xycoords='figure fraction', fontsize=8,
                ha='left', va='bottom',
                bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    ax.grid(True, linestyle='--', alpha=0.3, color='gray')
    ax.title.set_path_effects([PathEffects.withStroke(linewidth=1.5, foreground='white')])
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')

def criar_legenda_personalizada(ax, cores, titulo="Legenda"):
    elementos_legenda = [Patch(facecolor=cor, edgecolor='k', alpha=0.7, label=operadora)
                          for operadora, cor in cores.items()]
    legenda = ax.legend(handles=elementos_legenda, title=titulo, loc='upper right',
                        frameon=True, framealpha=0.8, edgecolor='k')
    legenda.get_frame().set_linewidth(0.8)
    plt.setp(legenda.get_title(), fontweight='bold')

# --- Função para Carregar e Processar Dados ---

def carregar_e_processar_dados(caminho_arquivo):
    print("Carregando dados de ERBs...")
    dados_erb = pd.read_csv(caminho_arquivo)
    operadoras_alvo = ['CLARO', 'OI', 'VIVO', 'TIM', 'CLARO S.A.', 'OI MÓVEL S.A.', 'TELEFÔNICA BRASIL S.A.', 'TIM S.A.']
    print(f"Operadoras encontradas: {', '.join(sorted(dados_erb['NomeEntidade'].unique()[:10]))}")
    dados_filtrados = dados_erb[dados_erb['NomeEntidade'].str.contains('|'.join(operadoras_alvo), case=False, na=False)]
    mapeamento_operadoras = {
        'CLARO': 'CLARO',
        'CLARO S.A.': 'CLARO',
        'OI': 'OI',
        'OI MÓVEL S.A.': 'OI',
        'VIVO': 'VIVO',
        'TELEFÔNICA BRASIL S.A.': 'VIVO',
        'TIM': 'TIM',
        'TIM S.A.': 'TIM'
    }
    dados_filtrados.loc[:, 'Operadora'] = dados_filtrados['NomeEntidade'].apply(
        lambda x: next((v for k, v in mapeamento_operadoras.items() if k in x.upper()), 'OUTRA')
    )
    dados_filtrados = dados_filtrados[dados_filtrados['Operadora'].isin(['CLARO', 'OI', 'VIVO', 'TIM'])]
    sorocaba_bbox = [-23.60, -23.30, -47.65, -47.25]  # [lat_min, lat_max, lon_min, lon_max]
    dados_filtrados = dados_filtrados[
        (dados_filtrados['Latitude'] >= sorocaba_bbox[0]) &
        (dados_filtrados['Latitude'] <= sorocaba_bbox[1]) &
        (dados_filtrados['Longitude'] >= sorocaba_bbox[2]) &
        (dados_filtrados['Longitude'] <= sorocaba_bbox[3])
    ]
    dados_filtrados = dados_filtrados.drop_duplicates(subset=['Latitude', 'Longitude', 'Operadora'])
    max_erbs_por_operadora = {'CLARO': 120, 'TIM': 110, 'VIVO': 100, 'OI': 80}
    amostras = []
    for operadora, max_erbs in max_erbs_por_operadora.items():
        subset = dados_filtrados[dados_filtrados['Operadora'] == operadora]
        if len(subset) > max_erbs:
            subset = subset.sample(max_erbs, random_state=42)
        amostras.append(subset)
    dados_filtrados = pd.concat(amostras).reset_index(drop=True)
    print(f"Dados filtrados: {len(dados_filtrados)} ERBs encontradas.")
    print("Convertendo tipos de dados...")
    dados_filtrados['PotenciaTransmissorWatts'] = pd.to_numeric(dados_filtrados['PotenciaTransmissorWatts'], errors='coerce')
    dados_filtrados['FreqTxMHz'] = pd.to_numeric(dados_filtrados['FreqTxMHz'], errors='coerce')
    dados_filtrados['GanhoAntena'] = pd.to_numeric(dados_filtrados['GanhoAntena'], errors='coerce')
    dados_filtrados['Azimute'] = pd.to_numeric(dados_filtrados['Azimute'], errors='coerce')
    print("Preenchendo valores ausentes...")
    potencia_media = dados_filtrados.groupby('Operadora')['PotenciaTransmissorWatts'].median()
    for operadora, potencia in potencia_media.items():
        if pd.isna(potencia) or potencia <= 0:
            potencia = 20.0
        mask = (dados_filtrados['Operadora'] == operadora) & (
            (pd.isna(dados_filtrados['PotenciaTransmissorWatts'])) | (dados_filtrados['PotenciaTransmissorWatts'] <= 0))
        dados_filtrados.loc[mask, 'PotenciaTransmissorWatts'] = potencia
    ganho_medio = dados_filtrados['GanhoAntena'].median()
    if pd.isna(ganho_medio) or ganho_medio <= 0:
        ganho_medio = 16.0
    dados_filtrados.loc[pd.isna(dados_filtrados['GanhoAntena']) | (dados_filtrados['GanhoAntena'] <= 0), 'GanhoAntena'] = ganho_medio
    freq_media = dados_filtrados.groupby('Operadora')['FreqTxMHz'].median()
    for operadora, freq in freq_media.items():
        if pd.isna(freq) or freq <= 0:
            freq = 850.0
        mask = (dados_filtrados['Operadora'] == operadora) & (
            (pd.isna(dados_filtrados['FreqTxMHz'])) | (dados_filtrados['FreqTxMHz'] <= 0))
        dados_filtrados.loc[mask, 'FreqTxMHz'] = freq
    azimutes_padrao = [0, 120, 240]
    for i, row in dados_filtrados.iterrows():
        if pd.isna(row['Azimute']):
            dados_filtrados.loc[i, 'Azimute'] = azimutes_padrao[i % len(azimutes_padrao)]
    print("Calculando tipo de área...")
    geometria = [Point(xy) for xy in zip(dados_filtrados['Longitude'], dados_filtrados['Latitude'])]
    gdf_erb = gpd.GeoDataFrame(dados_filtrados, geometry=geometria, crs="EPSG:4326")
    def calcular_densidade_erb(ponto, gdf, raio=0.01):
        buffer = ponto.buffer(raio)
        return len(gdf[gdf.geometry.intersects(buffer)])
    gdf_erb['densidade_erb'] = gdf_erb.geometry.apply(lambda p: calcular_densidade_erb(p, gdf_erb))
    gdf_erb['tipo_area'] = pd.cut(gdf_erb['densidade_erb'], bins=[0, 3, 6, 10, float('inf')],
                                  labels=['rural', 'suburbana', 'urbana', 'urbana_densa'])
    gdf_erb['tipo_area'] = gdf_erb['tipo_area'].fillna('urbana')
    print("Calculando EIRP...")
    gdf_erb['EIRP_dBm'] = gdf_erb.apply(lambda row: calcular_eirp(row['PotenciaTransmissorWatts'], row['GanhoAntena']), axis=1)
    print("Calculando raio de cobertura...")
    gdf_erb['Raio_Cobertura_km'] = gdf_erb.apply(lambda row: calcular_raio_cobertura_aprimorado(row['EIRP_dBm'], row['FreqTxMHz'], row['tipo_area']), axis=1)
    print("Calculando área de cobertura...")
    gdf_erb['Area_Cobertura_km2'] = gdf_erb.apply(lambda row: calcular_area_cobertura(row['Raio_Cobertura_km']), axis=1)
    print("Calculando setores de cobertura...")
    gdf_erb['setor_geometria'] = gdf_erb.apply(lambda row: criar_setor_preciso(row['Latitude'], row['Longitude'], row['Raio_Cobertura_km'], row['Azimute']), axis=1)
    gdf_setores = gpd.GeoDataFrame(
        gdf_erb[['Operadora', 'EIRP_dBm', 'Raio_Cobertura_km', 'Area_Cobertura_km2', 'tipo_area']],
        geometry=gdf_erb['setor_geometria'],
        crs="EPSG:4326"
    ).dropna(subset=['geometry'])
    print(f"Processamento concluído. {len(gdf_erb)} ERBs e {len(gdf_setores)} setores de cobertura.")
    return gdf_erb, gdf_setores

# --- Funções de Visualização (Usando reprojeção para EPSG:3857) ---

def criar_mapa_posicionamento(gdf_erb, caminho_saida):
    print("Criando mapa de posicionamento das ERBs...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    min_eirp = gdf_erb_3857['EIRP_dBm'].min()
    max_eirp = gdf_erb_3857['EIRP_dBm'].max()
    tamanhos = ((gdf_erb_3857['EIRP_dBm'] - min_eirp) / (max_eirp - min_eirp) * 120 + 30)
    for operadora, cor in cores_operadoras.items():
        subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        if subset.empty:
            continue
        subset_tamanhos = tamanhos.loc[subset.index]
        ax.scatter(subset.geometry.x, subset.geometry.y,
                   s=subset_tamanhos * 1.5, color=cor, alpha=0.2, edgecolor='none')
        ax.scatter(subset.geometry.x, subset.geometry.y,
                   s=subset_tamanhos, color=cor, alpha=0.8, edgecolor='white', linewidth=0.5, label=operadora)
    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)
    adicionar_elementos_cartograficos(ax, 'Posicionamento das Estações Rádio Base (ERBs) por Operadora em Sorocaba')
    criar_legenda_personalizada(ax, cores_operadoras, "Operadoras")
    ax.annotate('Tamanho dos pontos proporcional à potência irradiada (EIRP)',
                xy=(0.5, 0.02), xycoords='figure fraction', ha='center',
                fontsize=10, bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de posicionamento salvo em {caminho_saida}")

def criar_mapa_cobertura_por_operadora(gdf_erb, gdf_setores, caminho_saida):
    print("Criando mapa de cobertura por operadora...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
    fig, axes = plt.subplots(2, 2, figsize=(20, 15))
    axes = axes.flatten()
    operadoras = ['CLARO', 'OI', 'VIVO', 'TIM']
    for i, operadora in enumerate(operadoras):
        ax = axes[i]
        subset_erb = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        subset_setores = gdf_setores_3857[gdf_setores_3857['Operadora'] == operadora]
        if subset_setores.empty:
            ax.set_title(f'Sem dados para {operadora}', fontsize=16)
            continue
        cor_base = cores_operadoras[operadora]
        cor_setores = f"{cor_base}66"
        subset_setores.plot(ax=ax, color=cor_setores, edgecolor=cor_base, linewidth=0.3, alpha=0.6)
        subset_erb.plot(ax=ax, color=cor_base, markersize=50, marker='o',
                        edgecolor='white', linewidth=0.7, alpha=0.9)
        ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        adicionar_elementos_cartograficos(ax, f'Cobertura da Operadora {operadora} em Sorocaba')
        n_erbs = len(subset_erb)
        cobertura_media = subset_erb['Raio_Cobertura_km'].mean()
        densidade_cobertura = n_erbs / 325
        info_text = (f"Total de ERBs: {n_erbs}\n"
                     f"Raio médio: {cobertura_media:.2f} km\n"
                     f"Densidade: {densidade_cobertura:.2f} ERBs/km²")
        ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                    fontsize=11, ha='left', va='top',
                    bbox=dict(boxstyle="round,pad=0.5", fc="white", ec=cor_base, alpha=0.8))
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.1, hspace=0.15)
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de cobertura por operadora salvo em {caminho_saida}")

def criar_mapa_sobreposicao(gdf_erb, gdf_setores, caminho_saida):
    print("Criando mapa de sobreposição de cobertura (versão otimizada)...")
    try:
        # Reprojetar para Web Mercator
        gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
        gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
        bbox = gdf_erb_3857.total_bounds
        x_min, y_min, x_max, y_max = bbox
        grid_size = GRID_SIZE_SOBREPOSICAO  # uso do parâmetro reduzido
        transform = from_bounds(x_min, y_min, x_max, y_max, grid_size, grid_size)
        contagem_sobreposicao = np.zeros((grid_size, grid_size), dtype=np.uint8)
        # Usar rasterização para cada operadora
        for operadora in cores_operadoras.keys():
            subset = gdf_setores_3857[gdf_setores_3857['Operadora'] == operadora]
            if not subset.empty:
                shapes = [(geom, 1) for geom in subset.geometry if geom.is_valid]
                if shapes:
                    mask = rasterize(
                        shapes=shapes,
                        out_shape=(grid_size, grid_size),
                        transform=transform,
                        all_touched=True,
                        fill=0,
                        dtype=np.uint8
                    )
                    contagem_sobreposicao += mask
        fig, ax = plt.subplots(figsize=(16, 12))
        im = ax.imshow(contagem_sobreposicao, extent=[x_min, x_max, y_min, y_max],
                       cmap=plt.cm.viridis, origin='lower', alpha=0.7, interpolation='bilinear')
        # Plotar limites dos setores para cada operadora
        for operadora in cores_operadoras.keys():
            subset = gdf_setores_3857[gdf_setores_3857['Operadora'] == operadora]
            if not subset.empty:
                subset.boundary.plot(ax=ax, color=cores_operadoras[operadora], linewidth=1, alpha=0.8)
        # Plotar as ERBs
        for operadora, cor in cores_operadoras.items():
            subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
            if not subset.empty:
                subset.plot(ax=ax, color=cor, markersize=20, edgecolor='white', linewidth=0.5)
        ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        adicionar_elementos_cartograficos(ax, 'Sobreposição de Cobertura entre Operadoras em Sorocaba (Otimizado)',
                                         "Operadoras sobrepostas: CLARO, OI, TIM, VIVO")
        cbar = plt.colorbar(im, ax=ax, shrink=0.7)
        cbar.set_label('Número de Operadoras com Cobertura', fontsize=12)
        cbar.set_ticks(range(5))
        cbar.set_ticklabels(['Sem cobertura', '1 operadora', '2 operadoras', '3 operadoras', '4 operadoras'])
        criar_legenda_personalizada(ax, cores_operadoras, "Operadoras")
        plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"Mapa de sobreposição salvo em {caminho_saida}")
    except Exception as e:
        print(f"Erro ao criar mapa de sobreposição (otimizado): {e}")
        plt.close('all')

def criar_mapa_calor_potencia(gdf_erb, caminho_saida):
    print("Criando mapa de calor de potência...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    bbox = gdf_erb_3857.total_bounds
    x_min, y_min, x_max, y_max = bbox
    margin = 0.01
    x_min -= margin; y_min -= margin; x_max += margin; y_max += margin
    grid_size = GRID_SIZE_CALOR  # uso de resolução reduzida
    xi = np.linspace(x_min, x_max, grid_size)
    yi = np.linspace(y_min, y_max, grid_size)
    xi, yi = np.meshgrid(xi, yi)
    pontos = np.array([(p.x, p.y) for p in gdf_erb_3857.geometry])
    valores = gdf_erb_3857['EIRP_dBm'].values
    grid_potencia = griddata(pontos, valores, (xi, yi), method='cubic', fill_value=np.min(valores))
    vmin = np.percentile(grid_potencia, 5)
    vmax = np.percentile(grid_potencia, 95)
    cmap = LinearSegmentedColormap.from_list('potencia',
                ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1',
                 '#6baed6', '#4292c6', '#2171b5', '#08519c', '#08306b'], N=256)
    im = ax.imshow(grid_potencia, extent=[x_min, x_max, y_min, y_max],
                   origin='lower', cmap=cmap, alpha=0.8, vmin=vmin, vmax=vmax, aspect='auto')
    contornos = ax.contour(xi, yi, grid_potencia, levels=5, colors='white', alpha=0.6, linewidths=0.8)
    plt.clabel(contornos, inline=1, fontsize=8, fmt='%.1f dBm')
    scatter = ax.scatter(gdf_erb_3857.geometry.x, gdf_erb_3857.geometry.y,
                         c=gdf_erb_3857['EIRP_dBm'], cmap=cmap, s=50, edgecolor='white',
                         linewidth=0.5, alpha=0.9, vmin=vmin, vmax=vmax)
    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
    adicionar_elementos_cartograficos(ax, 'Potência Efetivamente Irradiada (EIRP) das ERBs em Sorocaba')
    cbar = plt.colorbar(im, ax=ax, shrink=0.7)
    cbar.set_label('EIRP (dBm)', fontsize=12)
    eirp_min = gdf_erb_3857['EIRP_dBm'].min()
    eirp_max = gdf_erb_3857['EIRP_dBm'].max()
    eirp_media = gdf_erb_3857['EIRP_dBm'].mean()
    info_text = (f"EIRP Média: {eirp_media:.1f} dBm\n"
                 f"EIRP Mínima: {eirp_min:.1f} dBm\n"
                 f"EIRP Máxima: {eirp_max:.1f} dBm")
    ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                fontsize=11, ha='left', va='top',
                bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de calor de potência salvo em {caminho_saida}")

def criar_mapa_vulnerabilidade(gdf_erb, gdf_setores, caminho_saida):
    print("Criando mapa de vulnerabilidade de cobertura...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    bbox = gdf_erb_3857.total_bounds
    x_min, y_min, x_max, y_max = bbox
    hex_size = 0.005  # tamanho aproximado para cada hexágono
    hex_centers = []
    for x in np.arange(x_min, x_max, hex_size):
        for y in np.arange(y_min, y_max, hex_size * 0.866):
            offset = hex_size / 2 if int((y - y_min) / hex_size) % 2 == 1 else 0
            hex_centers.append((x + offset, y))
    def create_hexagon(center, size):
        angles = np.linspace(0, 2*np.pi, 7)[:-1]
        x_coords = center[0] + size * np.cos(angles)
        y_coords = center[1] + size * np.sin(angles) * 0.866
        return Polygon(zip(x_coords, y_coords))
    hex_polygons = [create_hexagon(center, hex_size / 1.5) for center in hex_centers]
    gdf_hex = gpd.GeoDataFrame(geometry=hex_polygons, crs=gdf_erb_3857.crs)
    operadoras = gdf_erb_3857['Operadora'].unique()
    cobertura_hex = []
    for hex_geom in gdf_hex.geometry:
        operadoras_presentes = set()
        for op in operadoras:
            setores_op = gdf_setores_3857[gdf_setores_3857['Operadora'] == op]
            if any(setor.intersects(hex_geom) for setor in setores_op.geometry):
                operadoras_presentes.add(op)
        cobertura_hex.append(len(operadoras_presentes))
    gdf_hex['num_operadoras'] = cobertura_hex
    gdf_hex['vulnerabilidade'] = pd.cut(gdf_hex['num_operadoras'],
                                         bins=[-1, 0, 1, 2, 5],
                                         labels=['Sem cobertura', 'Vulnerável', 'Redundância baixa', 'Cobertura ideal'])
    cores_vulnerabilidade = {
        'Sem cobertura': '#d73027',
        'Vulnerável': '#fc8d59',
        'Redundância baixa': '#fee090',
        'Cobertura ideal': '#1a9850'
    }
    gdf_hex['cor'] = gdf_hex['vulnerabilidade'].map(cores_vulnerabilidade)
    for categoria, cor in cores_vulnerabilidade.items():
        subset = gdf_hex[gdf_hex['vulnerabilidade'] == categoria]
        if not subset.empty:
            subset.plot(ax=ax, color=cor, edgecolor='white', linewidth=0.3, alpha=0.7, label=categoria)
    areas_criticas = gdf_hex[gdf_hex['num_operadoras'] <= 1]
    if not areas_criticas.empty:
        areas_criticas.plot(ax=ax, facecolor='none', edgecolor='#d73027', linewidth=0.8, alpha=0.9)
    for operadora, cor in cores_operadoras.items():
        subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        if not subset.empty:
            subset.plot(ax=ax, color=cor, markersize=20, alpha=0.7, edgecolor='white', linewidth=0.5)
    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
    adicionar_elementos_cartograficos(ax, 'Análise de Vulnerabilidade de Cobertura em Sorocaba')
    elementos_legenda = [Patch(facecolor=cor, edgecolor='white', alpha=0.7, label=cat)
                          for cat, cor in cores_vulnerabilidade.items()]
    for operadora, cor in cores_operadoras.items():
        elementos_legenda.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=cor,
                                        markersize=8, label=f'ERB {operadora}'))
    ax.legend(handles=elementos_legenda, title="Vulnerabilidade e ERBs",
              loc='upper right', frameon=True, framealpha=0.8, edgecolor='k')
    total_areas = len(gdf_hex)
    areas_sem_cobertura = len(gdf_hex[gdf_hex['num_operadoras'] == 0])
    areas_vulneraveis = len(gdf_hex[gdf_hex['num_operadoras'] == 1])
    areas_redundantes = len(gdf_hex[gdf_hex['num_operadoras'] >= 2])
    info_text = (f"Áreas sem cobertura: {areas_sem_cobertura} ({areas_sem_cobertura/total_areas*100:.1f}%)\n"
                 f"Áreas vulneráveis: {areas_vulneraveis} ({areas_vulneraveis/total_areas*100:.1f}%)\n"
                 f"Áreas com redundância: {areas_redundantes} ({areas_redundantes/total_areas*100:.1f}%)")
    ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                fontsize=11, ha='left', va='top',
                bbox=dict(boxstyle="round,pad=0.5", fc="white", ec='black', alpha=0.8))
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Mapa de vulnerabilidade salvo em {caminho_saida}")

def criar_mapa_voronoi_setores(gdf_erb, caminho_saida):
    print("Criando mapa de setores de Voronoi...")
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    fig, ax = plt.subplots(figsize=(16, 12))
    pontos = np.array([(p.x, p.y) for p in gdf_erb_3857.geometry])
    if len(pontos) < 4:
        print("Dados insuficientes para criar diagrama de Voronoi")
        plt.close(fig)
        return
    try:
        vor = Voronoi(pontos)
        bounds = gdf_erb_3857.total_bounds
        boundary = Polygon([(bounds[0], bounds[1]), (bounds[2], bounds[1]),
                            (bounds[2], bounds[3]), (bounds[0], bounds[3])])
        regions = []
        for region_idx, region in enumerate(vor.regions):
            if not region or -1 in region:
                continue
            polygon = [vor.vertices[i] for i in region]
            if len(polygon) < 3:
                continue
            poly = Polygon(polygon)
            if poly.is_valid:
                poly = poly.intersection(boundary)
                if not poly.is_empty:
                    regions.append((region_idx, poly))
        region_to_erb = {}
        for i, region_idx in enumerate(vor.point_region):
            region_to_erb[region_idx] = i
        voronoi_regions = []
        for region_idx, poly in regions:
            if region_idx in region_to_erb:
                erb_idx = region_to_erb[region_idx]
                if erb_idx < len(gdf_erb_3857):
                    voronoi_regions.append({
                        'geometry': poly,
                        'operadora': gdf_erb_3857.iloc[erb_idx]['Operadora'],
                        'eirp': gdf_erb_3857.iloc[erb_idx]['EIRP_dBm'],
                        'erb_idx': erb_idx
                    })
        if not voronoi_regions:
            print("Nenhuma região de Voronoi válida gerada")
            plt.close(fig)
            return
        gdf_voronoi = gpd.GeoDataFrame(voronoi_regions, crs=gdf_erb_3857.crs)
        for operadora, cor in cores_operadoras.items():
            subset = gdf_voronoi[gdf_voronoi['operadora'] == operadora]
            if not subset.empty:
                subset.plot(ax=ax, color=cor, edgecolor='white', linewidth=0.8, alpha=0.4)
        for linha in vor.ridge_vertices:
            if -1 not in linha:
                x, y = vor.vertices[linha, 0], vor.vertices[linha, 1]
                ax.plot(x, y, 'k-', linewidth=0.5, alpha=0.3)
        for operadora, cor in cores_operadoras.items():
            subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
            if not subset.empty:
                subset.plot(ax=ax, color=cor, markersize=30, marker='o',
                            edgecolor='white', linewidth=0.7, alpha=0.9)
        ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
        adicionar_elementos_cartograficos(ax, 'Análise de Setores de Voronoi por Operadora em Sorocaba')
        criar_legenda_personalizada(ax, cores_operadoras, "Operadoras")
        ax.annotate("Células de Voronoi representam áreas onde cada ponto está mais próximo\n"
                    "de uma determinada ERB do que de qualquer outra ERB.",
                    xy=(0.5, 0.03), xycoords='figure fraction', ha='center',
                    fontsize=10, bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
        estatisticas = gdf_voronoi.groupby('operadora')['geometry'].agg(lambda x: sum(geom.area for geom in x))
        estatisticas = (estatisticas / estatisticas.sum() * 100).round(1)
        info_text = "Distribuição espacial por operadora:\n"
        for op, valor in estatisticas.items():
            info_text += f"{op}: {valor:.1f}%\n"
        ax.annotate(info_text, xy=(0.02, 0.96), xycoords='axes fraction',
                    fontsize=10, ha='left', va='top',
                    bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
        plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"Mapa de setores de Voronoi salvo em {caminho_saida}")
    except Exception as e:
        print(f"Erro ao criar diagrama de Voronoi: {e}")
        plt.close(fig)

def criar_mapa_3d_cobertura(gdf_erb, gdf_setores, caminho_saida):
    print("Criando visualização 3D da cobertura...")
    from mpl_toolkits.mplot3d import Axes3D
    gdf_erb_3857 = gdf_erb.to_crs(epsg=3857)
    gdf_setores_3857 = gdf_setores.to_crs(epsg=3857)
    fig = plt.figure(figsize=(16, 12))
    ax = fig.add_subplot(111, projection='3d')
    xmin, ymin, xmax, ymax = gdf_erb_3857.total_bounds
    n_bins = 50
    x_edges = np.linspace(xmin, xmax, n_bins)
    y_edges = np.linspace(ymin, ymax, n_bins)
    densidade_cobertura = np.zeros((n_bins-1, n_bins-1))
    potencia_media = np.zeros((n_bins-1, n_bins-1))
    for i in range(n_bins-1):
        for j in range(n_bins-1):
            x_min_cell, x_max_cell = x_edges[i], x_edges[i+1]
            y_min_cell, y_max_cell = y_edges[j], y_edges[j+1]
            cell_poly = Polygon([(x_min_cell, y_min_cell),
                                 (x_max_cell, y_min_cell),
                                 (x_max_cell, y_max_cell),
                                 (x_min_cell, y_max_cell)])
            setores_na_celula = gdf_setores_3857[gdf_setores_3857.geometry.intersects(cell_poly)]
            densidade_cobertura[j, i] = len(setores_na_celula)
            if len(setores_na_celula) > 0:
                potencia_media[j, i] = setores_na_celula['EIRP_dBm'].mean()
            else:
                potencia_media[j, i] = np.nan
    densidade_max = np.percentile(densidade_cobertura[densidade_cobertura > 0], 95)
    densidade_norm = np.clip(densidade_cobertura / densidade_max, 0, 1)
    densidade_norm = np.where(densidade_norm > 0, densidade_norm, np.nan)
    X, Y = np.meshgrid((x_edges[:-1] + x_edges[1:]) / 2,
                       (y_edges[:-1] + y_edges[1:]) / 2)
    Z = densidade_norm * 5  # escala para visualização
    vmin = np.nanpercentile(potencia_media, 5)
    vmax = np.nanpercentile(potencia_media, 95)
    norm = Normalize(vmin=vmin, vmax=vmax)
    surf = ax.plot_surface(X, Y, Z,
                           facecolors=plt.cm.viridis(norm(potencia_media)),
                           rstride=1, cstride=1, alpha=0.8,
                           shade=True, edgecolor='none')
    for operadora, cor in cores_operadoras.items():
        subset = gdf_erb_3857[gdf_erb_3857['Operadora'] == operadora]
        if not subset.empty:
            ax.scatter(subset.geometry.x, subset.geometry.y,
                       np.ones(len(subset)) * 5.2,
                       color=cor, s=50, edgecolor='white',
                       linewidth=0.5, alpha=0.9, label=operadora)
    ax.set_title('Visualização 3D da Cobertura de ERBs em Sorocaba', fontsize=16, pad=20)
    ax.set_xlabel('Longitude', fontsize=12, labelpad=10)
    ax.set_ylabel('Latitude', fontsize=12, labelpad=10)
    ax.set_zlabel('Intensidade de Cobertura', fontsize=12, labelpad=10)
    ax.grid(False)
    ax.view_init(elev=35, azim=220)
    m = plt.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis)
    m.set_array([])
    cbar = fig.colorbar(m, ax=ax, shrink=0.5, pad=0.1)
    cbar.set_label('Potência Média (EIRP dBm)', fontsize=10)
    ax.legend(title="Operadoras", loc="upper right", frameon=True, framealpha=0.8)
    ax.text2D(0.02, 0.02,
              "Altura representa intensidade de cobertura.\n"
              "Cor representa potência média (EIRP) das ERBs.",
              transform=ax.transAxes, fontsize=10,
              bbox=dict(boxstyle="round,pad=0.5", fc="white", alpha=0.8))
    plt.figtext(0.5, 0.01,
                f"Dados: Anatel (ERBs) | Processado em: {datetime.now().strftime('%d/%m/%Y')}",
                ha='center', fontsize=8)
    plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Visualização 3D salva em {caminho_saida}")

def gerar_mapa_interativo(gdf_erb, gdf_setores, caminho_saida):
    print("Gerando mapa interativo...")
    gdf_erb_wgs84 = gdf_erb.to_crs(epsg=4326)
    gdf_setores_wgs84 = gdf_setores.to_crs(epsg=4326)
    centro = [gdf_erb_wgs84.geometry.y.mean(), gdf_erb_wgs84.geometry.x.mean()]
    mapa = folium.Map(location=centro, zoom_start=13, tiles='CartoDB positron')
    folium.LayerControl().add_to(mapa)
    operadoras = gdf_erb['Operadora'].unique()
    grupos_operadoras = {op: folium.FeatureGroup(name=f"ERBs {op}") for op in operadoras}
    grupos_setores = {op: folium.FeatureGroup(name=f"Cobertura {op}") for op in operadoras}
    for operadora in operadoras:
        cor = cores_operadoras[operadora]
        setores_op = gdf_setores_wgs84[gdf_setores_wgs84['Operadora'] == operadora]
        for idx, row in setores_op.iterrows():
            geojson = folium.GeoJson(row.geometry.__geo_interface__,
                                     style_function=lambda x, cor=cor: {
                                         'fillColor': cor,
                                         'color': cor,
                                         'weight': 1,
                                         'fillOpacity': 0.3
                                     })
            folium.Popup(
                f"<b>Operadora:</b> {operadora}<br>"
                f"<b>EIRP:</b> {row['EIRP_dBm']:.1f} dBm<br>"
                f"<b>Raio:</b> {row['Raio_Cobertura_km']:.2f} km<br>"
                f"<b>Área:</b> {row['Area_Cobertura_km2']:.2f} km²"
            ).add_to(geojson)
            geojson.add_to(grupos_setores[operadora])
    for operadora in operadoras:
        cor = cores_operadoras[operadora]
        erbs_op = gdf_erb_wgs84[gdf_erb_wgs84['Operadora'] == operadora]
        cluster = MarkerCluster(name=f"ERBs {operadora}")
        for idx, row in erbs_op.iterrows():
            icone = folium.Icon(color='white', icon_color=cor, icon='antenna', prefix='fa')
            marcador = folium.Marker(
                location=[row.geometry.y, row.geometry.x],
                icon=icone,
                popup=folium.Popup(
                    f"<b>Operadora:</b> {operadora}<br>"
                    f"<b>EIRP:</b> {row['EIRP_dBm']:.1f} dBm<br>"
                    f"<b>Tecnologia:</b> {row['Tecnologia']}<br>"
                    f"<b>Frequência:</b> {row['FreqTxMHz']:.1f} MHz<br>"
                    f"<b>Raio estimado:</b> {row['Raio_Cobertura_km']:.2f} km",
                    max_width=300
                )
            )
            marcador.add_to(cluster)
        cluster.add_to(grupos_operadoras[operadora])
    pontos_calor = []
    for idx, row in gdf_erb_wgs84.iterrows():
        peso = (row['EIRP_dBm'] - gdf_erb['EIRP_dBm'].min()) / (gdf_erb['EIRP_dBm'].max() - gdf_erb['EIRP_dBm'].min())
        peso = max(0.1, min(1.0, peso))
        centro = [row.geometry.y, row.geometry.x]
        raio_km = row['Raio_Cobertura_km']
        pontos_calor.append(centro + [peso * 1.5])
        azimute = row['Azimute']
        if not pd.isna(azimute):
            azimute_rad = np.radians((450 - float(azimute)) % 360)
            angulo_setor = np.radians(ANGULO_SETOR)
            for i in range(10):
                angulo = azimute_rad - angulo_setor / 2 + i * (angulo_setor / 9)
                for dist_fator in [0.3, 0.6, 0.9]:
                    dist = raio_km * dist_fator
                    dx = dist * np.cos(angulo) / 111.32
                    dy = dist * np.sin(angulo) / (111.32 * np.cos(np.radians(centro[0])))
                    peso_ajustado = peso * (1 - 0.7 * dist_fator)
                    pontos_calor.append([centro[0] + dy, centro[1] + dx, peso_ajustado])
    mapa_calor = HeatMap(
        pontos_calor,
        name="Mapa de Calor de Cobertura",
        min_opacity=0.4,
        max_zoom=13,
        radius=25,
        blur=15,
        gradient={
            0.0: 'blue',
            0.25: 'cyan',
            0.5: 'lime',
            0.75: 'yellow',
            1.0: 'red'
        }
    )
    mapa_calor.add_to(mapa)
    for operadora in operadoras:
        grupos_setores[operadora].add_to(mapa)
        grupos_operadoras[operadora].add_to(mapa)
    titulo_html = '''
    <div style="position: fixed; top: 10px; left: 50%; transform: translateX(-50%);
         z-index: 9999; background-color: white; padding: 10px; border-radius: 5px;
         box-shadow: 0 0 10px rgba(0,0,0,0.2); font-family: Arial, sans-serif;
         font-size: 16px; font-weight: bold;">
         Mapa Interativo de Cobertura de ERBs em Sorocaba
    </div>
    '''
    mapa.get_root().html.add_child(folium.Element(titulo_html))
    info_html = f'''
    <div style="position: fixed; bottom: 10px; left: 10px; z-index: 9999; background-color: white;
         padding: 8px; border-radius: 5px; box-shadow: 0 0 5px rgba(0,0,0,0.2);
         font-family: Arial, sans-serif; font-size: 12px;">
         <b>Dados:</b> Anatel (ERBs) | <b>Gerado em:</b> {datetime.now().strftime('%d/%m/%Y')}
    </div>
    '''
    mapa.get_root().html.add_child(folium.Element(info_html))
    mapa.save(caminho_saida)
    print(f"Mapa interativo salvo em {caminho_saida}")

def gerar_visualizacoes(gdf_erb, gdf_setores, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    caminho_mapa_posicionamento = os.path.join(output_dir, "01_mapa_posicionamento_erbs.png")
    caminho_mapa_cobertura = os.path.join(output_dir, "02_mapa_cobertura_por_operadora.png")
    caminho_mapa_sobreposicao = os.path.join(output_dir, "03_mapa_sobreposicao_cobertura.png")
    caminho_mapa_calor_potencia = os.path.join(output_dir, "04_mapa_calor_potencia.png")
    caminho_mapa_vulnerabilidade = os.path.join(output_dir, "05_mapa_vulnerabilidade.png")
    caminho_dashboard = os.path.join(output_dir, "06_dashboard_comparativo.png")
    caminho_mapa_voronoi = os.path.join(output_dir, "07_mapa_setores_voronoi.png")
    caminho_visualizacao_3d = os.path.join(output_dir, "08_visualizacao_3d_cobertura.png")
    caminho_mapa_interativo = os.path.join(output_dir, "09_mapa_interativo.html")

    try:
        criar_mapa_posicionamento(gdf_erb, caminho_mapa_posicionamento)
    except Exception as e:
        print(f"Erro ao criar mapa de posicionamento: {e}")

    try:
        criar_mapa_cobertura_por_operadora(gdf_erb, gdf_setores, caminho_mapa_cobertura)
    except Exception as e:
        print(f"Erro ao criar mapa de cobertura por operadora: {e}")

    try:
        criar_mapa_sobreposicao(gdf_erb, gdf_setores, caminho_mapa_sobreposicao)
    except Exception as e:
        print(f"Erro ao criar mapa de sobreposição: {e}")

    try:
        criar_mapa_calor_potencia(gdf_erb, caminho_mapa_calor_potencia)
    except Exception as e:
        print(f"Erro ao criar mapa de calor de potência: {e}")

    try:
        criar_mapa_vulnerabilidade(gdf_erb, gdf_setores, caminho_mapa_vulnerabilidade)
    except Exception as e:
        print(f"Erro ao criar mapa de vulnerabilidade: {e}")

    try:
        criar_dashboard_operadoras(gdf_erb, gdf_setores, caminho_dashboard)
    except Exception as e:
        print(f"Erro ao criar dashboard: {e}")

    try:
        criar_mapa_voronoi_setores(gdf_erb, caminho_mapa_voronoi)
    except Exception as e:
        print(f"Erro ao criar mapa de setores Voronoi: {e}")

    try:
        criar_mapa_3d_cobertura(gdf_erb, gdf_setores, caminho_visualizacao_3d)
    except Exception as e:
        print(f"Erro ao criar visualização 3D: {e}")

    try:
        gerar_mapa_interativo(gdf_erb, gdf_setores, caminho_mapa_interativo)
    except Exception as e:
        print(f"Erro ao gerar mapa interativo: {e}")

    try:
        gdf_erb.to_file(os.path.join(output_dir, "erb_sorocaba_processado.geojson"), driver="GeoJSON")
    except Exception as e:
        print(f"Erro ao salvar ERBs processadas: {e}")

    try:
        gdf_setores_simples = gdf_setores.copy()
        gdf_setores_simples['geometry'] = gdf_setores_simples.geometry.buffer(0).simplify(0.0001)
        gdf_setores_simples.to_file(os.path.join(output_dir, "erb_sorocaba_setores_simplificado.geojson"), driver="GeoJSON")
    except Exception as e:
        print(f"Erro ao salvar setores: {e}")

    print(f"Todas as visualizações foram tentadas. Verifique a pasta: {output_dir}")

def main():
    try:
        caminho_arquivo = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_sorocaba.csv'
        print(f"Iniciando processamento do arquivo: {caminho_arquivo}")
        gdf_erb, gdf_setores = carregar_e_processar_dados(caminho_arquivo)
        gerar_visualizacoes(gdf_erb, gdf_setores, output_dir)
        print("Processamento concluído com sucesso!")
    except Exception as e:
        print(f"Erro crítico durante o processamento: {e}")

if __name__ == "__main__":
    main()

!pip install -U numpy geopandas

# Parte 1 - Carregamento e pré-processamento dos dados de ERBs
from google.colab import drive
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_CSV = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_sorocaba.csv'
CAMINHO_CSV_FILTRADO = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_filtradas.csv'
CAMINHO_GPKG_SAIDA = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'

# Função para carregar e limpar dados
def carregar_dados_erbs(caminho_csv):
    df = pd.read_csv(caminho_csv)
    operadoras_alvo = ['CLARO', 'OI', 'VIVO', 'TIM', 'CLARO S.A.', 'OI MÓVEL S.A.',
                       'TELEFÔNICA BRASIL S.A.', 'TIM S.A.']
    df = df[df['NomeEntidade'].str.contains('|'.join(operadoras_alvo), case=False, na=False)]
    mapeamento = {
        'CLARO': 'CLARO', 'CLARO S.A.': 'CLARO',
        'OI': 'OI', 'OI MÓVEL S.A.': 'OI',
        'VIVO': 'VIVO', 'TELEFÔNICA BRASIL S.A.': 'VIVO',
        'TIM': 'TIM', 'TIM S.A.': 'TIM'
    }
    df['Operadora'] = df['NomeEntidade'].apply(lambda x: next((v for k, v in mapeamento.items() if k in x.upper()), 'OUTRA'))
    df = df[df['Operadora'].isin(['CLARO', 'OI', 'VIVO', 'TIM'])]
    sorocaba_bbox = [-23.60, -23.30, -47.65, -47.25]
    df = df[
        (df['Latitude'] >= sorocaba_bbox[0]) & (df['Latitude'] <= sorocaba_bbox[1]) &
        (df['Longitude'] >= sorocaba_bbox[2]) & (df['Longitude'] <= sorocaba_bbox[3])
    ]
    df = df.drop_duplicates(subset=['Latitude', 'Longitude', 'Operadora'])
    return df.reset_index(drop=True)

# Executar carregamento e salvar CSV
print("Carregando e filtrando dados...")
df_erbs = carregar_dados_erbs(CAMINHO_CSV)
df_erbs.to_csv(CAMINHO_CSV_FILTRADO, index=False)
print(f"CSV filtrado salvo em: {CAMINHO_CSV_FILTRADO}")

# Converter para GeoDataFrame e salvar em GPKG
geometry = [Point(xy) for xy in zip(df_erbs['Longitude'], df_erbs['Latitude'])]
gdf_erbs = gpd.GeoDataFrame(df_erbs, geometry=geometry, crs="EPSG:4326")
gdf_erbs.to_file(CAMINHO_GPKG_SAIDA, layer='erbs_pontos', driver='GPKG')
print(f"GeoPackage salvo com camada 'erbs_pontos' em: {CAMINHO_GPKG_SAIDA}")

df_erbs.head()

# Parte 2 - Conversão de tipos, preenchimento de valores ausentes e criação do GeoDataFrame
from google.colab import drive
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import numpy as np

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_CSV_FILTRADO = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_filtradas.csv'
CAMINHO_GPKG_SAIDA = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'

# Função para preparar os dados
def preparar_dados_para_geo(caminho_csv, caminho_gpkg):
    df = pd.read_csv(caminho_csv)

    if 'Operadora' not in df.columns:
        raise ValueError("A coluna 'Operadora' não está presente no arquivo CSV. Certifique-se de que a filtragem foi feita.")

    df['PotenciaTransmissorWatts'] = pd.to_numeric(df['PotenciaTransmissorWatts'], errors='coerce')
    df['FreqTxMHz'] = pd.to_numeric(df['FreqTxMHz'], errors='coerce')
    df['GanhoAntena'] = pd.to_numeric(df['GanhoAntena'], errors='coerce')
    df['Azimute'] = pd.to_numeric(df['Azimute'], errors='coerce')

    for col, default in [('PotenciaTransmissorWatts', 20.0), ('FreqTxMHz', 850.0)]:
        medias = df.groupby('Operadora')[col].median()
        for op, val in medias.items():
            mask = (df['Operadora'] == op) & ((df[col].isna()) | (df[col] <= 0))
            df.loc[mask, col] = val if pd.notna(val) else default

    ganho_medio = df['GanhoAntena'].median()
    df.loc[(df['GanhoAntena'].isna()) | (df['GanhoAntena'] <= 0), 'GanhoAntena'] = ganho_medio if pd.notna(ganho_medio) else 16.0

    azimutes_padrao = [0, 120, 240]
    for i, row in df.iterrows():
        if pd.isna(row['Azimute']):
            df.at[i, 'Azimute'] = azimutes_padrao[i % len(azimutes_padrao)]

    df.to_csv(caminho_csv, index=False)

    geometria = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]
    gdf = gpd.GeoDataFrame(df, geometry=geometria, crs="EPSG:4326")
    gdf.to_file(caminho_gpkg, layer='erbs_preparadas', driver='GPKG')
    return gdf

# Executar Parte 2
print("Executando preparação dos dados...")
gdf_erbs = preparar_dados_para_geo(CAMINHO_CSV_FILTRADO, CAMINHO_GPKG_SAIDA)
print(f"GeoPackage atualizado com camada 'erbs_preparadas' em: {CAMINHO_GPKG_SAIDA}")

gdf_erbs.head()

# Parte 3 - Cálculo de EIRP, raio de cobertura e área de cobertura
from google.colab import drive
import pandas as pd
import geopandas as gpd
import numpy as np

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_CSV = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_filtradas.csv'
CAMINHO_GPKG = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'

SENSIBILIDADE_RECEPTOR = -100  # dBm
ANGULO_SETOR = 120  # graus

# Funções auxiliares
def calcular_eirp(potencia_watts, ganho_antena):
    try:
        potencia = float(potencia_watts)
        if potencia <= 0:
            return np.nan
    except (ValueError, TypeError):
        return np.nan
    return 10 * np.log10(potencia * 1000) + ganho_antena

def calcular_raio_cobertura_aprimorado(eirp, freq_mhz, tipo_area='urbana'):
    if np.isnan(eirp) or np.isnan(freq_mhz) or freq_mhz <= 0:
        return np.nan
    atenuacao = {'urbana_densa': 22, 'urbana': 16, 'suburbana': 12, 'rural': 8}
    raio_base = 10 ** ((eirp - SENSIBILIDADE_RECEPTOR - 32.44 - 20 * np.log10(freq_mhz)) / 20)
    raio_ajustado = raio_base * 0.7 / (atenuacao.get(tipo_area, 16) / 10)
    limite_freq = min(7, 15000 / freq_mhz) if freq_mhz > 0 else 5
    return min(raio_ajustado, limite_freq)

def calcular_area_cobertura(raio, angulo=ANGULO_SETOR):
    if np.isnan(raio):
        return np.nan
    return (np.pi * raio**2 * angulo) / 360

def calcular_densidade_erb(ponto, gdf, raio=0.01):
    buffer = ponto.buffer(raio)
    return len(gdf[gdf.geometry.intersects(buffer)])

# Carregar CSV com dados preparados
df = pd.read_csv(CAMINHO_CSV)
geometria = gpd.points_from_xy(df['Longitude'], df['Latitude'])
gdf = gpd.GeoDataFrame(df, geometry=geometria, crs="EPSG:4326")

# Calcular densidade e tipo de área
gdf['densidade_erb'] = gdf.geometry.apply(lambda p: calcular_densidade_erb(p, gdf))
gdf['tipo_area'] = pd.cut(gdf['densidade_erb'], bins=[0, 3, 6, 10, float('inf')],
                          labels=['rural', 'suburbana', 'urbana', 'urbana_densa'])
gdf['tipo_area'] = gdf['tipo_area'].fillna('urbana')

# Cálculos principais
gdf['EIRP_dBm'] = gdf.apply(lambda row: calcular_eirp(row['PotenciaTransmissorWatts'], row['GanhoAntena']), axis=1)
gdf['Raio_Cobertura_km'] = gdf.apply(lambda row: calcular_raio_cobertura_aprimorado(row['EIRP_dBm'], row['FreqTxMHz'], row['tipo_area']), axis=1)
gdf['Area_Cobertura_km2'] = gdf['Raio_Cobertura_km'].apply(calcular_area_cobertura)

# Atualiza CSV com novos dados (sem geometria)
gdf.drop(columns='geometry').to_csv(CAMINHO_CSV, index=False)

# Salva como nova camada no GPKG
gdf.to_file(CAMINHO_GPKG, layer='erbs_com_cobertura', driver='GPKG')

print("Parte 3 executada com sucesso. Arquivos salvos:")
print(f"- CSV atualizado: {CAMINHO_CSV}")
print(f"- GPKG atualizado com camada 'erbs_com_cobertura': {CAMINHO_GPKG}")

# Parte 4 - Geração dos setores de cobertura (polígonos)
from google.colab import drive
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, Polygon
import numpy as np

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_CSV = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_filtradas.csv'
CAMINHO_GPKG = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'

ANGULO_SETOR = 120

# Função para criar setor a partir de ponto, raio e azimute
def criar_setor_preciso(lat, lon, raio, azimute, angulo=ANGULO_SETOR, resolucao=30):
    if np.isnan(raio) or np.isnan(azimute) or raio <= 0:
        return None
    azimute_rad = np.radians((450 - float(azimute)) % 360)
    metade_angulo = np.radians(angulo / 2)
    pontos = [(lon, lat)]
    for i in range(resolucao + 1):
        angulo_atual = azimute_rad - metade_angulo + (i * 2 * metade_angulo / resolucao)
        for j in [0.8, 0.9, 0.95, 1.0]:
            dist = raio * j
            dx = dist * np.cos(angulo_atual) / 111.32
            dy = dist * np.sin(angulo_atual) / (111.32 * np.cos(np.radians(lat)))
            pontos.append((lon + dx, lat + dy))
    pontos.append((lon, lat))
    try:
        return Polygon(pontos)
    except:
        return None

# Carrega CSV com cálculos de cobertura
df = pd.read_csv(CAMINHO_CSV)
geometria = gpd.points_from_xy(df['Longitude'], df['Latitude'])
gdf = gpd.GeoDataFrame(df, geometry=geometria, crs="EPSG:4326")

# Cria setores com geometria poligonal
gdf['setor_geometria'] = gdf.apply(lambda row: criar_setor_preciso(row['Latitude'], row['Longitude'],
                                                                    row['Raio_Cobertura_km'], row['Azimute']), axis=1)
gdf_setores = gpd.GeoDataFrame(
    gdf[['Operadora', 'EIRP_dBm', 'Raio_Cobertura_km', 'Area_Cobertura_km2', 'tipo_area']],
    geometry=gdf['setor_geometria'],
    crs="EPSG:4326"
).dropna(subset=['geometry'])

# Salvar camada dos setores
gdf_setores.to_file(CAMINHO_GPKG, layer='erbs_setores', driver='GPKG')

print("Parte 4 executada com sucesso. Arquivo salvo:")
print(f"- GPKG atualizado com camada 'erbs_setores': {CAMINHO_GPKG}")

# Parte 5 - Correção para uso de fiona
import fiona

# Salvar a camada 'hex_vulnerabilidade' no GPKG
layer_name = 'hex_vulnerabilidade'
gdf_hex_4326 = gdf_hex.to_crs(epsg=4326)

# Verifica se a camada já existe no GPKG
with fiona.Env():
    with fiona.open(CAMINHO_GPKG, layer=None, driver="GPKG") as src:
        existing_layers = src.schema

    # Reabre para listar camadas corretamente
    layers_disponiveis = fiona.listlayers(CAMINHO_GPKG)
    if layer_name in layers_disponiveis:
        # Remove camada anterior (não remove o arquivo todo)
        with fiona.open(CAMINHO_GPKG, mode='a', driver="GPKG") as gpkg:
            gpkg.remove_layer(layer_name)

# Salva a nova camada
gdf_hex_4326.to_file(CAMINHO_GPKG, layer=layer_name, driver='GPKG')

print("Parte 5 executada com sucesso. Arquivos salvos:")
print(f"- Mapa PNG: {CAMINHO_SAIDA_PARTE5_MAPA}")
print(f"- Arquivo GPKG com camada '{layer_name}': {CAMINHO_GPKG}")

# Parte 7.2 - Mapa de Voronoi com salvamento em GPKG
from google.colab import drive
import geopandas as gpd
import numpy as np
from shapely.geometry import Polygon
from scipy.spatial import Voronoi
import matplotlib.pyplot as plt

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_GPKG = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'
CAMINHO_SAIDA_VORONOI = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/parte7_voronoi.png'

# Carregar pontos e projetar para cálculo
gdf_pontos = gpd.read_file(CAMINHO_GPKG, layer='erbs_pontos')
gdf_proj = gdf_pontos.to_crs(epsg=3857)

# Gerar coordenadas e Voronoi
pontos = np.array([(geom.x, geom.y) for geom in gdf_proj.geometry])
vor = Voronoi(pontos)

# Plotar gráfico
fig, ax = plt.subplots(figsize=(10, 10))
bounds = gdf_proj.total_bounds
ax.set_xlim(bounds[[0, 2]])
ax.set_ylim(bounds[[1, 3]])

# Desenhar regiões válidas
polygons = []
for region in vor.regions:
    if not region or -1 in region:
        continue
    coords = [vor.vertices[i] for i in region if i != -1]
    poly = Polygon(coords)
    if poly.is_valid:
        gpd.GeoSeries([poly], crs=gdf_proj.crs).plot(ax=ax, color='none', edgecolor='black', linewidth=0.5)
        polygons.append(poly)

# Plotar ERBs com cor
cores_operadoras = {'CLARO': '#E02020', 'OI': '#FFD700', 'VIVO': '#9932CC', 'TIM': '#0000CD'}
gdf_proj.plot(ax=ax, color=[cores_operadoras.get(op, '#999') for op in gdf_proj['Operadora']], markersize=15)
ax.set_title("Setores de Voronoi por ERB")
plt.savefig(CAMINHO_SAIDA_VORONOI)
plt.close()
print(f"Parte 7.2 executada: Voronoi salvo em {CAMINHO_SAIDA_VORONOI}")

# Criar GeoDataFrame com polígonos e salvar no GPKG
gdf_voronoi = gpd.GeoDataFrame(geometry=polygons, crs=gdf_proj.crs).to_crs(epsg=4326)
gdf_voronoi.to_file(CAMINHO_GPKG, layer='voronoi', driver='GPKG')
print(f"Camada 'voronoi' salva no GPKG: {CAMINHO_GPKG}")

# Parte 7.3 - Visualização 3D da cobertura e salvamento da grade no GPKG
from google.colab import drive
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from shapely.geometry import box
from matplotlib.colors import Normalize
import pandas as pd
import os
import fiona

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_GPKG = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'
CAMINHO_SAIDA_3D = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/parte7_3d.png'
layer_name = 'cobertura_estimativa_2d'

# Carregar dados e reprojetar
gdf_setores = gpd.read_file(CAMINHO_GPKG, layer='erbs_setores').to_crs(epsg=3857)
gdf_pontos = gpd.read_file(CAMINHO_GPKG, layer='erbs_pontos').to_crs(epsg=3857)

# Definir grade espacial com base nos setores
bounds = gdf_setores.total_bounds
xmin, ymin, xmax, ymax = bounds
n_bins = 50
x_edges = np.linspace(xmin, xmax, n_bins + 1)
y_edges = np.linspace(ymin, ymax, n_bins + 1)

# Inicializar arrays
densidade = np.zeros((n_bins, n_bins))
potencia = np.full((n_bins, n_bins), np.nan)
geometrias, potencias, densidades = [], [], []

# Processar cada célula da grade
for i in range(n_bins):
    for j in range(n_bins):
        x0, x1 = x_edges[i], x_edges[i+1]
        y0, y1 = y_edges[j], y_edges[j+1]
        cell = box(x0, y0, x1, y1)
        subset = gdf_setores[gdf_setores.geometry.intersects(cell)]
        dens = len(subset)
        densidade[j, i] = dens
        eirp = subset['EIRP_dBm'].mean() if dens > 0 else np.nan
        potencia[j, i] = eirp
        geometrias.append(cell)
        potencias.append(eirp)
        densidades.append(dens)

# Gerar visualização 3D
fig = plt.figure(figsize=(14, 10))
ax = fig.add_subplot(111, projection='3d')
X, Y = np.meshgrid((x_edges[:-1] + x_edges[1:]) / 2,
                   (y_edges[:-1] + y_edges[1:]) / 2)
Z = densidade

norm = Normalize(vmin=np.nanpercentile(potencia, 5),
                 vmax=np.nanpercentile(potencia, 95))
colors = plt.cm.viridis(norm(np.nan_to_num(potencia, nan=0)))

surf = ax.plot_surface(X, Y, Z, facecolors=colors, rstride=1, cstride=1,
                       linewidth=0, antialiased=False, shade=True)

ax.set_title("Visualização 3D da Cobertura (ERBs)", fontsize=15, pad=20)
ax.set_xlabel("X (m)")
ax.set_ylabel("Y (m)")
ax.set_zlabel("Densidade de Cobertura")
ax.view_init(elev=35, azim=235)
m = plt.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis)
m.set_array([])
fig.colorbar(m, ax=ax, shrink=0.5, pad=0.1, label="Potência Média (EIRP dBm)")
plt.savefig(CAMINHO_SAIDA_3D, dpi=300, bbox_inches='tight')
plt.close()
print(f"Parte 7.3 executada: gráfico 3D salvo em {CAMINHO_SAIDA_3D}")

# Salvar camada no GPKG (sobrescrevendo se já existir)
gdf_grid = gpd.GeoDataFrame({
    'densidade': densidades,
    'potencia_eirp': potencias
}, geometry=geometrias, crs=gdf_setores.crs).to_crs(epsg=4326)

# Remover camada anterior se existir
if layer_name in fiona.listlayers(CAMINHO_GPKG):
    with fiona.Env():
        layers_existentes = fiona.listlayers(CAMINHO_GPKG)
        temp_layers = [l for l in layers_existentes if l != layer_name]
        gdf_temp = [gpd.read_file(CAMINHO_GPKG, layer=l) for l in temp_layers]
        temp_path = CAMINHO_GPKG.replace('.gpkg', '_temp.gpkg')
        for i, gdf in enumerate(gdf_temp):
            gdf.to_file(temp_path, layer=temp_layers[i], driver='GPKG')
        os.replace(temp_path, CAMINHO_GPKG)

# Agora salva a nova camada
gdf_grid.to_file(CAMINHO_GPKG, layer=layer_name, driver='GPKG')
print(f"Camada '{layer_name}' salva no arquivo: {CAMINHO_GPKG}")

# Parte 7.4 - Mapa Interativo com Folium (corrigido e revisado)
from google.colab import drive
import geopandas as gpd
import folium
from folium import Popup, GeoJson
import pandas as pd

# Montar o Google Drive
drive.mount('/content/drive', force_remount=True)

CAMINHO_GPKG = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'
CAMINHO_SAIDA_MAPA_HTML = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mapa_interativo_erbs.html'

# Carregar camadas atualizadas
gdf_pontos = gpd.read_file(CAMINHO_GPKG, layer='erbs_pontos')
gdf_setores = gpd.read_file(CAMINHO_GPKG, layer='erbs_setores')
gdf_hex = gpd.read_file(CAMINHO_GPKG, layer='hex_vulnerabilidade')
gdf_grade = gpd.read_file(CAMINHO_GPKG, layer='cobertura_estimativa_2d')

# Reprojetar para WGS84
gdf_pontos = gdf_pontos.to_crs(epsg=4326)
gdf_setores = gdf_setores.to_crs(epsg=4326)
gdf_hex = gdf_hex.to_crs(epsg=4326)
gdf_grade = gdf_grade.to_crs(epsg=4326)

# Centro do mapa
centro = [gdf_pontos.geometry.y.mean(), gdf_pontos.geometry.x.mean()]
mapa = folium.Map(location=centro, zoom_start=12, tiles='CartoDB positron')

# Cores por operadora
cores_operadoras = {
    'CLARO': '#E02020',     # vermelho
    'OI': '#FFD700',        # amarelo
    'VIVO': '#9932CC',      # roxo
    'TIM': '#0000CD'        # azul escuro
}

# Adicionar setores de cobertura por operadora
for op in gdf_setores['Operadora'].unique():
    subset = gdf_setores[gdf_setores['Operadora'] == op]
    cor = cores_operadoras.get(op, '#888888')
    grupo = folium.FeatureGroup(name=f'Setores {op}')
    for _, row in subset.iterrows():
        popup = Popup(f"""
            <b>Operadora:</b> {op}<br>
            <b>EIRP:</b> {row['EIRP_dBm']:.1f} dBm<br>
            <b>Raio:</b> {row['Raio_Cobertura_km']:.2f} km""", max_width=250)
        geo = GeoJson(row.geometry.__geo_interface__,
                      style_function=lambda x, cor=cor: {
                          'fillColor': cor,
                          'color': cor,
                          'weight': 1,
                          'fillOpacity': 0.3
                      })
        geo.add_child(popup)
        geo.add_to(grupo)
    grupo.add_to(mapa)

# Adicionar pontos das ERBs individuais com cor por operadora
grupo_erbs = folium.FeatureGroup(name='ERBs')
for _, row in gdf_pontos.iterrows():
    try:
        pot = float(row['PotenciaTransmissorWatts'])
        eirp = float(row['EIRP_dBm']) if 'EIRP_dBm' in row else None
        freq = float(row['FreqTxMHz'])
        ganho = float(row['GanhoAntena'])
        cor = cores_operadoras.get(row['Operadora'], '#888888')

        info = f"""
        <b>Operadora:</b> {row['Operadora']}<br>
        <b>Frequência:</b> {freq:.1f} MHz<br>
        <b>Ganho:</b> {ganho:.1f} dBi<br>
        <b>Potência:</b> {pot:.1f} W"""
        if eirp:
            info += f"<br><b>EIRP:</b> {eirp:.1f} dBm"

        popup = folium.Popup(info, max_width=300)

        folium.CircleMarker(
            location=(row.geometry.y, row.geometry.x),
            radius=7,
            color=cor,
            fill=True,
            fill_color=cor,
            fill_opacity=1.0,
            weight=2,
            popup=popup
        ).add_to(grupo_erbs)
    except Exception as e:
        print(f"Erro ao adicionar ponto: {e}")
grupo_erbs.add_to(mapa)

# Adicionar vulnerabilidade como camada colorida
cores_vulnerabilidade = {
    'Sem cobertura': '#d73027',
    'Vulnerável': '#fc8d59',
    'Redundância baixa': '#fee090',
    'Cobertura ideal': '#1a9850'
}
vuln_group = folium.FeatureGroup(name='Vulnerabilidade')
for _, row in gdf_hex.iterrows():
    cat = row['vulnerabilidade']
    cor = cores_vulnerabilidade.get(cat, '#cccccc')
    geo = GeoJson(row.geometry.__geo_interface__,
                  style_function=lambda x, cor=cor: {
                      'fillColor': cor,
                      'color': cor,
                      'weight': 0.5,
                      'fillOpacity': 0.4
                  })
    popup = folium.Popup(f"<b>Vulnerabilidade:</b> {cat}", max_width=200)
    geo.add_child(popup)
    geo.add_to(vuln_group)
vuln_group.add_to(mapa)

# Adicionar camada de grade com coloração por densidade
grade_group = folium.FeatureGroup(name='Cobertura Estimada (Grade)')
for _, row in gdf_grade.iterrows():
    dens = row['densidade'] if pd.notna(row['densidade']) else 0
    cor = '#fef0d9'  # default
    if dens >= 10:
        cor = '#bd0026'
    elif dens >= 7:
        cor = '#f03b20'
    elif dens >= 4:
        cor = '#fd8d3c'
    elif dens >= 1:
        cor = '#fecc5c'
    tooltip = folium.Tooltip(f"Densidade: {dens}<br>Potência: {row['potencia_eirp']:.1f} dBm")
    geo = GeoJson(row.geometry.__geo_interface__,
                  style_function=lambda x, cor=cor: {
                      'fillColor': cor,
                      'color': 'black',
                      'weight': 0.2,
                      'fillOpacity': 0.3
                  })
    geo.add_child(tooltip)
    geo.add_to(grade_group)
grade_group.add_to(mapa)

# Adicionar controle de camadas
folium.LayerControl(collapsed=False).add_to(mapa)

# Salvar
mapa.save(CAMINHO_SAIDA_MAPA_HTML)
print(f"Parte 7.4 executada com sucesso. Mapa salvo em: {CAMINHO_SAIDA_MAPA_HTML}")

import geopandas as gpd
import fiona
import json
import pandas as pd

def serialize_record(record):
    """
    Converte os objetos de geometria para WKT, se existirem.
    """
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            record[key] = value.wkt
    return record

# Caminho para o arquivo .gpkg
file_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'

# Obter a lista de camadas do arquivo GPKG
layers = fiona.listlayers(file_path)
print("Camadas encontradas:", layers)

# Dicionário que armazenará o relatório de análise de cada camada
analysis_report = {}

# Iterar sobre cada camada encontrada
for layer in layers:
    print(f"\nAnalisando a camada: {layer}")
    # Carrega a camada como um GeoDataFrame
    gdf = gpd.read_file(file_path, layer=layer)

    # Cria um dicionário com informações da camada
    report = {}
    report['layer_name'] = layer
    report['num_records'] = len(gdf)
    report['columns'] = list(gdf.columns)
    # Obtém os tipos de dados de cada coluna e converte para string
    report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

    # Seleciona uma amostra aleatória de até 30 linhas
    num_amostras = min(30, len(gdf))
    sample_df = gdf.sample(n=num_amostras, random_state=42) if len(gdf) > 0 else gdf
    # Converte a amostra para dicionário e serializa os objetos de geometria
    sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
    report['sample_records'] = sample_records

    # Se houver colunas numéricas, calcula estatísticas descritivas
    numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
    if numeric_cols:
        report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
    else:
        report['numeric_summary'] = {}

    analysis_report[layer] = report

# Salvar o relatório de análise na pasta "MBA" do Google Drive
output_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analysis_report.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_report, f, ensure_ascii=False, indent=4)
print(f"\nRelatório de análise salvo em: {output_file}")

# Imprimir o relatório de cada camada no console
for layer, report in analysis_report.items():
    print("\n" + "="*80)
    print("Camada:", layer)
    print("Número de registros:", report['num_records'])
    print("Colunas:", report['columns'])
    print("Tipos de dados:", report['dtypes'])
    print("\nAmostra de até 30 linhas:")
    for rec in report['sample_records']:
        print(rec)
    print("\nEstatísticas descritivas (colunas numéricas):")
    print(report['numeric_summary'])

import geopandas as gpd
import pandas as pd
import json
import numpy as np
from shapely.geometry import Polygon, Point, mapping
import fiona
import os
from tqdm import tqdm

def serialize_record(record):
    """
    Converte os objetos de geometria para WKT, se existirem.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def consolidar_erbs(erbs_preparadas, erbs_com_cobertura, erbs_setores):
    """
    Consolida as camadas de ERBs em uma única camada otimizada.

    Args:
        erbs_preparadas: GeoDataFrame com os dados preparados das ERBs
        erbs_com_cobertura: GeoDataFrame com dados de cobertura das ERBs
        erbs_setores: GeoDataFrame com a setorização das ERBs

    Returns:
        GeoDataFrame consolidado
    """
    print("Consolidando camadas de ERBs...")

    # Cria uma cópia dos dados preparados como base
    erbs_unificadas = erbs_preparadas.copy()

    # Seleciona apenas as colunas fundamentais
    colunas_base = [
        'Status.state', 'NomeEntidade', 'Operadora', 'Tecnologia', 'tipoTecnologia',
        'FreqTxMHz', 'FreqRxMHz', 'Azimute', 'GanhoAntena', 'AnguloElevacao',
        'Polarizacao', 'AlturaAntena', 'PotenciaTransmissorWatts',
        'Latitude', 'Longitude', 'Municipio.NomeMunicipio', 'geometry'
    ]

    # Filtra apenas as colunas desejadas, mantendo as que existem
    colunas_existentes = [col for col in colunas_base if col in erbs_unificadas.columns]
    erbs_unificadas = erbs_unificadas[colunas_existentes]

    # Adiciona os atributos de cobertura
    colunas_cobertura = [
        'densidade_erb', 'tipo_area', 'EIRP_dBm',
        'Raio_Cobertura_km', 'Area_Cobertura_km2'
    ]

    # Cria um ID único para cada ERB baseado em latitude e longitude
    erbs_unificadas['erb_id'] = erbs_unificadas.apply(
        lambda x: f"{x['Operadora']}_{x['Latitude']:.6f}_{x['Longitude']:.6f}",
        axis=1
    )

    erbs_com_cobertura['erb_id'] = erbs_com_cobertura.apply(
        lambda x: f"{x['Operadora']}_{x['Latitude']:.6f}_{x['Longitude']:.6f}",
        axis=1
    )

    # Adiciona as colunas de cobertura
    for coluna in colunas_cobertura:
        if coluna in erbs_com_cobertura.columns:
            erbs_unificadas = erbs_unificadas.merge(
                erbs_com_cobertura[['erb_id', coluna]],
                on='erb_id',
                how='left'
            )

    # Adiciona uma coluna para a geometria setorial
    erbs_unificadas['geometria_setorial'] = None

    # Cria um mapeamento para os setores
    setores_dict = {}
    for idx, row in erbs_setores.iterrows():
        operadora = row['Operadora']
        setores_dict[operadora] = setores_dict.get(operadora, []) + [row]

    # Associa a geometria setorial
    for idx, row in erbs_unificadas.iterrows():
        operadora = row['Operadora']
        if operadora in setores_dict:
            # Aqui teríamos que fazer um matching mais sofisticado
            # Por simplicidade, atribuímos o primeiro setor da operadora
            # Em um caso real, verificaríamos proximidade espacial
            if len(setores_dict[operadora]) > 0:
                erbs_unificadas.at[idx, 'geometria_setorial'] = str(setores_dict[operadora][0]['geometry'])

    # Remove a coluna de ID temporária
    erbs_unificadas.drop('erb_id', axis=1, inplace=True)

    print(f"Consolidação concluída. Total de {len(erbs_unificadas)} registros unificados.")
    return erbs_unificadas

def consolidar_analises_espaciais(hex_vulnerabilidade, cobertura_estimativa_2d):
    """
    Consolida os dados de vulnerabilidade e cobertura em uma única camada de análise.

    Args:
        hex_vulnerabilidade: GeoDataFrame com dados de vulnerabilidade
        cobertura_estimativa_2d: GeoDataFrame com dados de cobertura 2D

    Returns:
        GeoDataFrame consolidado
    """
    print("Consolidando análises espaciais...")

    # Primeiro, unificamos os sistemas de coordenadas
    if hex_vulnerabilidade.crs != cobertura_estimativa_2d.crs:
        cobertura_estimativa_2d = cobertura_estimativa_2d.to_crs(hex_vulnerabilidade.crs)

    # Criamos um novo GeoDataFrame para armazenar a análise consolidada
    analise_consolidada = hex_vulnerabilidade.copy()

    # Adicionamos informações de densidade e potência média
    analise_consolidada['densidade_media'] = 0
    analise_consolidada['potencia_media'] = np.nan

    # Processamos cada hexágono
    print("Processando hexágonos...")
    for idx, hex_row in tqdm(analise_consolidada.iterrows(), total=len(analise_consolidada)):
        # Encontramos células da cobertura que intersectam com o hexágono
        intersecoes = cobertura_estimativa_2d[cobertura_estimativa_2d.intersects(hex_row.geometry)]

        if len(intersecoes) > 0:
            # Calculamos a densidade média e potência média ponderada pela área
            areas = []
            densidades = []
            potencias = []

            for _, cel_row in intersecoes.iterrows():
                area_intersecao = hex_row.geometry.intersection(cel_row.geometry).area
                areas.append(area_intersecao)
                densidades.append(cel_row.densidade)
                potencias.append(cel_row.potencia_eirp if not pd.isna(cel_row.potencia_eirp) else 0)

            if sum(areas) > 0:
                analise_consolidada.at[idx, 'densidade_media'] = sum(d * a for d, a in zip(densidades, areas)) / sum(areas)

                # Apenas calculamos a média de potência se houver valores não nulos
                valid_potencias = [p for p, a in zip(potencias, areas) if p > 0 and a > 0]
                valid_areas = [a for p, a in zip(potencias, areas) if p > 0 and a > 0]

                if valid_areas and sum(valid_areas) > 0:
                    analise_consolidada.at[idx, 'potencia_media'] = sum(p * a for p, a in zip(valid_potencias, valid_areas)) / sum(valid_areas)

    # Criamos uma classificação de cobertura baseada em densidade e potência
    analise_consolidada['classificacao_cobertura'] = analise_consolidada.apply(
        lambda x: classificar_cobertura(x['num_operadoras'], x['potencia_media']),
        axis=1
    )

    print(f"Consolidação de análises concluída. Total de {len(analise_consolidada)} hexágonos processados.")
    return analise_consolidada

def classificar_cobertura(num_operadoras, potencia_media):
    """
    Classifica a cobertura baseada no número de operadoras e potência média.
    """
    if pd.isna(potencia_media) or num_operadoras == 0:
        return "Sem cobertura"

    if num_operadoras == 1:
        if potencia_media < 60:
            return "Vulnerável - Sinal fraco"
        else:
            return "Vulnerável - Sinal adequado"
    elif num_operadoras == 2:
        if potencia_media < 60:
            return "Redundância baixa - Sinal fraco"
        else:
            return "Redundância baixa - Sinal adequado"
    else:
        if potencia_media < 60:
            return "Cobertura ideal - Sinal fraco"
        else:
            return "Cobertura ideal - Sinal adequado"

def analisar_camadas(file_path):
    """
    Realiza uma análise detalhada das camadas do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG

    Returns:
        Dicionário com o relatório de análise
    """
    print(f"Analisando camadas do arquivo: {file_path}")

    # Obter a lista de camadas do arquivo GPKG
    layers = fiona.listlayers(file_path)
    print(f"Camadas encontradas: {layers}")

    # Dicionário que armazenará o relatório de análise de cada camada
    analysis_report = {}

    # Iterar sobre cada camada encontrada
    for layer in layers:
        print(f"\nAnalisando a camada: {layer}")

        # Carrega a camada como um GeoDataFrame
        gdf = gpd.read_file(file_path, layer=layer)

        # Cria um dicionário com informações da camada
        report = {}
        report['layer_name'] = layer
        report['num_records'] = len(gdf)
        report['columns'] = list(gdf.columns)

        # Obtém os tipos de dados de cada coluna e converte para string
        report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

        # Seleciona uma amostra aleatória de até 30 linhas
        num_amostras = min(30, len(gdf))
        sample_df = gdf.sample(n=num_amostras, random_state=42) if len(gdf) > 0 else gdf

        # Converte a amostra para dicionário e serializa os objetos de geometria
        sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
        report['sample_records'] = sample_records

        # Se houver colunas numéricas, calcula estatísticas descritivas
        numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
        else:
            report['numeric_summary'] = {}

        # Análise adicional: informações sobre a geometria
        if 'geometry' in gdf.columns:
            report['geometry_type'] = str(gdf.geometry.geom_type.value_counts().to_dict())
            report['geometry_is_valid'] = str(gdf.geometry.is_valid.value_counts().to_dict())
            report['geometry_bounds'] = str(gdf.total_bounds.tolist())

        # Análise adicional: verificação de valores nulos
        report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

        # Análise adicional: valores únicos para colunas categóricas
        categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
        if categorical_cols:
            unique_values = {}
            for col in categorical_cols:
                if col != 'geometry':
                    unique_counts = gdf[col].value_counts().head(10).to_dict()  # Top 10 valores
                    unique_values[col] = {
                        'count': len(gdf[col].unique()),
                        'top_values': unique_counts
                    }
            report['categorical_summary'] = unique_values

        analysis_report[layer] = report

    return analysis_report

def main():
    # Caminho para o arquivo .gpkg original
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_camadas.gpkg'

    # Caminho para o arquivo .gpkg otimizado
    output_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_camada_otimizado.gpkg'

    # Caminho para o relatório de análise
    report_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analysis_report_otimizado.json'

    try:
        # 1. Carregar as camadas originais
        print("Carregando camadas originais...")
        erbs_preparadas = gpd.read_file(input_file, layer='erbs_preparadas')
        erbs_com_cobertura = gpd.read_file(input_file, layer='erbs_com_cobertura')
        erbs_setores = gpd.read_file(input_file, layer='erbs_setores')
        hex_vulnerabilidade = gpd.read_file(input_file, layer='hex_vulnerabilidade')
        cobertura_estimativa_2d = gpd.read_file(input_file, layer='cobertura_estimativa_2d')

        # 2. Consolidar as camadas de ERBs
        erbs_unificadas = consolidar_erbs(erbs_preparadas, erbs_com_cobertura, erbs_setores)

        # 3. Consolidar as análises espaciais
        analise_consolidada = consolidar_analises_espaciais(hex_vulnerabilidade, cobertura_estimativa_2d)

        # 4. Salvar as camadas otimizadas no arquivo de saída
        print(f"Salvando camadas otimizadas em: {output_file}")
        erbs_unificadas.to_file(output_file, layer='erbs_unificadas', driver='GPKG')
        analise_consolidada.to_file(output_file, layer='analise_consolidada', driver='GPKG')

        # Como referência, também salvar as outras camadas originais que podem ser úteis
        print("Copiando camadas complementares...")
        voronoi = gpd.read_file(input_file, layer='voronoi')
        voronoi.to_file(output_file, layer='voronoi', driver='GPKG')

        # 5. Realizar a análise do arquivo otimizado
        print("Realizando análise do arquivo otimizado...")
        analysis_report = analisar_camadas(output_file)

        # 6. Salvar o relatório de análise
        print(f"Salvando relatório de análise em: {report_file}")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_report, f, ensure_ascii=False, indent=4)

        print("Processo de otimização e análise concluído com sucesso!")

    except Exception as e:
        print(f"Erro durante o processamento: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# Instalar bibliotecas necessárias caso não estejam presentes
!pip install geopandas rasterio numpy cupy-cuda11x rasterstats

import geopandas as gpd
import rasterio
from rasterio.warp import transform_bounds
import numpy as np
import os
import time
from rasterstats import zonal_stats, point_query
import cupy as cp  # Para processamento CUDA
from shapely.geometry import Point, Polygon, mapping

!pip uninstall cupy-cuda11x cupy-cuda12x -y
!pip install cupy-cuda11x

import geopandas as gpd
import rasterio
from rasterio.warp import transform_bounds
import numpy as np
import os
import time
from rasterstats import zonal_stats, point_query
import cupy as cp  # Para processamento CUDA
from shapely.geometry import Point, Polygon, mapping
from tqdm import tqdm

def process_erbs_with_dem_cuda():
    start_time = time.time()

    # Caminhos dos arquivos
    erbs_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_camada_otimizado.gpkg'
    dem_path = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/mde_sorocaba.tif'
    output_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_camada_com_altimetria.gpkg'

    print("Carregando dados de ERBs...")
    # Identificar as camadas disponíveis no GeoPackage
    layers = fiona.listlayers(erbs_path)
    print(f"Camadas disponíveis no GeoPackage: {layers}")

    # Carregar a camada principal de ERBs
    if 'erbs_unificadas' in layers:
        erbs = gpd.read_file(erbs_path, layer='erbs_unificadas')
        layer_name = 'erbs_unificadas'
    else:
        # Se não encontrar a camada específica, carregar a primeira camada
        erbs = gpd.read_file(erbs_path, layer=layers[0])
        layer_name = layers[0]

    print(f"Carregados {len(erbs)} registros da camada {layer_name}")

    # Verificar se já existe uma coluna de elevação
    if 'altitude_m' in erbs.columns:
        print("Coluna de altitude já existe, será sobrescrita")

    print("Carregando o Modelo Digital de Elevação...")
    with rasterio.open(dem_path) as dem_src:
        # Obter metadados do raster
        dem_meta = dem_src.meta
        dem_crs = dem_src.crs

        # Verificar se os CRS são compatíveis
        if erbs.crs != dem_crs:
            print(f"Reprojetando ERBs de {erbs.crs} para {dem_crs}")
            erbs = erbs.to_crs(dem_crs)

        print("Processando extração de altitude com CUDA...")

        # Converter o raster para um array CUDA
        dem_data = dem_src.read(1)  # Ler a banda 1 (elevação)
        dem_transform = dem_src.transform

        # Transferir o array para GPU
        if cp.is_available():
            dem_data_gpu = cp.asarray(dem_data)
            print("Usando aceleração CUDA")
        else:
            print("CUDA não disponível, usando processamento CPU")
            dem_data_gpu = dem_data

        # Função para extrair elevação usando GPU
        def get_elevation_at_point(x, y):
            # Converter coordenadas para índices do raster
            row, col = rasterio.transform.rowcol(dem_transform, x, y)

            # Verificar se o ponto está dentro dos limites do raster
            if 0 <= row < dem_data.shape[0] and 0 <= col < dem_data.shape[1]:
                # Extrair valor do raster na GPU
                if cp.is_available():
                    elevation = dem_data_gpu[row, col].get()  # Transferir de volta para CPU
                else:
                    elevation = dem_data[row, col]

                # Verificar valores inválidos (geralmente -9999 ou similar para NoData)
                if elevation < -1000:  # Valor arbitrário para detectar NoData
                    return np.nan
                return float(elevation)
            else:
                return np.nan

        # Opção 1: Abordagem vetorizada com rasterstats (geralmente mais rápida)
        print("Usando abordagem vetorizada para extração de altitude...")

        if all(isinstance(geom, Point) for geom in erbs.geometry):
            # Para feições de ponto
            elevations = point_query(erbs.geometry, dem_path)
            erbs['altitude_m'] = elevations
        else:
            # Para feições de polígono ou linha
            stats = zonal_stats(erbs.geometry, dem_path, stats="mean")
            erbs['altitude_m'] = [s['mean'] if s else np.nan for s in stats]

        # Opção 2: Processamento ponto a ponto (alternativa caso a abordagem vetorizada falhe)
        if 'altitude_m' not in erbs.columns or erbs['altitude_m'].isna().all():
            print("Abordagem vetorizada falhou, usando processamento ponto a ponto...")
            # Extrair coordenadas dos centroides para polígonos ou as próprias coordenadas para pontos
            if all(isinstance(geom, Point) for geom in erbs.geometry):
                coords = [(p.x, p.y) for p in erbs.geometry]
            else:
                coords = [(p.centroid.x, p.centroid.y) for p in erbs.geometry]

            # Extrair elevação para cada ponto
            elevations = []
            for x, y in tqdm(coords):
                elev = get_elevation_at_point(x, y)
                elevations.append(elev)

            erbs['altitude_m'] = elevations

    # Cálculos adicionais relevantes para análise de propagação de sinal
    if 'AlturaAntena' in erbs.columns:
        # Altura total em relação ao nível do mar
        erbs['altura_total_m'] = erbs['altitude_m'] + erbs['AlturaAntena']
        print("Calculada altura total (terreno + antena)")

    # Calcular inclinação do terreno em um raio ao redor da ERB (opcional)
    # Este cálculo seria mais complexo e poderia ser implementado posteriormente

    # Calcular estatísticas básicas da coluna de elevação
    altitude_stats = {
        'min': erbs['altitude_m'].min(),
        'max': erbs['altitude_m'].max(),
        'mean': erbs['altitude_m'].mean(),
        'median': erbs['altitude_m'].median(),
        'std': erbs['altitude_m'].std(),
        'na_count': erbs['altitude_m'].isna().sum()
    }

    print("\nEstatísticas de elevação:")
    for stat, value in altitude_stats.items():
        print(f"{stat}: {value}")

    # Salvar resultado
    print(f"\nSalvando resultado em {output_path}")
    erbs.to_file(output_path, layer=layer_name, driver="GPKG")

    # Verificar se há outras camadas no arquivo original
    if len(layers) > 1:
        for layer in layers:
            if layer != layer_name:
                print(f"Copiando camada adicional: {layer}")
                other_layer = gpd.read_file(erbs_path, layer=layer)
                other_layer.to_file(output_path, layer=layer, driver="GPKG")

    elapsed_time = time.time() - start_time
    print(f"\nProcessamento concluído em {elapsed_time:.2f} segundos")
    print(f"Resultado salvo em: {output_path}")

    return erbs, altitude_stats

# Executar o processamento
if __name__ == "__main__":
    import fiona
    # Verificar a disponibilidade de CUDA
    print(f"CUDA disponível: {cp.is_available()}")
    if cp.is_available():
        print(f"Dispositivo CUDA: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}")

    erbs_with_altitude, stats = process_erbs_with_dem_cuda()

    # Visualização opcional (descomentar se desejar)
    """
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(1, 1, figsize=(12, 10))
    erbs_with_altitude.plot(column='altitude_m', cmap='terrain', legend=True, ax=ax)
    plt.title('ERBs com dados de elevação (m)')
    plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_altimetria_mapa.png', dpi=300)
    plt.show()
    """

import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx
from mpl_toolkits.axes_grid1 import make_axes_locatable

# Carregar dados
erbs = gpd.read_file('/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_camada_com_altimetria.gpkg', layer='erbs_unificadas')
erbs = erbs.to_crs(epsg=3857)  # Projetar para Web Mercator para usar basemap

# Criar visualização
fig, ax = plt.subplots(1, 1, figsize=(15, 12))

# Plotar ERBs coloridas por altitude
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.1)
erbs.plot(column='altitude_m', cmap='terrain', legend=True, ax=ax, cax=cax,
          markersize=erbs['PotenciaTransmissorWatts']/5, alpha=0.8)

# Adicionar mapa base
ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)

# Configurar plot
ax.set_title('Distribuição Espacial de ERBs por Altitude', fontsize=16)
ax.set_axis_off()
plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/MBA/erbs_altimetria_mapa.png', dpi=300, bbox_inches='tight')
plt.show()

import geopandas as gpd
import pandas as pd
import json
import numpy as np
from shapely.geometry import Polygon, Point, mapping
import fiona
import os
from tqdm import tqdm

def serialize_record(record):
    """
    Converte os objetos de geometria para WKT, se existirem,
    e converte valores numéricos do tipo numpy para tipos nativos do Python.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def analisar_camadas(file_path):
    """
    Realiza uma análise detalhada das camadas do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG

    Returns:
        Dicionário com o relatório de análise para cada camada
    """
    print(f"Analisando camadas do arquivo: {file_path}")

    # Obter a lista de camadas do arquivo GPKG
    layers = fiona.listlayers(file_path)
    print(f"Camadas encontradas: {layers}")

    # Dicionário que armazenará o relatório de análise de cada camada
    analysis_report = {}

    # Iterar sobre cada camada encontrada
    for layer in layers:
        print(f"\nAnalisando a camada: {layer}")

        # Carrega a camada como um GeoDataFrame
        gdf = gpd.read_file(file_path, layer=layer)

        # Cria um dicionário com informações da camada
        report = {}
        report['layer_name'] = layer
        report['num_records'] = len(gdf)
        report['columns'] = list(gdf.columns)

        # Obtém os tipos de dados de cada coluna e converte para string
        report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

        # Seleciona uma amostra aleatória de até 30 linhas
        num_amostras = min(30, len(gdf))
        sample_df = gdf.sample(n=num_amostras, random_state=42) if len(gdf) > 0 else gdf

        # Converte a amostra para dicionário e serializa as geometrias
        sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
        report['sample_records'] = sample_records

        # Se houver colunas numéricas, calcula estatísticas descritivas
        numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
        else:
            report['numeric_summary'] = {}

        # Análise adicional: informações sobre a geometria
        if 'geometry' in gdf.columns:
            report['geometry_type'] = str(gdf.geometry.geom_type.value_counts().to_dict())
            report['geometry_is_valid'] = str(gdf.geometry.is_valid.value_counts().to_dict())
            report['geometry_bounds'] = str(gdf.total_bounds.tolist())

        # Análise adicional: verificação de valores nulos
        report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

        # Análise adicional: valores únicos para colunas categóricas
        categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
        if categorical_cols:
            unique_values = {}
            for col in categorical_cols:
                if col != 'geometry':
                    unique_counts = gdf[col].value_counts().head(10).to_dict()  # Top 10 valores
                    unique_values[col] = {
                        'count': len(gdf[col].unique()),
                        'top_values': unique_counts
                    }
            report['categorical_summary'] = unique_values

        analysis_report[layer] = report

    return analysis_report

def main():
    # Caminho para o arquivo .gpkg com altimetria
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_camada_com_altimetria.gpkg'

    # Caminho para salvar o relatório de análise
    report_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analysis_report_otimizado.json'

    # Executa a análise das camadas
    analysis_report = analisar_camadas(input_file)

    # Salva o relatório de análise em formato JSON
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_report, f, ensure_ascii=False, indent=4)

    print(f"\nRelatório de análise salvo em: {report_file}")

if __name__ == "__main__":
    main()

import geopandas as gpd
import pandas as pd
import json
import numpy as np
from shapely.geometry import Polygon, Point, mapping
import fiona
import os
from tqdm import tqdm

def serialize_record(record):
    """
    Converte os objetos de geometria para WKT, se existirem,
    e converte valores numéricos do tipo numpy para tipos nativos do Python.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def analisar_camadas(file_path):
    """
    Realiza uma análise detalhada das camadas do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG

    Returns:
        Dicionário com o relatório de análise para cada camada
    """
    print(f"Analisando camadas do arquivo: {file_path}")

    # Obter a lista de camadas do arquivo GPKG
    layers = fiona.listlayers(file_path)
    print(f"Camadas encontradas: {layers}")

    # Dicionário que armazenará o relatório de análise de cada camada
    analysis_report = {}

    # Iterar sobre cada camada encontrada
    for layer in layers:
        print(f"\nAnalisando a camada: {layer}")

        # Carrega a camada como um GeoDataFrame
        gdf = gpd.read_file(file_path, layer=layer)

        # Cria um dicionário com informações da camada
        report = {}
        report['layer_name'] = layer
        report['num_records'] = len(gdf)
        report['columns'] = list(gdf.columns)

        # Obtém os tipos de dados de cada coluna e converte para string
        report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

        # Seleciona uma amostra aleatória de até 30 linhas
        num_amostras = min(30, len(gdf))
        sample_df = gdf.sample(n=num_amostras, random_state=42) if len(gdf) > 0 else gdf

        # Converte a amostra para dicionário e serializa as geometrias
        sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
        report['sample_records'] = sample_records

        # Se houver colunas numéricas, calcula estatísticas descritivas
        numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
        else:
            report['numeric_summary'] = {}

        # Análise adicional: informações sobre a geometria
        if 'geometry' in gdf.columns:
            report['geometry_type'] = str(gdf.geometry.geom_type.value_counts().to_dict())
            report['geometry_is_valid'] = str(gdf.geometry.is_valid.value_counts().to_dict())
            report['geometry_bounds'] = str(gdf.total_bounds.tolist())

        # Análise adicional: verificação de valores nulos
        report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

        # Análise adicional: valores únicos para colunas categóricas
        categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
        if categorical_cols:
            unique_values = {}
            for col in categorical_cols:
                if col != 'geometry':
                    unique_counts = gdf[col].value_counts().head(10).to_dict()  # Top 10 valores
                    unique_values[col] = {
                        'count': len(gdf[col].unique()),
                        'top_values': unique_counts
                    }
            report['categorical_summary'] = unique_values

        analysis_report[layer] = report

    return analysis_report

def main():
    # Caminho para o arquivo .gpkg com os dados integrados de Sorocaba com altimetria
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_integrados_alt.gpkg'

    # Caminho para salvar o relatório de análise
    report_file = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/analysis_report_pitcic.json'

    # Executa a análise das camadas
    analysis_report = analisar_camadas(input_file)

    # Salva o relatório de análise em formato JSON
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_report, f, ensure_ascii=False, indent=4)

    print(f"\nRelatório de análise salvo em: {report_file}")

if __name__ == "__main__":
    main()

import os
import geopandas as gpd
import numpy as np
import rasterio
from rasterio.transform import from_bounds
from shapely.geometry import Polygon, Point, LineString
from shapely import wkt
import math
import time
import logging
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
from pyproj import Geod
import pandas as pd
from tqdm.auto import tqdm
from datetime import datetime
from scipy.ndimage import convolve
from scipy.interpolate import griddata
import fiona
import contextily as ctx
from matplotlib.colors import LinearSegmentedColormap, Normalize
from matplotlib.patches import Patch
from matplotlib.lines import Line2D
from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar
import matplotlib.patheffects as PathEffects

# Configuração de logging
log_dir = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/logs"
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"processamento_erbs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ERB_Cobertura")

# Constantes
ANGULO_SETOR = 120      # Ângulo do setor (graus)
NUM_ANGULOS = 8         # Número de ângulos amostrados dentro do setor
NUM_PASSOS = 30         # Número de passos ao longo de cada direção (distância)
MARGIN = 10.0           # Margem mínima (m) para a linha de visada
DIFFRACTION_FACTOR = 0.2  # Fator de difração (20%)
REFLECTION_FACTOR_URBAN = 0.15  # Fator de reflexão em áreas urbanas
REFLECTION_FACTOR_DENSE = 0.25  # Fator de reflexão em áreas urbanas densas
CHECKPOINT_INTERVAL = 50  # Intervalo para salvar checkpoint (número de registros)
KM_TO_DEGREES = 1 / 111.32  # Aproximação: 1 km ≈ 1/111.32 graus

# Inicializar calculador geodésico para cálculos precisos
geod = Geod(ellps='WGS84')

def calcular_ponto_geodesico(lon, lat, azimute, distancia_km):
    """Calcula um ponto geodésico a partir de origem, azimute e distância."""
    lon2, lat2, _ = geod.fwd(lon, lat, azimute, distancia_km * 1000)
    return lon2, lat2

def amostra_segura_mde(mde_src, lon, lat, valor_default=None):
    """Amostra o MDE de forma segura, retornando um valor default se necessário."""
    try:
        bounds = mde_src.bounds
        if not (bounds.left <= lon <= bounds.right and bounds.bottom <= lat <= bounds.top):
            return valor_default
        for val in mde_src.sample([(lon, lat)]):
            sample_value = val[0]
            if sample_value < -1000 or np.isnan(sample_value):
                return valor_default
            return sample_value
    except Exception as e:
        logger.warning(f"Erro ao amostrar MDE em ({lon}, {lat}): {str(e)}")
        return valor_default

def calc_effective_radius(row, mde_path, original_radius_km,
                          angulo_setor=None, num_angulos=None,
                          num_passos=None, margin=None,
                          considerar_difracao=True, considerar_reflexao=True):
    """
    Calcula o raio de cobertura efetivo considerando a interferência do terreno.

    Abre o MDE localmente para evitar problemas de pickle e, para cada direção do setor,
    amostra pontos verificando se a linha de visada é comprometida.
    """
    try:
        angulo_setor = angulo_setor or ANGULO_SETOR
        num_angulos = num_angulos or NUM_ANGULOS
        num_passos = num_passos or NUM_PASSOS
        margin = margin or MARGIN

        lat = row['Latitude']
        lon = row['Longitude']
        h0 = row['altitude_m'] + row['AlturaAntena']  # Altura efetiva da antena
        azimute = row['Azimute']

        logger.debug(f"Processando ERB em ({lon}, {lat}), azimute: {azimute}°, altura: {h0}m")

        angulo_inicial = azimute - angulo_setor / 2
        angulo_final = azimute + angulo_setor / 2
        angulos = np.linspace(angulo_inicial, angulo_final, num_angulos)

        effective_radii = []
        for a in angulos:
            passo_distancias = np.linspace(0.1, original_radius_km, num_passos)
            raio_dir = original_radius_km
            with rasterio.open(mde_path) as mde_src:
                for d in passo_distancias:
                    sample_lon, sample_lat = calcular_ponto_geodesico(lon, lat, a, d)
                    terrain_elev = amostra_segura_mde(mde_src, sample_lon, sample_lat, row['altitude_m'])
                    h_feixe = h0 - d * 1000 * math.tan(math.radians(0.5))
                    delta_h = h_feixe - terrain_elev
                    if delta_h < margin:
                        raio_dir = d
                        logger.debug(f"Bloqueio na direção {a:.1f}° a {d:.2f} km (terreno: {terrain_elev}m)")
                        break
            effective_radii.append(raio_dir)

        los_radius = min(effective_radii) if effective_radii else original_radius_km
        final_radius = los_radius
        if considerar_difracao:
            final_radius = los_radius * (1 + DIFFRACTION_FACTOR)
        if considerar_reflexao and 'tipo_area' in row:
            if row['tipo_area'] == 'urbana_densa':
                final_radius = final_radius * (1 + REFLECTION_FACTOR_DENSE)
            elif row['tipo_area'] == 'urbana':
                final_radius = final_radius * (1 + REFLECTION_FACTOR_URBAN)
        final_radius = min(final_radius, original_radius_km)
        logger.debug(f"Raio efetivo: {final_radius:.2f} km (original: {original_radius_km:.2f} km)")
        return final_radius
    except Exception as e:
        logger.error(f"Erro ao calcular raio efetivo para ERB ({row['Longitude']}, {row['Latitude']}): {str(e)}")
        return original_radius_km * 0.9

def criar_setor_preciso(lat, lon, raio, azimute, angulo=ANGULO_SETOR, resolucao=36):
    """Cria um polígono representando o setor de cobertura da ERB."""
    try:
        if np.isnan(raio) or np.isnan(azimute) or raio <= 0:
            logger.warning(f"Parâmetros inválidos para criar setor: raio={raio}, azimute={azimute}")
            return None
        raio_km = float(raio)
        azimute_graus = float(azimute)
        pontos = [(lon, lat)]
        angulo_inicial = (azimute_graus - angulo/2) % 360
        for i in range(resolucao + 1):
            angulo_atual = angulo_inicial + (i * angulo / resolucao)
            for fator in [0.7, 0.8, 0.9, 0.95, 1.0]:
                dist = raio_km * fator
                x, y = calcular_ponto_geodesico(lon, lat, angulo_atual, dist)
                pontos.append((x, y))
        pontos.append((lon, lat))
        setor = Polygon(pontos)
        if not setor.is_valid:
            setor = setor.buffer(0)
            if not setor.is_valid:
                logger.warning(f"Não foi possível criar polígono válido para ({lon}, {lat})")
                return None
        return setor
    except Exception as e:
        logger.error(f"Erro em criar_setor_preciso para ({lon}, {lat}): {str(e)}")
        return None

def processar_erb(idx, row, mde_path, considerar_difracao=True, considerar_reflexao=True):
    """Processa uma única ERB para cálculo do raio efetivo e geometria do setor."""
    try:
        row_copy = row.copy()
        raio_original = float(row_copy['Raio_Cobertura_km'])
        raio_efetivo = calc_effective_radius(row_copy, mde_path, raio_original,
                                              considerar_difracao=considerar_difracao,
                                              considerar_reflexao=considerar_reflexao)
        area_efetiva = (np.pi * raio_efetivo**2 * ANGULO_SETOR) / 360 if not np.isnan(raio_efetivo) else np.nan
        setor_geometria = criar_setor_preciso(row_copy['Latitude'], row_copy['Longitude'], raio_efetivo, row_copy['Azimute'])
        geometria_valida = setor_geometria is not None and setor_geometria.is_valid
        resultado = {
            'Raio_Cobertura_Original_km': raio_original,
            'Raio_Cobertura_km': raio_efetivo,
            'Area_Cobertura_Original_km2': row_copy['Area_Cobertura_km2'],
            'Area_Cobertura_km2': area_efetiva,
            'setor_geometria': setor_geometria,
            'geometria_valida': geometria_valida
        }
        if idx % 10 == 0:
            logger.info(f"Processado registro {idx}, raio: {raio_original:.2f} km -> {raio_efetivo:.2f} km")
        return idx, resultado
    except Exception as e:
        logger.error(f"Erro ao processar ERB #{idx}: {str(e)}")
        return idx, {'erro': str(e)}

def main(input_gpkg, mde_path, output_gpkg,
         considerar_difracao=True, considerar_reflexao=True,
         n_jobs=-1):
    """
    Processa as ERBs considerando a interferência do terreno e salva os resultados.

    Args:
        input_gpkg: Caminho para o GeoPackage de entrada contendo a camada 'erbs_unificadas'
        mde_path: Caminho para o MDE (arquivo TIFF)
        output_gpkg: Caminho para o GeoPackage de saída
        considerar_difracao: Se True, aplica fator de difração
        considerar_reflexao: Se True, aplica fator de reflexão
        n_jobs: Número de processos paralelos (-1 para usar todos os cores)

    Retorna:
        (GeoDataFrame atualizado, estatísticas de processamento)
    """
    start_time = time.time()

    output_dir = os.path.dirname(output_gpkg)
    os.makedirs(output_dir, exist_ok=True)
    checkpoint_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)

    logger.info("Iniciando processamento de ERBs com interferência do terreno")
    logger.info(f"Arquivo de entrada: {input_gpkg}")
    logger.info(f"MDE: {mde_path}")
    logger.info(f"Arquivo de saída: {output_gpkg}")

    layers = fiona.listlayers(input_gpkg)
    logger.info(f"Camadas encontradas no arquivo de entrada: {layers}")

    try:
        erbs = gpd.read_file(input_gpkg, layer="erbs_unificadas")
        logger.info(f"Registros carregados: {len(erbs)}")
    except Exception as e:
        logger.error(f"Erro ao carregar camada 'erbs_unificadas': {str(e)}")
        raise

    erbs['Raio_Cobertura_Original_km'] = erbs['Raio_Cobertura_km'].copy()
    erbs['Area_Cobertura_Original_km2'] = erbs['Area_Cobertura_km2'].copy()

    logger.info(f"Processando {len(erbs)} registros usando {n_jobs} cores...")
    parallel_results = Parallel(n_jobs=n_jobs)(
        delayed(processar_erb)(idx, row, mde_path, considerar_difracao, considerar_reflexao)
        for idx, row in erbs.iterrows()
    )
    resultados = sorted(parallel_results, key=lambda x: x[0])
    erros = 0
    for idx, res in resultados:
        if 'erro' in res:
            erros += 1
            logger.warning(f"Erro no registro {idx}: {res['erro']}")
            continue
        for key, value in res.items():
            if key != 'erro':
                erbs.loc[idx, key] = value

    logger.info(f"{erros} registros apresentaram erro ({erros/len(erbs)*100:.1f}%).")

    erbs['razao_raios'] = erbs['Raio_Cobertura_km'] / erbs['Raio_Cobertura_Original_km']
    stats = {
        'raio_original_medio': float(erbs['Raio_Cobertura_Original_km'].mean()),
        'raio_efetivo_medio': float(erbs['Raio_Cobertura_km'].mean()),
        'razao_media': float(erbs['razao_raios'].mean()),
        'erbs_com_reducao': int((erbs['razao_raios'] < 0.99).sum()),
        'erbs_sem_alteracao': int((erbs['razao_raios'] >= 0.99).sum()),
        'percentual_com_reducao': float((erbs['razao_raios'] < 0.99).mean() * 100)
    }

    logger.info("Estatísticas de processamento:")
    for key, value in stats.items():
        if isinstance(value, float):
            logger.info(f"  {key}: {value:.2f}")
        else:
            logger.info(f"  {key}: {value}")

    elapsed_time = time.time() - start_time
    logger.info(f"Processamento concluído em {elapsed_time:.2f} segundos")

    logger.info(f"Salvando resultado em {output_gpkg}")
    erbs.to_file(output_gpkg, layer="erbs_unificadas", driver="GPKG")
    logger.info(f"Camada 'erbs_unificadas' salva com sucesso")

    # Copiar as outras camadas sem modificações
    for layer in layers:
        if layer != "erbs_unificadas":
            logger.info(f"Copiando camada '{layer}' para o arquivo de saída...")
            try:
                gdf_aux = gpd.read_file(input_gpkg, layer=layer)
                gdf_aux.to_file(output_gpkg, layer=layer, driver="GPKG")
                logger.info(f"Camada '{layer}' copiada com sucesso")
            except Exception as e:
                logger.error(f"Erro ao copiar camada '{layer}': {str(e)}")

    logger.info(f"Todas as camadas foram processadas e salvas em {output_gpkg}")
    return erbs, stats

def visualizar_setores(input_gpkg, output_dir=None, layer="erbs_unificadas", max_erbs=50):
    """
    Cria visualizações dos setores de cobertura das ERBs com e sem correção topográfica.

    Args:
        input_gpkg: Caminho para o GeoPackage contendo os dados das ERBs
        output_dir: Diretório para salvar as visualizações
        layer: Nome da camada contendo os dados das ERBs
        max_erbs: Número máximo de ERBs a serem visualizadas
    """
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    erbs = gpd.read_file(input_gpkg, layer=layer)
    required_cols = ['Raio_Cobertura_km', 'Raio_Cobertura_Original_km', 'setor_geometria']
    if not all(col in erbs.columns for col in required_cols):
        logger.error(f"Dados não contêm todas as colunas necessárias: {required_cols}")
        return

    if 'razao_raios' not in erbs.columns:
        erbs['razao_raios'] = erbs['Raio_Cobertura_km'] / erbs['Raio_Cobertura_Original_km']

    if len(erbs) > max_erbs:
        logger.info(f"Selecionando {max_erbs} ERBs aleatoriamente para visualização")
        erbs_viz = erbs.sample(max_erbs, random_state=42)
    else:
        erbs_viz = erbs

    erbs_viz = erbs_viz.to_crs(epsg=3857)

    fig, ax = plt.subplots(figsize=(15, 15))
    # Converter e validar a coluna de setores
    if 'setor_geometria' in erbs_viz.columns:
        erbs_viz['setor_geometria'] = erbs_viz['setor_geometria'].apply(
            lambda x: wkt.loads(x) if isinstance(x, str) else x
        )
        erbs_viz['setor_geometria'] = erbs_viz['setor_geometria'].apply(
            lambda geom: geom if (geom is None or geom.is_valid) else geom.buffer(0)
        )
        setores = gpd.GeoDataFrame(
            {'razao_raios': erbs_viz['razao_raios']},
            geometry=erbs_viz['setor_geometria'],
            crs=erbs_viz.crs
        )
        setores = setores[~setores.geometry.isna()]
        cmap = plt.cm.RdYlGn
        norm = Normalize(vmin=setores['razao_raios'].min(), vmax=1.0)
        setores.plot(
            column='razao_raios',
            ax=ax,
            alpha=0.7,
            cmap=cmap,
            norm=norm,
            legend=True,
            legend_kwds={'label': 'Razão (Efetivo/Original)'}
        )

    erbs_viz.plot(
        ax=ax,
        color='black',
        markersize=20,
        alpha=0.8
    )

    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)
    ax.set_title('Setores de Cobertura Corrigidos para Topografia', fontsize=16)
    ax.set_axis_off()
    scale_bar = AnchoredSizeBar(
        ax.transData,
        5000,
        '5 km',
        'lower right',
        pad=0.5,
        color='black',
        frameon=False,
        size_vertical=100
    )
    ax.add_artist(scale_bar)
    legend_elements = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor='black',
               markersize=10, label='Estação Rádio Base')
    ]
    ax.legend(handles=legend_elements, loc='upper left')

    if output_dir:
        output_path = os.path.join(output_dir, 'setores_cobertura_corrigidos.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"Visualização dos setores salva em: {output_path}")
    else:
        plt.tight_layout()
        plt.show()

if __name__ == "__main__":
    # Definir caminhos de entrada e saída
    input_gpkg = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_camada_com_altimetria.gpkg"
    mde_path = "/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/mde_sorocaba.tif"
    output_gpkg = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_refinada.gpkg"

    considerar_difracao = True
    considerar_reflexao = True
    n_jobs = -1  # Usar todos os cores disponíveis

    # Processar as ERBs e salvar resultados refinados
    erbs_result, stats = main(
        input_gpkg=input_gpkg,
        mde_path=mde_path,
        output_gpkg=output_gpkg,
        considerar_difracao=considerar_difracao,
        considerar_reflexao=considerar_reflexao,
        n_jobs=n_jobs
    )

    # Criar visualizações dos setores
    viz_dir = os.path.join(os.path.dirname(output_gpkg), "visualizacoes")
    visualizar_setores(output_gpkg, output_dir=viz_dir)

    print("\nProcessamento concluído!")
    print(f"Raio médio original: {stats['raio_original_medio']:.2f} km")
    print(f"Raio médio efetivo: {stats['raio_efetivo_medio']:.2f} km")
    print(f"Razão média: {stats['razao_media']:.2f}")
    print(f"ERBs com redução de cobertura: {stats['erbs_com_reducao']} ({stats['percentual_com_reducao']:.1f}%)")
    print(f"Resultado salvo em: {output_gpkg}")
    print(f"Visualizações salvas em: {viz_dir}")

import geopandas as gpd
import pandas as pd
import json
import numpy as np
from shapely.geometry import Polygon, Point, mapping
import fiona
import os
from tqdm import tqdm

def serialize_record(record):
    """
    Converte os objetos de geometria para WKT, se existirem,
    e converte valores numéricos do tipo numpy para tipos nativos do Python.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def analisar_camadas(file_path):
    """
    Realiza uma análise detalhada das camadas do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG

    Returns:
        Dicionário com o relatório de análise para cada camada
    """
    print(f"Analisando camadas do arquivo: {file_path}")

    # Obter a lista de camadas do arquivo GPKG
    layers = fiona.listlayers(file_path)
    print(f"Camadas encontradas: {layers}")

    # Dicionário que armazenará o relatório de análise de cada camada
    analysis_report = {}

    # Iterar sobre cada camada encontrada
    for layer in layers:
        print(f"\nAnalisando a camada: {layer}")

        # Carrega a camada como um GeoDataFrame
        gdf = gpd.read_file(file_path, layer=layer)

        # Cria um dicionário com informações da camada
        report = {}
        report['layer_name'] = layer
        report['num_records'] = len(gdf)
        report['columns'] = list(gdf.columns)

        # Obtém os tipos de dados de cada coluna e converte para string
        report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

        # Seleciona uma amostra aleatória de até 30 linhas
        num_amostras = min(30, len(gdf))
        sample_df = gdf.sample(n=num_amostras, random_state=42) if len(gdf) > 0 else gdf

        # Converte a amostra para dicionário e serializa as geometrias
        sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
        report['sample_records'] = sample_records

        # Se houver colunas numéricas, calcula estatísticas descritivas
        numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
        if numeric_cols:
            report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
        else:
            report['numeric_summary'] = {}

        # Análise adicional: informações sobre a geometria
        if 'geometry' in gdf.columns:
            report['geometry_type'] = str(gdf.geometry.geom_type.value_counts().to_dict())
            report['geometry_is_valid'] = str(gdf.geometry.is_valid.value_counts().to_dict())
            report['geometry_bounds'] = str(gdf.total_bounds.tolist())

        # Análise adicional: verificação de valores nulos
        report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

        # Análise adicional: valores únicos para colunas categóricas
        categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
        if categorical_cols:
            unique_values = {}
            for col in categorical_cols:
                if col != 'geometry':
                    unique_counts = gdf[col].value_counts().head(10).to_dict()  # Top 10 valores
                    unique_values[col] = {
                        'count': len(gdf[col].unique()),
                        'top_values': unique_counts
                    }
            report['categorical_summary'] = unique_values

        analysis_report[layer] = report

    return analysis_report

def main():
    # Caminho para o arquivo .gpkg com altimetria
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_refinada.gpkg'

    # Caminho para salvar o relatório de análise
    report_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analysis_refinado.json'

    # Executa a análise das camadas
    analysis_report = analisar_camadas(input_file)

    # Salva o relatório de análise em formato JSON
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_report, f, ensure_ascii=False, indent=4)

    print(f"\nRelatório de análise salvo em: {report_file}")

if __name__ == "__main__":
    main()

import geopandas as gpd
import folium
from folium import Popup, GeoJson
import pandas as pd
import contextily as ctx
from shapely import wkt
import os

# Definir caminhos dos arquivos
INPUT_GPKG = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_refinada.gpkg"
OUTPUT_HTML = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/mapa_interativo_erb_refinada.html"

# Carregar a camada única do GeoPackage refinado
gdf = gpd.read_file(INPUT_GPKG, layer="erb_refinada")

# Criar GeoDataFrame para os pontos (usando a coluna "geometry")
gdf_pontos = gdf.copy()

# Se a coluna "setor_geometria" existir, converter de WKT para objeto geométrico
if "setor_geometria" in gdf.columns:
    def converter_wkt(geom):
        if isinstance(geom, str):
            try:
                return wkt.loads(geom)
            except Exception as e:
                print(f"Erro na conversão de WKT: {e}")
                return None
        return geom

    gdf_setores = gdf.copy()
    gdf_setores["geometry"] = gdf_setores["setor_geometria"].apply(converter_wkt)
else:
    gdf_setores = None

# Reprojetar para EPSG:4326 (WGS84) para compatibilidade com Folium
gdf_pontos = gdf_pontos.to_crs(epsg=4326)
if gdf_setores is not None:
    gdf_setores = gdf_setores.to_crs(epsg=4326)

# Calcular o centro do mapa a partir dos pontos
centro_lat = gdf_pontos.geometry.y.mean()
centro_lon = gdf_pontos.geometry.x.mean()

# Criar o mapa interativo com Folium
mapa = folium.Map(location=[centro_lat, centro_lon], zoom_start=12, tiles='CartoDB positron')

# Dicionário de cores por operadora
cores_operadoras = {
    'CLARO': '#E02020',  # vermelho
    'OI': '#FFD700',     # amarelo
    'VIVO': '#9932CC',   # roxo
    'TIM': '#0000CD'     # azul escuro
}

# 1. Adicionar a camada de setores de cobertura (caso disponível)
if gdf_setores is not None:
    setores_group = folium.FeatureGroup(name="Setores de Cobertura")
    for op in gdf_setores["Operadora"].unique():
        subset = gdf_setores[gdf_setores["Operadora"] == op]
        cor = cores_operadoras.get(op, "#888888")
        grupo_op = folium.FeatureGroup(name=f"Setores {op}")
        for _, row in subset.iterrows():
            try:
                eirp = float(row.get("EIRP_dBm", 0))
                raio = float(row.get("Raio_Cobertura_km", 0))
                popup_html = f"""
                <b>Operadora:</b> {op}<br>
                <b>EIRP:</b> {eirp:.1f} dBm<br>
                <b>Raio:</b> {raio:.2f} km
                """
            except Exception:
                popup_html = f"<b>Operadora:</b> {op}"
            popup = Popup(popup_html, max_width=250)
            geom = row.geometry
            if geom is None:
                continue
            geo = GeoJson(geom.__geo_interface__,
                          style_function=lambda feature, cor=cor: {
                              "fillColor": cor,
                              "color": cor,
                              "weight": 1,
                              "fillOpacity": 0.3
                          })
            geo.add_child(popup)
            geo.add_to(grupo_op)
        grupo_op.add_to(setores_group)
    setores_group.add_to(mapa)

# 2. Adicionar os pontos individuais das ERBs
pontos_group = folium.FeatureGroup(name="ERBs Individuais")
for _, row in gdf_pontos.iterrows():
    try:
        cor = cores_operadoras.get(row["Operadora"], "#888888")
        freq = float(row.get("FreqTxMHz", 0))
        ganho = float(row.get("GanhoAntena", 0))
        pot = float(row.get("PotenciaTransmissorWatts", 0))
        eirp = None
        try:
            eirp = float(row.get("EIRP_dBm", 0))
        except Exception:
            pass
        info_html = f"""
        <b>Operadora:</b> {row['Operadora']}<br>
        <b>Frequência:</b> {freq:.1f} MHz<br>
        <b>Ganho:</b> {ganho:.1f} dBi<br>
        <b>Potência:</b> {pot:.1f} W
        """
        if eirp:
            info_html += f"<br><b>EIRP:</b> {eirp:.1f} dBm"
        popup = Popup(info_html, max_width=300)
        folium.CircleMarker(
            location=[row.geometry.y, row.geometry.x],
            radius=7,
            color=cor,
            fill=True,
            fill_color=cor,
            fill_opacity=1.0,
            weight=2,
            popup=popup
        ).add_to(pontos_group)
    except Exception as e:
        print(f"Erro ao adicionar ponto: {e}")
pontos_group.add_to(mapa)

# Se não houver camadas de hexágonos ou grade de cobertura no arquivo refinado, omitimos estas etapas.

# Adicionar controle de camadas
folium.LayerControl(collapsed=False).add_to(mapa)

# Salvar o mapa interativo em HTML
mapa.save(OUTPUT_HTML)
print(f"Mapa interativo salvo em: {OUTPUT_HTML}")

import geopandas as gpd
import fiona

# Caminho do arquivo de entrada
input_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_refinada.gpkg'

# Listar todas as camadas do arquivo de entrada
layers = fiona.listlayers(input_gpkg)
print("Camadas presentes no arquivo original:", layers)

# Definir os nomes das duas camadas que serão unidas
layer1 = "erb_refinada"
layer2 = "erbs_unificadas"

if layer1 not in layers or layer2 not in layers:
    print("Uma ou ambas as camadas para unificação não foram encontradas no arquivo.")
else:
    # Ler as duas camadas
    gdf1 = gpd.read_file(input_gpkg, layer=layer1).reset_index(drop=True)
    gdf2 = gpd.read_file(input_gpkg, layer=layer2).reset_index(drop=True)

    # Criar um GeoDataFrame para a camada unificada
    # Se houver correspondência de registros (mesma ordem), podemos combinar linha a linha
    gdf_merged = gdf1.copy()
    for col in gdf2.columns:
        if col in gdf_merged.columns:
            gdf_merged[col] = gdf_merged[col].fillna(gdf2[col])
        else:
            gdf_merged[col] = gdf2[col]

    # Definir o caminho do novo arquivo de saída
    output_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_refinada_merged.gpkg'

    # Escrever a camada unificada no novo GeoPackage (modo 'w' para criar)
    gdf_merged.to_file(output_gpkg, driver='GPKG', layer="merged_layer", mode='w')

    # Para cada camada do arquivo original que NÃO seja uma das duas que foram unidas, copie-as para o novo arquivo
    for lyr in layers:
        if lyr not in [layer1, layer2]:
            gdf_temp = gpd.read_file(input_gpkg, layer=lyr)
            # Adiciona a camada usando modo 'a' (append) para não sobrescrever o arquivo
            gdf_temp.to_file(output_gpkg, driver='GPKG', layer=lyr, mode='a')

    print(f"Novo arquivo criado com as camadas unificadas: {output_gpkg}")

import geopandas as gpd
import fiona
import os

# Caminhos dos arquivos
file_merged = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/erb_refinada_merged.gpkg'
file_base = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_integrados_alt.gpkg'
output_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Se o arquivo de saída já existir, remova-o para criar um novo
if os.path.exists(output_file):
    os.remove(output_file)

# Listar as camadas de cada arquivo
layers_merged = fiona.listlayers(file_merged)
layers_base = fiona.listlayers(file_base)

print("Camadas do arquivo unificado (erb_refinada_merged.gpkg):", layers_merged)
print("Camadas do arquivo base (sorocaba_dados_integrados_alt.gpkg):", layers_base)

# Variável para controlar a criação (modo 'w') ou a adição (modo 'a')
first_layer = True

# Adicionar as camadas do arquivo merged
for layer in layers_merged:
    gdf = gpd.read_file(file_merged, layer=layer)
    if first_layer:
        # Cria o arquivo novo com a primeira camada
        gdf.to_file(output_file, driver='GPKG', layer=layer, mode='w')
        first_layer = False
    else:
        # Adiciona as demais camadas no modo 'append'
        gdf.to_file(output_file, driver='GPKG', layer=layer, mode='a')

# Adicionar as camadas do arquivo base
for layer in layers_base:
    gdf = gpd.read_file(file_base, layer=layer)
    # Verifica se o nome da camada já existe no arquivo de saída
    existing_layers = fiona.listlayers(output_file)
    new_layer_name = layer
    if layer in existing_layers:
        new_layer_name = f"base_{layer}"
    gdf.to_file(output_file, driver='GPKG', layer=new_layer_name, mode='a')

print("Arquivo consolidado salvo em:", output_file)

import os
import fiona
import geopandas as gpd

file_path = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg"

# Ajuste aqui exatamente para os nomes que você quer manter:
keep_layers = [
    "merged_layer",
    "analise_consolidada",
    "voronoi",
    "setores_com_edificacoes",
    "edificacoes_prioritarias",
    "hidrografia",
    "rodovias",
    "ferrovias",
    "curvas_nivel_com_elevacao",
    "clima_pontos_grade",
    "clima_arestas_grade",
    "uso_terra_ocupacao",
    "setores_censitarios",
    "edificacoes"
]

all_layers = fiona.listlayers(file_path)
print("Camadas encontradas no arquivo original:")
for lyr in all_layers:
    print("  -", lyr)

temp_file = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_temp.gpkg"

# Se existir, remove o temporário anterior
if os.path.exists(temp_file):
    os.remove(temp_file)

first_layer = True
for layer in keep_layers:
    if layer not in all_layers:
        print(f"Aviso: a camada '{layer}' não foi encontrada no arquivo. Pulando...")
        continue

    # Ler a camada
    gdf = gpd.read_file(file_path, layer=layer)

    # Escrever no arquivo temporário
    if first_layer:
        gdf.to_file(temp_file, layer=layer, driver="GPKG", mode="w")
        first_layer = False
    else:
        gdf.to_file(temp_file, layer=layer, driver="GPKG", mode="a")

# Se o arquivo temporário não foi criado (nenhuma camada válida copiada), não apague o original
if not os.path.exists(temp_file):
    print("\nNenhuma camada foi copiada. Verifique os nomes. O arquivo original permanece inalterado.")
else:
    # Apagar o arquivo original
    os.remove(file_path)
    # Renomear o temporário para o nome original
    os.rename(temp_file, file_path)
    print("\nArquivo final salvo no mesmo caminho, contendo apenas as camadas selecionadas.")

import geopandas as gpd
import json
import numpy as np
import os
import fiona

def serialize_record(record):
    """
    Converte objetos de geometria para WKT, se existirem,
    e converte valores numéricos do tipo numpy para tipos nativos do Python.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def analyze_layer(file_path, layer):
    """
    Realiza a análise detalhada de uma camada específica do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG.
        layer: Nome da camada a ser analisada.

    Returns:
        Um dicionário contendo o relatório de análise.
    """
    print(f"Analisando a camada: {layer}")
    gdf = gpd.read_file(file_path, layer=layer)

    report = {}
    report['layer_name'] = layer
    report['num_records'] = len(gdf)
    report['columns'] = list(gdf.columns)
    report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

    # Seleciona uma amostra aleatória de até 30 registros
    num_samples = min(30, len(gdf))
    sample_df = gdf.sample(n=num_samples, random_state=42) if len(gdf) > 0 else gdf
    sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
    report['sample_records'] = sample_records

    # Estatísticas para colunas numéricas
    numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
    if numeric_cols:
        report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
    else:
        report['numeric_summary'] = {}

    # Informações sobre a geometria
    if 'geometry' in gdf.columns:
        report['geometry_type'] = str(gdf.geometry.geom_type.value_counts().to_dict())
        report['geometry_is_valid'] = str(gdf.geometry.is_valid.value_counts().to_dict())
        report['geometry_bounds'] = str(gdf.total_bounds.tolist())

    # Contagem de valores nulos por coluna
    report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

    # Resumo das colunas categóricas (top 10 valores)
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
    if categorical_cols:
        unique_values = {}
        for col in categorical_cols:
            if col != 'geometry':
                unique_counts = gdf[col].value_counts().head(10).to_dict()
                unique_values[col] = {
                    'count': len(gdf[col].unique()),
                    'top_values': unique_counts
                }
        report['categorical_summary'] = unique_values

    return report

def main():
    # Caminho do arquivo que contém as camadas consolidadas
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

    # Lista de camadas a serem analisadas
    layers_to_keep = [
        "merged_layer",
        "analise_consolidada",
        "voronoi",
        "setores_com_edificacoes",
        "edificacoes_prioritarias",
        "hidrografia",
        "rodovias",
        "ferrovias",
        "curvas_nivel_com_elevacao",
        "clima_pontos_grade",
        "clima_arestas_grade",
        "uso_terra_ocupacao",
        "setores_censitarios",
        "edificacoes"
    ]

    # Pasta para salvar os relatórios (cria se não existir)
    output_dir = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analise_json'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Lista as camadas disponíveis no arquivo
    all_layers = fiona.listlayers(input_file)
    print("Camadas encontradas no arquivo original:")
    for lyr in all_layers:
        print("  -", lyr)

    # Itera sobre cada camada desejada e gera um relatório individual
    for layer in layers_to_keep:
        if layer not in all_layers:
            print(f"Aviso: a camada '{layer}' não foi encontrada. Pulando...")
            continue

        report = analyze_layer(input_file, layer)
        report_file = os.path.join(output_dir, f'analysis_report_{layer}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=4)
        print(f"Relatório para a camada '{layer}' salvo em: {report_file}")

if __name__ == "__main__":
    main()

"""1. Análise da camada "analise_consolidada"
Esta camada contém informações sobre a análise consolidada de cobertura de sinal, com 2.360 registros.
Colunas presentes:

num_operadoras: número de operadoras (int64)
vulnerabilidade: classificação de vulnerabilidade (object)
densidade_media: densidade média (float64)
potencia_media: potência média (float64)
classificacao_cobertura: classificação de cobertura (object)
geometry: geometria (geometry)

Valores nulos:

potencia_media: 725 valores nulos (aproximadamente 30,7% dos dados)
As demais colunas não têm valores nulos

Recomendações:

Todas as colunas parecem relevantes para análise de cobertura de sinal
Para os valores nulos em potencia_media:

Podemos inferir que estes correspondem aos registros com "Sem cobertura" (829 registros), o que faz sentido
É importante manter esses registros, pois a ausência de sinal também é uma informação valiosa
Podemos preencher com 0 ou manter como NaN, dependendo de como o modelo GNN lidará com valores nulos



2. Análise da camada "clima_arestas_grade"
Esta camada contém dados climáticos associados a arestas de uma grade, com 314.419 registros.
Colunas principais:

tipo: tipo de aresta (horizontal/vertical)
nivel_vertical: nível vertical (valores 0-10)
z_medio: altura média
Várias colunas climáticas (precipitação, radiação, temperatura, vento)
geometry: geometria da aresta

Valores nulos:

nivel_vertical: 98.940 valores nulos (31,5%)
nivel_inferior e nivel_superior: 215.479 valores nulos (68,5%)

Recomendações:

Colunas que parecem redundantes:

"PRECIPITAÇÃO TOTAL, HORÁRIO (mm)_media" e "precipitacao_media" têm valores idênticos
"Unnamed: 19_media" contém apenas zeros, pode ser removida


Para os valores nulos:

Os nulos em nivel_vertical, nivel_inferior e nivel_superior seguem um padrão: quando "tipo" é "horizontal", nivel_vertical tem valor, mas nivel_inferior/superior são nulos; quando "tipo" é "vertical", nivel_vertical é nulo, mas nivel_inferior/superior têm valores
Podemos manter essa estrutura já que representa a natureza dos dados



3. Análise da camada "clima_pontos_grade"
Esta camada contém dados climáticos associados a pontos de uma grade, com 108.834 registros.
Colunas principais:

x, y, z: coordenadas e elevação
nivel_vertical, altura_normalizada: nível vertical (0-10) e altura normalizada (0-1)
Várias colunas climáticas (precipitação, radiação, temperatura, vento)
geometry: geometria do ponto

Valores nulos:

Não há valores nulos nesta camada

Recomendações:

Colunas redundantes:

"PRECIPITAÇÃO TOTAL, HORÁRIO (mm)" e "precipitacao" têm valores idênticos
"Unnamed: 19" contém apenas zeros, pode ser removida


Todas as demais colunas parecem relevantes para análise espacial e climática

4. Análise da camada "curvas_nivel_com_elevacao"
Esta camada contém curvas de nível com informações de elevação, com 7.953 registros.
Colunas presentes:

elevation: elevação (float64)
geometry: geometria (LineString)

Valores nulos:

Não há valores nulos nesta camada

Recomendações:

Camada muito simples e completa, sem necessidade de tratamento adicional
As curvas de nível são importantes para o modelo de terreno e análise de propagação de sinal

5. Análise da camada "edificacoes"
Esta camada contém informações sobre edificações, com 35.812 registros.
Colunas principais:

source, layer_type, building, building_type: informações da fonte e tipo de edificação
priority: prioridade (1-12)
name, osm_id, osm_way_id: identificadores
amenity: tipo de serviço
elevation: elevação
Colunas de alunos (est_alunos, alunos_atual_*)
geometry: geometria da edificação

Valores nulos:

name: 32.109 valores nulos (89,7%)
osm_id: 35.684 valores nulos (99,6%)
osm_way_id: 128 valores nulos (0,4%)
amenity: 34.887 valores nulos (97,4%)

Recomendações:

Campos com alta taxa de valores nulos:

name, osm_id, amenity: alta presença de nulos, mas são informações complementares que podem ser mantidas nos registros onde existem


Colunas potencialmente redundantes:

alunos_atual_0800, alunos_atual_1200, alunos_atual_1500 têm valores idênticos
alunos_atual_1900, alunos_atual_2300, est_alunos têm valores idênticos em outro conjunto
Podemos consolidar essas informações em menos colunas



6. Análise da camada "edificacoes_prioritarias"
Esta camada contém informações sobre edificações prioritárias, com 154 registros.
Colunas principais:

Similares à camada "edificacoes", mas sem as colunas de alunos
priority: valores entre 1-3 (mais prioritárias que na camada anterior)

Valores nulos:

name: 55 valores nulos (35,7%)
osm_id: 150 valores nulos (97,4%)
osm_way_id: 4 valores nulos (2,6%)
amenity: 136 valores nulos (88,3%)

Recomendações:

Esta camada parece ser um subconjunto da camada "edificacoes", focada em edificações mais prioritárias
Podemos manter ambas as camadas separadas ou integrá-las, marcando claramente as prioritárias

7. Análise da camada "ferrovias"
Esta camada contém informações sobre ferrovias, com 84 registros.
Colunas principais:

source, layer_type, railway, rail_class: informações da fonte e tipo de ferrovia
name, osm_id: identificadores
elevation: elevação
geometry: geometria da ferrovia

Valores nulos:

name: 30 valores nulos (35,7%)

Recomendações:

Camada completa com poucos valores nulos
O campo "name" pode ser mantido mesmo com nulos, já que é informação complementar

8. Análise da camada "hidrografia"
Esta camada contém informações sobre hidrografia, com 1.297 registros.
Colunas principais:

source: origem dos dados (OSM ou BHO)
Colunas específicas da BHO (cotrecho, cocursodag, etc.)
Colunas do OSM (osm_id, name, waterway)
elevation: elevação
geometry: geometria do curso d'água

Valores nulos:

Muitos valores nulos nas colunas específicas da BHO (cerca de 95%)
name: 1.110 valores nulos (85,6%)

Recomendações:

A presença de valores nulos é consistente com a fonte dos dados:

Registros da OSM (1.233) não têm informações da BHO
Registros da BHO (64) não têm informações do OSM


Podemos manter as duas fontes de dados ou separá-las em camadas distintas

Conclusões e Recomendações Gerais

Estrutura de Dados para o Grafo:

Nós do Grafo: podem ser formados pelos pontos da grade climática e edificações
Arestas do Grafo: podem ser formadas pelas arestas da grade climática, ferrovias e hidrografia


Tratamento de Valores Nulos:

potencia_media: podemos preencher com 0 onde classificacao_cobertura = "Sem cobertura"
Colunas estruturais: manter os nulos onde fazem sentido (como nos níveis verticais)
Colunas de identificação: manter os nulos onde não há informação (name, osm_id)


Redução de Dimensionalidade:

Remover colunas redundantes, como "Unnamed: 19_media" (todos zeros)
Consolidar colunas de alunos em menos variáveis


Normalização de Dados:

Garantir consistência em elevation entre todas as camadas
Padronizar os tipos de geometria para processamento uniforme



Essas análises e recomendações devem ajudar a preparar os dados para alimentar o modelo de GNN, garantindo que tenhamos informações de qualidade sobre a topografia, edificações, clima e outros elementos que afetam a propagação do sinal, além de uma estrutura de grafo que capture adequadamente as relações espaciais entre esses elementos.
Gostaria de aprofundar algum aspecto específico desta análise, ou podemos prosseguir para a próxima etapa do tratamento dos dados?
"""

import geopandas as gpd
import pandas as pd
import numpy as np

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Carregar a camada 'analise_consolidada'
analise_consolidada = gpd.read_file(filepath, layer='analise_consolidada')

# Visualizar informações sobre a camada
print("Número de registros:", len(analise_consolidada))
print("Número de valores nulos em potencia_media:", analise_consolidada['potencia_media'].isna().sum())
print("\nDistribuição de classificacao_cobertura:")
print(analise_consolidada['classificacao_cobertura'].value_counts())

# Verificar se os valores nulos coincidem com "Sem cobertura"
print("\nValores nulos por classificação de cobertura:")
print(analise_consolidada[analise_consolidada['potencia_media'].isna()]['classificacao_cobertura'].value_counts())

# Preencher valores nulos com 0 apenas onde classificacao_cobertura é "Sem cobertura"
analise_consolidada.loc[analise_consolidada['classificacao_cobertura'] == 'Sem cobertura', 'potencia_media'] = 0

# Verificar se ainda há valores nulos após o preenchimento
print("\nNúmero de valores nulos restantes em potencia_media:", analise_consolidada['potencia_media'].isna().sum())

# Se ainda houver valores nulos, vamos identificar em quais classificações eles ocorrem
if analise_consolidada['potencia_media'].isna().sum() > 0:
    print("Classificações com valores nulos restantes:")
    print(analise_consolidada[analise_consolidada['potencia_media'].isna()]['classificacao_cobertura'].value_counts())

# Salvar as alterações de volta no arquivo
analise_consolidada.to_file(filepath, layer='analise_consolidada', driver='GPKG')

print("\nAlterações salvas com sucesso!")

import geopandas as gpd
import pandas as pd
import numpy as np

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Função para processar ambas as camadas
def processar_camada_clima(nome_camada):
    print(f"\nProcessando camada: {nome_camada}")

    # Carregar a camada
    gdf = gpd.read_file(filepath, layer=nome_camada)
    print(f"Número de registros originais: {len(gdf)}")
    print(f"Colunas originais: {gdf.columns.tolist()}")

    # Identificar e remover colunas redundantes
    colunas_para_remover = []

    # Verificar se "Unnamed: 19_media" ou "Unnamed: 19" existe e contém apenas zeros
    if "Unnamed: 19_media" in gdf.columns and gdf["Unnamed: 19_media"].sum() == 0:
        colunas_para_remover.append("Unnamed: 19_media")
    if "Unnamed: 19" in gdf.columns and gdf["Unnamed: 19"].sum() == 0:
        colunas_para_remover.append("Unnamed: 19")

    # Verificar duplicação nas colunas de precipitação
    if "PRECIPITAÇÃO TOTAL, HORÁRIO (mm)_media" in gdf.columns and "precipitacao_media" in gdf.columns:
        # Verificar se são idênticas
        if gdf["PRECIPITAÇÃO TOTAL, HORÁRIO (mm)_media"].equals(gdf["precipitacao_media"]):
            colunas_para_remover.append("PRECIPITAÇÃO TOTAL, HORÁRIO (mm)_media")

    if "PRECIPITAÇÃO TOTAL, HORÁRIO (mm)" in gdf.columns and "precipitacao" in gdf.columns:
        # Verificar se são idênticas
        if gdf["PRECIPITAÇÃO TOTAL, HORÁRIO (mm)"].equals(gdf["precipitacao"]):
            colunas_para_remover.append("PRECIPITAÇÃO TOTAL, HORÁRIO (mm)")

    # Remover as colunas identificadas
    if colunas_para_remover:
        print(f"Removendo colunas redundantes: {colunas_para_remover}")
        gdf = gdf.drop(columns=colunas_para_remover)
    else:
        print("Nenhuma coluna redundante encontrada")

    # Para clima_arestas_grade: não alteramos os valores nulos pois eles seguem um padrão estrutural
    if nome_camada == "clima_arestas_grade":
        # Verificar a relação entre tipos de aresta e valores nulos
        tipo_counts = gdf["tipo"].value_counts()
        print(f"\nDistribuição por tipo de aresta:\n{tipo_counts}")

        nulos_por_tipo = gdf.groupby("tipo")[["nivel_vertical", "nivel_inferior", "nivel_superior"]].apply(
            lambda x: x.isna().sum()
        )
        print(f"\nValores nulos por tipo de aresta:\n{nulos_por_tipo}")

        print("\nManteremos os valores nulos em nivel_vertical, nivel_inferior e nivel_superior, "
              "pois eles representam a estrutura natural dos dados.")

    # Salvar as alterações de volta no arquivo
    gdf.to_file(filepath, layer=nome_camada, driver='GPKG')
    print(f"Camada {nome_camada} processada e salva com {len(gdf)} registros e {len(gdf.columns)} colunas")
    print(f"Colunas finais: {gdf.columns.tolist()}")

# Processar ambas as camadas
processar_camada_clima("clima_arestas_grade")
processar_camada_clima("clima_pontos_grade")

print("\nProcessamento concluído com sucesso!")

import geopandas as gpd
import pandas as pd
import numpy as np

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Carregar as duas camadas
edificacoes = gpd.read_file(filepath, layer='edificacoes')
edificacoes_prioritarias = gpd.read_file(filepath, layer='edificacoes_prioritarias')

print(f"Registros em edificacoes: {len(edificacoes)}")
print(f"Registros em edificacoes_prioritarias: {len(edificacoes_prioritarias)}")

# 1. Primeiro, vamos consolidar as colunas de alunos em edificacoes
# Verificar se as colunas são realmente idênticas
grupo1_iguais = edificacoes['alunos_atual_0800'].equals(edificacoes['alunos_atual_1200']) and edificacoes['alunos_atual_1200'].equals(edificacoes['alunos_atual_1500'])
grupo2_iguais = edificacoes['alunos_atual_1900'].equals(edificacoes['alunos_atual_2300']) and edificacoes['alunos_atual_2300'].equals(edificacoes['est_alunos'])

print(f"Colunas do grupo 1 são idênticas: {grupo1_iguais}")
print(f"Colunas do grupo 2 são idênticas: {grupo2_iguais}")

# Criar colunas consolidadas
if grupo1_iguais:
    edificacoes['alunos_manha'] = edificacoes['alunos_atual_0800']
    # Remover colunas redundantes
    edificacoes = edificacoes.drop(columns=['alunos_atual_0800', 'alunos_atual_1200', 'alunos_atual_1500'])
    print("Consolidadas colunas do período da manhã")

if grupo2_iguais:
    edificacoes['alunos_tarde_noite'] = edificacoes['alunos_atual_1900']
    # Remover colunas redundantes
    edificacoes = edificacoes.drop(columns=['alunos_atual_1900', 'alunos_atual_2300', 'est_alunos'])
    print("Consolidadas colunas do período da tarde/noite")

# 2. Adicionar uma nova coluna em edificacoes para indicar se é prioritária
edificacoes['edificacao_prioritaria'] = False

# 3. Identificar edificações prioritárias por correspondência de osm_way_id
# Primeiro, criar um dicionário de edificações prioritárias para busca rápida
prioritarias_dict = {}
for idx, row in edificacoes_prioritarias.iterrows():
    if pd.notna(row['osm_way_id']):
        prioritarias_dict[row['osm_way_id']] = True

# Marcar edificações prioritárias na camada principal
contador_prioritarias = 0
for idx, row in edificacoes.iterrows():
    if pd.notna(row['osm_way_id']) and row['osm_way_id'] in prioritarias_dict:
        edificacoes.at[idx, 'edificacao_prioritaria'] = True
        contador_prioritarias += 1

print(f"Edificações marcadas como prioritárias: {contador_prioritarias}")

# 4. Para edificações prioritárias que não foram encontradas, vamos adicionar à camada principal
# Identificar edificações prioritárias que não foram correspondidas
osm_way_ids_edificacoes = set(edificacoes['osm_way_id'].dropna())
edificacoes_para_adicionar = edificacoes_prioritarias[~edificacoes_prioritarias['osm_way_id'].isin(osm_way_ids_edificacoes)]

# Adicionar colunas de alunos que existem em edificacoes mas não em edificacoes_prioritarias
for col in ['alunos_manha', 'alunos_tarde_noite']:
    if col not in edificacoes_para_adicionar.columns:
        edificacoes_para_adicionar[col] = 0

# Marcar como prioritárias
edificacoes_para_adicionar['edificacao_prioritaria'] = True

print(f"Edificações prioritárias a serem adicionadas: {len(edificacoes_para_adicionar)}")

# Unir as duas camadas
colunas_edificacoes = edificacoes.columns
edificacoes_para_adicionar = edificacoes_para_adicionar.reindex(columns=colunas_edificacoes)
edificacoes_consolidado = pd.concat([edificacoes, edificacoes_para_adicionar], ignore_index=True)

print(f"Total de registros na camada consolidada: {len(edificacoes_consolidado)}")

# 5. Salvar a camada consolidada no arquivo, substituindo a camada edificacoes
edificacoes_consolidado.to_file(filepath, layer='edificacoes', driver='GPKG')

# 6. Opcionalmente, podemos manter a camada edificacoes_prioritarias para referência
# ou removê-la para ter apenas a versão consolidada
# Para este exemplo, vamos manter a camada original

print("\nProcessamento concluído com sucesso!")
print("A camada 'edificacoes' foi atualizada com as seguintes mudanças:")
print("1. Colunas de alunos consolidadas em 'alunos_manha' e 'alunos_tarde_noite'")
print("2. Nova coluna 'edificacao_prioritaria' indicando as edificações prioritárias")
print("3. Edificações prioritárias que não estavam na camada original foram adicionadas")
print("A camada 'edificacoes_prioritarias' foi mantida no arquivo original para referência")

import geopandas as gpd
import fiona

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Listar as camadas antes da remoção
print("Camadas antes da remoção:")
camadas_originais = fiona.listlayers(filepath)
print(camadas_originais)

# Criar um novo GeoPackage sem a camada indesejada
camadas_para_manter = [layer for layer in camadas_originais if layer != 'edificacoes_prioritarias']

# Verificar se a camada que queremos remover existe
if 'edificacoes_prioritarias' in camadas_originais:
    # Criar um dicionário para armazenar os GeoDataFrames de todas as camadas que queremos manter
    gdfs = {}

    # Ler todas as camadas que queremos manter
    for layer in camadas_para_manter:
        gdfs[layer] = gpd.read_file(filepath, layer=layer)

    # Agora vamos reescrever todas as camadas de volta para o arquivo, exceto a indesejada
    # Primeiro, salvar a primeira camada para criar o arquivo
    first_layer = camadas_para_manter[0]
    gdfs[first_layer].to_file(filepath, driver='GPKG', layer=first_layer)

    # Em seguida, adicionar todas as outras camadas ao arquivo existente
    for layer in camadas_para_manter[1:]:
        gdfs[layer].to_file(filepath, driver='GPKG', layer=layer, mode='a')

    print("\nCamadas após a remoção:")
    print(fiona.listlayers(filepath))
    print("\nA camada 'edificacoes_prioritarias' foi removida com sucesso!")
else:
    print("\nA camada 'edificacoes_prioritarias' não foi encontrada no arquivo.")

import geopandas as gpd
import pandas as pd
import numpy as np

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Carregar a camada hidrografia
hidrografia = gpd.read_file(filepath, layer='hidrografia')

# Exibir informações iniciais
print(f"Número total de registros: {len(hidrografia)}")
print(f"Registros por fonte: {hidrografia['source'].value_counts().to_dict()}")

# Verificar valores nulos por coluna
nulos_por_coluna = hidrografia.isna().sum().sort_values(ascending=False)
print("\nValores nulos por coluna:")
for coluna, nulos in nulos_por_coluna.items():
    if nulos > 0:
        percentual = (nulos / len(hidrografia)) * 100
        print(f"{coluna}: {nulos} ({percentual:.1f}%)")

# Verificar valores nulos nas colunas da BHO, separados por fonte
print("\nValores nulos nas colunas da BHO por fonte:")
colunas_bho = ['cotrecho', 'cocursodag', 'nogenerico', 'nuordemcda', 'nunivotto', 'nustrahler']
for fonte in hidrografia['source'].unique():
    subset = hidrografia[hidrografia['source'] == fonte]
    nulos = subset[colunas_bho].isna().sum().mean()
    percentual = (nulos / len(subset)) * 100
    print(f"{fonte}: {percentual:.1f}% de valores nulos nas colunas BHO")

# Verificar valores nulos nas colunas do OSM, separados por fonte
print("\nValores nulos nas colunas do OSM por fonte:")
colunas_osm = ['osm_id', 'name', 'waterway']
for fonte in hidrografia['source'].unique():
    subset = hidrografia[hidrografia['source'] == fonte]
    nulos = subset[colunas_osm].isna().sum().mean()
    percentual = (nulos / len(subset)) * 100
    print(f"{fonte}: {percentual:.1f}% de valores nulos nas colunas OSM")

# Criar coluna indicadora de fonte, facilitando a identificação da origem
hidrografia['fonte_dados'] = hidrografia['source'].map({'OSM': 'OpenStreetMap', 'BHO': 'Base Hidrográfica Ottocodificada'})

# Verificar se existem duplicações nas geometrias entre as fontes
# Isso pode indicar que o mesmo curso d'água está representado nas duas fontes
print("\nVerificando possíveis sobreposições entre fontes...")

# Para simplificar a análise, vamos criar uma versão simplificada das geometrias
hidrografia['geom_simplificada'] = hidrografia.geometry.simplify(1.0)

# Contar quantas geometrias de uma fonte se sobrepõem às de outra
osm_geoms = hidrografia[hidrografia['source'] == 'OSM']['geom_simplificada'].to_list()
bho_geoms = hidrografia[hidrografia['source'] == 'BHO']['geom_simplificada'].to_list()

# Esta checagem pode ser computacionalmente intensiva para muitas geometrias
# Vamos amostrar para demonstração
max_check = min(100, len(bho_geoms))
sobreposicoes = 0

for i in range(max_check):
    for osm_geom in osm_geoms[:100]:  # Limitar para 100 geometrias OSM para eficiência
        if bho_geoms[i].intersects(osm_geom):
            sobreposicoes += 1
            break

print(f"Entre as primeiras {max_check} geometrias BHO, {sobreposicoes} têm sobreposição com geometrias OSM")
print(f"Isso representa {(sobreposicoes/max_check)*100:.1f}% de sobreposição na amostra")

# Remover a coluna temporária
hidrografia = hidrografia.drop(columns=['geom_simplificada'])

# Com base na análise, vamos manter as duas fontes na mesma camada, mas:
# 1. Melhorar a documentação, mantendo os valores nulos consistentes
# 2. Padronizar os identificadores para facilitar o uso do GNN

# Não vamos remover nenhuma coluna, pois cada conjunto de dados tem seu valor próprio
# Vamos apenas confirmar que os valores nulos representam a natureza dos dados

# Salvar a camada atualizada
hidrografia.to_file(filepath, layer='hidrografia', driver='GPKG')

print("\nProcessamento concluído com sucesso!")
print("A camada 'hidrografia' foi atualizada com as seguintes características:")
print("1. Adicionada coluna 'fonte_dados' com descrição mais completa da origem")
print("2. Mantida estrutura original das colunas específicas de cada fonte")
print("3. Valores nulos mantidos, pois representam a natureza dos dados")
print("4. A análise de sobreposição indica em que grau os dados das fontes se complementam")

import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from shapely.geometry import Point
import math

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Carregar a camada merged_layer com os dados das ERBs
print("Carregando dados das ERBs...")
erbs = gpd.read_file(filepath, layer='merged_layer')

# Análise exploratória inicial
print(f"\nDimensões do dataset: {erbs.shape}")
print(f"Tipos de dados:\n{erbs.dtypes.sort_index()}")

# Verificação de valores nulos
nulos_por_coluna = erbs.isna().sum().sort_values(ascending=False)
print("\nValores nulos por coluna:")
for coluna, nulos in nulos_por_coluna.items():
    if nulos > 0:
        percentual = (nulos / len(erbs)) * 100
        print(f"{coluna}: {nulos} ({percentual:.1f}%)")

# Verificação de distribuição das tecnologias
if 'Tecnologia' in erbs.columns:
    print("\nDistribuição das tecnologias:")
    print(erbs['Tecnologia'].value_counts())

# Verificação de operadoras
if 'Operadora' in erbs.columns:
    print("\nDistribuição das operadoras:")
    print(erbs['Operadora'].value_counts())

# Tratamento específico para tipoTecnologia - verificar se há padrão entre Tecnologia e tipoTecnologia
# para possível imputação de valores nulos
if 'tipoTecnologia' in erbs.columns and 'Tecnologia' in erbs.columns:
    # Verificar correspondência entre Tecnologia e tipoTecnologia
    print("\nRelação entre Tecnologia e tipoTecnologia:")
    relacao = erbs[['Tecnologia', 'tipoTecnologia']].dropna().groupby(['Tecnologia', 'tipoTecnologia']).size().reset_index(name='count')
    print(relacao)

    # Se há um mapeamento claro, podemos usar para preencher valores nulos
    if erbs['tipoTecnologia'].isna().sum() > 0:
        # Criar um mapeamento das tecnologias mais comuns
        mapeamento = {}
        for tec in erbs['Tecnologia'].unique():
            subset = erbs[erbs['Tecnologia'] == tec]['tipoTecnologia'].dropna()
            if len(subset) > 0:
                # Usar o valor mais comum para essa tecnologia
                mapeamento[tec] = subset.mode()[0]

        # Preencher valores nulos com base no mapeamento
        for idx, row in erbs[erbs['tipoTecnologia'].isna()].iterrows():
            if row['Tecnologia'] in mapeamento:
                erbs.at[idx, 'tipoTecnologia'] = mapeamento[row['Tecnologia']]

        print(f"\nValores nulos em tipoTecnologia após imputação: {erbs['tipoTecnologia'].isna().sum()}")

# Verificação de consistência entre raio de cobertura e área de cobertura
if 'Raio_Cobertura_km' in erbs.columns and 'Area_Cobertura_km2' in erbs.columns:
    # A área teórica para um círculo é π*r²
    erbs['Area_Calculada_km2'] = np.pi * erbs['Raio_Cobertura_km']**2

    # Comparar área calculada com a área registrada
    erbs['Diferenca_Area_Percent'] = ((erbs['Area_Calculada_km2'] - erbs['Area_Cobertura_km2']) / erbs['Area_Calculada_km2']) * 100

    # Verificar estatísticas da diferença
    print("\nEstatísticas da diferença percentual entre área calculada e registrada:")
    print(erbs['Diferenca_Area_Percent'].describe())

    # Se houver diferenças significativas, podemos atualizar a área de cobertura
    limite_diferenca = 5  # 5% de diferença
    erbs_para_atualizar = erbs[abs(erbs['Diferenca_Area_Percent']) > limite_diferenca]

    if len(erbs_para_atualizar) > 0:
        print(f"\nEncontradas {len(erbs_para_atualizar)} ERBs com inconsistência >5% entre raio e área de cobertura")

        # Atualizar a área de cobertura para esses registros
        erbs.loc[abs(erbs['Diferenca_Area_Percent']) > limite_diferenca, 'Area_Cobertura_km2'] = erbs['Area_Calculada_km2']
        print("Áreas de cobertura inconsistentes foram atualizadas")

    # Remover colunas temporárias
    erbs = erbs.drop(columns=['Area_Calculada_km2', 'Diferenca_Area_Percent'])

# Verificação de outliers em parâmetros técnicos importantes
parametros_tecnicos = ['GanhoAntena', 'PotenciaTransmissorWatts', 'EIRP_dBm', 'Azimute']
for parametro in parametros_tecnicos:
    if parametro in erbs.columns:
        q1 = erbs[parametro].quantile(0.25)
        q3 = erbs[parametro].quantile(0.75)
        iqr = q3 - q1
        limite_inferior = q1 - (1.5 * iqr)
        limite_superior = q3 + (1.5 * iqr)

        outliers = erbs[(erbs[parametro] < limite_inferior) | (erbs[parametro] > limite_superior)]

        if len(outliers) > 0:
            print(f"\nEncontrados {len(outliers)} outliers em {parametro}")
            print(f"Limites: [{limite_inferior:.2f}, {limite_superior:.2f}]")
            print(f"Valores extremos: [{erbs[parametro].min():.2f}, {erbs[parametro].max():.2f}]")

            # Para EIRP_dBm, verificar se os outliers são realmente impossíveis
            # Valores muito altos podem ser válidos dependendo do tipo de antena
            if parametro == 'EIRP_dBm':
                # Verificar se há outliers extremos (por exemplo, >100 dBm, o que seria incomum)
                outliers_extremos = erbs[erbs[parametro] > 100]
                if len(outliers_extremos) > 0:
                    print(f"  Atenção: {len(outliers_extremos)} ERBs com EIRP_dBm > 100 dBm")
                    # Podemos ajustar ou marcar para revisão

# Adicionar coluna de categoria de potência para facilitar análises
if 'EIRP_dBm' in erbs.columns:
    # Criar categorias de potência de sinal
    bins = [float('-inf'), 40, 50, 60, 70, float('inf')]
    labels = ['Muito baixa', 'Baixa', 'Média', 'Alta', 'Muito alta']

    erbs['categoria_potencia'] = pd.cut(erbs['EIRP_dBm'], bins=bins, labels=labels)

    print("\nDistribuição das categorias de potência:")
    print(erbs['categoria_potencia'].value_counts())

# Adicionar coluna para identificar tecnologia principal
# (útil quando há múltiplas tecnologias por ERB)
if 'Tecnologia' in erbs.columns:
    def extrair_tecnologia_principal(tecnologia):
        if pd.isna(tecnologia):
            return "Não especificada"

        # Priorização das tecnologias (5G > 4G > 3G > 2G)
        if '5G' in tecnologia:
            return '5G'
        elif '4G' in tecnologia or 'LTE' in tecnologia:
            return '4G'
        elif '3G' in tecnologia or 'UMTS' in tecnologia:
            return '3G'
        elif '2G' in tecnologia or 'GSM' in tecnologia:
            return '2G'
        else:
            return tecnologia

    erbs['tecnologia_principal'] = erbs['Tecnologia'].apply(extrair_tecnologia_principal)

    print("\nDistribuição das tecnologias principais:")
    print(erbs['tecnologia_principal'].value_counts())

# Salvar as modificações na camada
erbs.to_file(filepath, layer='merged_layer', driver='GPKG')

print("\nProcessamento concluído com sucesso!")
print("Melhorias implementadas:")
print("1. Tratamento de valores nulos em tipoTecnologia quando possível")
print("2. Verificação e correção de inconsistências entre raio e área de cobertura")
print("3. Adição de categorização de potência para simplificar análises")
print("4. Extração de tecnologia principal para facilitar filtragem")
print("Os dados estão agora mais consistentes e enriquecidos para alimentar o modelo GNN")

import geopandas as gpd
import pandas as pd
import numpy as np

# Caminho para o arquivo
filepath = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Carregar a camada merged_layer com os dados das ERBs
print("Carregando dados das ERBs...")
erbs = gpd.read_file(filepath, layer='merged_layer')

# Mostrar dimensões iniciais
print(f"Dimensões originais do dataset: {erbs.shape}")

# Lista das colunas antes da remoção
print(f"Colunas originais: {sorted(erbs.columns.tolist())}")

# 1. Remover a coluna tipoTecnologia (alta taxa de nulos)
if 'tipoTecnologia' in erbs.columns:
    erbs = erbs.drop(columns=['tipoTecnologia'])
    print("Coluna 'tipoTecnologia' removida devido à alta taxa de valores nulos (91.1%)")

# 2. Remover a coluna Tecnologia (mantemos apenas tecnologia_principal que é mais consistente)
if 'Tecnologia' in erbs.columns:
    erbs = erbs.drop(columns=['Tecnologia'])
    print("Coluna 'Tecnologia' removida, mantendo apenas 'tecnologia_principal' que é mais consistente")

# 3. Remover colunas de valores originais (não precisamos do histórico)
colunas_originais = ['Area_Cobertura_Original_km2', 'Raio_Cobertura_Original_km']
colunas_para_remover = [col for col in colunas_originais if col in erbs.columns]
if colunas_para_remover:
    erbs = erbs.drop(columns=colunas_para_remover)
    print(f"Colunas de valores originais removidas: {colunas_para_remover}")

# 4. Verificar e remover colunas redundantes de geometria
# Primeiro, vamos verificar quais dessas colunas existem no dataset
colunas_geometria = ['geometria_setorial', 'setor_geometria', 'geometria_valida']
colunas_geometria_existentes = [col for col in colunas_geometria if col in erbs.columns]

# Verificar se são realmente redundantes (verificamos se são todas do tipo object e não têm valores únicos)
if colunas_geometria_existentes:
    info_redundancia = {}
    for col in colunas_geometria_existentes:
        tipo = erbs[col].dtype
        valores_unicos = erbs[col].nunique()
        info_redundancia[col] = {'tipo': tipo, 'valores_unicos': valores_unicos}

    print("\nInformações sobre colunas de geometria:")
    for col, info in info_redundancia.items():
        print(f"{col}: tipo={info['tipo']}, valores únicos={info['valores_unicos']}")

    # Identificar colunas que parecem ser redundantes (poucos valores únicos e tipo object)
    colunas_geometria_redundantes = [
        col for col in colunas_geometria_existentes
        if (info_redundancia[col]['tipo'] == 'object' and
            (info_redundancia[col]['valores_unicos'] <= 5 or
             info_redundancia[col]['valores_unicos'] / len(erbs) < 0.1))
    ]

    if colunas_geometria_redundantes:
        erbs = erbs.drop(columns=colunas_geometria_redundantes)
        print(f"Colunas de geometria redundantes removidas: {colunas_geometria_redundantes}")
    else:
        print("Nenhuma coluna de geometria foi identificada como redundante")
else:
    print("Nenhuma das colunas de geometria verificadas está presente no dataset")

# Mostrar dimensões finais
print(f"\nDimensões finais do dataset: {erbs.shape}")
print(f"Colunas finais: {sorted(erbs.columns.tolist())}")

# Calcular a redução de colunas
reducao_colunas = len(erbs.columns.tolist()) / len(sorted(erbs.columns.tolist()))
print(f"Redução no número de colunas: {(1 - reducao_colunas) * 100:.1f}%")

# Salvar as modificações na camada
erbs.to_file(filepath, layer='merged_layer', driver='GPKG')

print("\nProcessamento concluído com sucesso!")
print("Resumo das mudanças:")
print("1. Removida coluna 'tipoTecnologia' devido à alta taxa de valores nulos")
print("2. Removida coluna 'Tecnologia', mantendo apenas 'tecnologia_principal'")
print("3. Removidas colunas de valores originais para simplificar o dataset")
print("4. Verificadas e removidas colunas de geometria redundantes")
print("As colunas essenciais foram mantidas para o modelo GNN, incluindo:")
print("- EIRP_dBm e categoria_potencia para análise de potência de sinal")
print("- FreqRxMHz e FreqTxMHz para análise de frequência")
print("- Azimute, AnguloElevacao e GanhoAntena para características direcionais")
print("- geometry para representação espacial")

import geopandas as gpd
import pandas as pd
from shapely.geometry import Point
import os
from datetime import datetime

# Configurar o caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Criar uma cópia de backup
backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_path = f'/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_backup_{backup_time}.gpkg'
print(f"Criando backup em {backup_path}...")
import shutil
shutil.copy2(gpkg_path, backup_path)
print("Backup criado com sucesso.")

# Carregar a camada "voronoi"
print("Carregando a camada 'voronoi'...")
voronoi_gdf = gpd.read_file(gpkg_path, layer="voronoi")
print(f"Camada carregada com {len(voronoi_gdf)} registros.")
print("Sistema de coordenadas original:", voronoi_gdf.crs)

# Verificar as colunas atuais
print("Colunas originais:", voronoi_gdf.columns.tolist())

# Determinar um sistema de coordenadas UTM apropriado para a região
# Brasil central/Sorocaba está aproximadamente na zona UTM 23S
utm_crs = "EPSG:32723"  # UTM Zone 23S

# Converter para sistema de coordenadas projetadas para cálculos precisos
print(f"Convertendo para sistema projetado ({utm_crs})...")
voronoi_utm = voronoi_gdf.to_crs(utm_crs)

# Adicionar coluna de área em km²
print("Calculando áreas...")
voronoi_gdf['area_km2'] = voronoi_utm.geometry.area / 1_000_000  # m² para km²

# Adicionar coluna de perímetro em km
print("Calculando perímetros...")
voronoi_gdf['perimetro_km'] = voronoi_utm.geometry.length / 1_000  # m para km

# Calcular os centroides corretamente no sistema projetado
print("Calculando centroides...")
centroides_utm = voronoi_utm.geometry.centroid
# Converter os centroides de volta para o sistema original para consistência com outras camadas
centroides = centroides_utm.to_crs(voronoi_gdf.crs)
voronoi_gdf['centroide_x'] = centroides.x
voronoi_gdf['centroide_y'] = centroides.y

# Exibir estatísticas das novas colunas
print("\nEstatísticas das novas colunas:")
print("Área (km²):")
print(f"  Mínima: {voronoi_gdf['area_km2'].min():.4f}")
print(f"  Máxima: {voronoi_gdf['area_km2'].max():.4f}")
print(f"  Média:  {voronoi_gdf['area_km2'].mean():.4f}")
print(f"  Total:  {voronoi_gdf['area_km2'].sum():.4f}")

print("\nPerímetro (km):")
print(f"  Mínimo: {voronoi_gdf['perimetro_km'].min():.4f}")
print(f"  Máximo: {voronoi_gdf['perimetro_km'].max():.4f}")
print(f"  Médio:  {voronoi_gdf['perimetro_km'].mean():.4f}")

# Adicionar coluna com a forma (compacidade) do polígono
# Um círculo perfeito tem índice 1, formas menos compactas têm valores menores
print("Calculando índice de compacidade...")
voronoi_gdf['compacidade'] = (4 * 3.14159 * voronoi_gdf['area_km2']) / (voronoi_gdf['perimetro_km'] ** 2)

# Salvar a camada atualizada no GeoPackage
print("\nSalvando camada atualizada no GeoPackage...")
try:
    voronoi_gdf.to_file(gpkg_path, layer="voronoi", driver="GPKG", index=False)
    print("Camada 'voronoi' atualizada com sucesso!")

    # Verificar se as alterações foram salvas
    updated_voronoi = gpd.read_file(gpkg_path, layer="voronoi")
    print(f"Camada verificada: {len(updated_voronoi)} registros com {len(updated_voronoi.columns)} colunas")
    print("Novas colunas:", [col for col in updated_voronoi.columns if col not in ['geometry']])

except Exception as e:
    print(f"Erro ao salvar a camada atualizada: {e}")
    print(f"O backup está disponível em {backup_path}")

print("\nProcesso concluído!")

import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.geometry import Point
import os
from datetime import datetime
from tqdm.notebook import tqdm  # Para mostrar progresso em operações longas

# Configurar o caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Criar uma cópia de backup do arquivo original
backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_path = f'/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_backup_{backup_time}.gpkg'

# Verificar se o arquivo original existe antes de prosseguir
if not os.path.exists(gpkg_path):
    raise FileNotFoundError(f"O arquivo {gpkg_path} não foi encontrado.")

# Criar uma cópia de backup
print(f"Criando backup em {backup_path}...")
import shutil
shutil.copy2(gpkg_path, backup_path)
print("Backup criado com sucesso.")

# Carregar a camada "setores_com_edificacoes" do GeoPackage
print("Carregando a camada 'setores_com_edificacoes'...")
setores_gdf = gpd.read_file(gpkg_path, layer="setores_com_edificacoes")
print(f"Camada carregada com {len(setores_gdf)} registros.")
print("Sistema de coordenadas:", setores_gdf.crs)

# Verificar as colunas atuais
print("Colunas originais:", setores_gdf.columns.tolist())

# Definir sistema UTM para cálculos precisos
utm_crs = "EPSG:32723"  # UTM Zone 23S
print(f"Convertendo para sistema projetado ({utm_crs}) para cálculos...")
setores_utm = setores_gdf.to_crs(utm_crs)

# 1. Adicionar colunas de forma e centroides
print("Calculando métricas de forma e centroides...")
# Perímetro em km
setores_gdf['perimetro_km'] = setores_utm.geometry.length / 1000

# Compacidade (4π × Area/Perímetro²) - 1 para círculo perfeito, menor para formas mais complexas
setores_gdf['compacidade'] = (4 * np.pi * setores_gdf['area_km2']) / (setores_gdf['perimetro_km'] ** 2)

# Calcular centroides corretamente no sistema UTM e converter de volta
centroides_utm = setores_utm.geometry.centroid
centroides = centroides_utm.to_crs(setores_gdf.crs)
setores_gdf['centroide_x'] = centroides.x
setores_gdf['centroide_y'] = centroides.y

# 2. Adicionar métricas de densidade e proporções
print("Calculando métricas de densidade e proporções...")
# Densidade populacional estimada (pessoas por km²)
# Usar a camada "setores_censitarios" para obter dados populacionais, se disponível
try:
    setores_censo = gpd.read_file(gpkg_path, layer="setores_censitarios")
    # Verificar se há correspondência entre os códigos de setor
    if 'CD_SETOR' in setores_gdf.columns and 'CD_SETOR' in setores_censo.columns:
        # Juntar dados populacionais aos setores com edificações
        pop_data = setores_censo[['CD_SETOR', 'est_populacao', 'est_pop_pea']].copy()
        setores_gdf = setores_gdf.merge(pop_data, on='CD_SETOR', how='left')

        # Calcular densidade populacional
        setores_gdf['densidade_pop'] = setores_gdf['est_populacao'] / setores_gdf['area_km2']
        setores_gdf['densidade_pea'] = setores_gdf['est_pop_pea'] / setores_gdf['area_km2']

        print("Dados populacionais adicionados com sucesso.")
    else:
        print("Não foi possível relacionar os dados populacionais (campos de chave incompatíveis).")
        # Criar colunas vazias para manter consistência
        setores_gdf['densidade_pop'] = np.nan
        setores_gdf['densidade_pea'] = np.nan
except Exception as e:
    print(f"Não foi possível obter dados populacionais: {e}")
    # Criar colunas vazias para manter consistência
    setores_gdf['densidade_pop'] = np.nan
    setores_gdf['densidade_pea'] = np.nan

# 3. Métricas de desenvolvimento urbano
print("Calculando indicadores de desenvolvimento urbano...")
# Categorização da densidade de edificações
bins = [0, 5, 20, 100, 500, float('inf')]
labels = ['Muito baixa', 'Baixa', 'Média', 'Alta', 'Muito alta']
setores_gdf['categoria_densidade'] = pd.cut(setores_gdf['building_density'], bins=bins, labels=labels)

# Proporção de setores urbanos por município
urban_por_municipio = setores_gdf.groupby('NM_MUN')['is_urban'].mean() * 100
# Adicionar esta informação de volta aos setores
setores_gdf['urbanizacao_municipio'] = setores_gdf['NM_MUN'].map(urban_por_municipio)

# 4. Análise de vizinhança - abordagem mais eficiente usando índice espacial
print("Realizando análise de vizinhança (abordagem otimizada)...")
# Vamos usar uma abordagem mais eficiente para grandes conjuntos de dados
# Redefinir índice para garantir que não temos problemas com índices descontinuados
setores_utm = setores_utm.reset_index(drop=True)

# Criar buffers menores para reduzir o tempo de computação (100m em vez de 500m)
buffer_distance = 100  # metros
print(f"Criando buffers de {buffer_distance}m...")
buffers = setores_utm.geometry.buffer(buffer_distance)

# Usar o índice espacial do GeoPandas para acelerar a consulta
print("Construindo índice espacial...")
spatial_index = setores_utm.sindex

# Inicializar coluna de vizinhos
setores_gdf['num_vizinhos'] = 0

# Calcular vizinhança com progresso
print("Calculando intersecções (isso pode demorar um pouco)...")
# Para conjuntos de dados muito grandes, podemos processar em lotes
# Aqui vamos limitar o número de registros processados para demonstração
max_registros = min(len(setores_utm), 2000)  # Processar no máximo 2000 registros

for i in tqdm(range(max_registros), desc="Calculando vizinhos"):
    # Obter o buffer do setor atual
    buffer_geom = buffers.iloc[i]

    # Usar o índice espacial para encontrar possíveis intersecções
    # (muito mais rápido que verificar todos os polígonos)
    possible_matches_index = list(spatial_index.intersection(buffer_geom.bounds))

    # Remover o próprio setor da lista
    if i in possible_matches_index:
        possible_matches_index.remove(i)

    # Verificar intersecções reais (o bounds pode dar falsos positivos)
    neighbors = 0
    for j in possible_matches_index:
        if buffer_geom.intersects(setores_utm.loc[j, 'geometry']):
            neighbors += 1

    # Armazenar o número de vizinhos
    setores_gdf.loc[i, 'num_vizinhos'] = neighbors

print(f"Análise de vizinhança concluída para {max_registros} setores.")

# 5. Adicionar índice de complexidade da fronteira
print("Calculando índice de complexidade da fronteira...")
# Perímetro de um círculo com a mesma área: 2 * π * sqrt(área/π)
perimetro_circulo = 2 * np.pi * np.sqrt(setores_gdf['area_km2'] / np.pi)
setores_gdf['complexidade_fronteira'] = setores_gdf['perimetro_km'] / perimetro_circulo

# Exibir estatísticas das novas colunas
print("\nEstatísticas das novas colunas:")
print("Compacidade:")
print(f"  Mínima: {setores_gdf['compacidade'].min():.4f}")
print(f"  Máxima: {setores_gdf['compacidade'].max():.4f}")
print(f"  Média:  {setores_gdf['compacidade'].mean():.4f}")

if 'densidade_pop' in setores_gdf.columns and not setores_gdf['densidade_pop'].isna().all():
    print("\nDensidade populacional (hab/km²):")
    print(f"  Mínima: {setores_gdf['densidade_pop'].min():.2f}")
    print(f"  Máxima: {setores_gdf['densidade_pop'].max():.2f}")
    print(f"  Média:  {setores_gdf['densidade_pop'].mean():.2f}")

print("\nNúmero de vizinhos:")
print(f"  Mínimo: {setores_gdf['num_vizinhos'].min()}")
print(f"  Máximo: {setores_gdf['num_vizinhos'].max()}")
print(f"  Médio:  {setores_gdf['num_vizinhos'].mean():.2f}")

print("\nDistribuição de categorias de densidade:")
print(setores_gdf['categoria_densidade'].value_counts())

# Salvar a camada atualizada no GeoPackage
print("\nSalvando camada atualizada no GeoPackage...")
try:
    # Substituir a camada existente
    setores_gdf.to_file(gpkg_path, layer="setores_com_edificacoes", driver="GPKG", index=False)
    print("Camada 'setores_com_edificacoes' atualizada com sucesso!")

    # Verificar se as alterações foram salvas
    updated_setores = gpd.read_file(gpkg_path, layer="setores_com_edificacoes")
    print(f"Camada verificada: {len(updated_setores)} registros com {len(updated_setores.columns)} colunas")
    print("Novas colunas:", [col for col in updated_setores.columns if col not in setores_gdf.columns.tolist()[:10]])

except Exception as e:
    print(f"Erro ao salvar a camada atualizada: {e}")
    print(f"O backup está disponível em {backup_path}")

print("\nProcesso concluído!")

import geopandas as gpd
import pandas as pd
import numpy as np
import os
from datetime import datetime

# Configurar o caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Criar uma cópia de backup do arquivo original
backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_path = f'/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_backup_{backup_time}.gpkg'

print(f"Criando backup em {backup_path}...")
import shutil
shutil.copy2(gpkg_path, backup_path)
print("Backup criado com sucesso.")

# Carregar a camada "rodovias" do GeoPackage
print("Carregando a camada 'rodovias'...")
rodovias_gdf = gpd.read_file(gpkg_path, layer="rodovias")
print(f"Camada carregada com {len(rodovias_gdf)} registros.")

# Verificar as colunas atuais
print("Colunas originais:", rodovias_gdf.columns.tolist())
print("Sistema de coordenadas:", rodovias_gdf.crs)

# 1. Calcular comprimento das rodovias em km
print("Calculando comprimento das rodovias...")
if rodovias_gdf.crs and rodovias_gdf.crs.is_projected:
    # Já está em sistema projetado, calcular diretamente
    rodovias_gdf['comprimento_km'] = rodovias_gdf.geometry.length / 1000
else:
    # Converter para UTM para cálculos precisos
    utm_crs = "EPSG:32723"  # UTM Zone 23S para região de Sorocaba
    rodovias_utm = rodovias_gdf.to_crs(utm_crs)
    rodovias_gdf['comprimento_km'] = rodovias_utm.geometry.length / 1000

# 2. Criar um índice de importância baseado na classificação e prioridade
print("Criando índice de importância para vias...")
# Mapear tipos de via para um valor numérico de importância
highway_importance = {
    'motorway': 1.0,
    'trunk': 0.9,
    'primary': 0.8,
    'secondary': 0.7,
    'tertiary': 0.6,
    'residential': 0.4
}
rodovias_gdf['importancia_via'] = rodovias_gdf['highway'].map(highway_importance).fillna(0.3)

# Normalizar valores de prioridade para variar de 0 a 1 (onde 1 é mais importante)
max_priority = rodovias_gdf['priority'].max()
min_priority = rodovias_gdf['priority'].min()
if max_priority > min_priority:
    rodovias_gdf['prioridade_norm'] = 1 - ((rodovias_gdf['priority'] - min_priority) / (max_priority - min_priority))
else:
    rodovias_gdf['prioridade_norm'] = 1.0

# Índice composto de importância (50% tipo de via, 50% prioridade)
rodovias_gdf['indice_importancia'] = 0.5 * rodovias_gdf['importancia_via'] + 0.5 * rodovias_gdf['prioridade_norm']

# 3. Dividir geometrias MultiLineString em LineString simples para análises precisas
print("Convertendo MultiLineString para LineString simples...")
# Esta operação pode aumentar o número de registros
exploded = rodovias_gdf.explode(index_parts=True)
exploded = exploded.reset_index(drop=True)
print(f"Após explosão: {len(exploded)} segmentos de via (aumento de {len(exploded)-len(rodovias_gdf)} segmentos)")

# 4. Calcular a diferença de elevação para cada segmento
print("Analisando variação de elevação nos segmentos...")
try:
    # Assumindo que 'elevation' é uma única elevação para todo o segmento
    # Vamos buscar diferenças de elevação em segmentos adjacentes
    # Agregar por nome da via (quando disponível)
    elevations = exploded.dropna(subset=['name']).groupby('name')['elevation'].agg(['min', 'max', 'mean', 'std'])

    # Criar um dicionário para mapear nomes de vias para estatísticas de elevação
    elev_dict = elevations.to_dict('index')

    # Aplicar as estatísticas aos registros originais
    for stat in ['min', 'max', 'mean', 'std']:
        col_name = f'elevation_{stat}'
        # Inicializar colunas com NaN
        rodovias_gdf[col_name] = np.nan

        # Preencher valores para vias com nome
        for name, values in elev_dict.items():
            mask = rodovias_gdf['name'] == name
            rodovias_gdf.loc[mask, col_name] = values[stat]

    # Calcular variação de elevação (max - min)
    rodovias_gdf['elevation_range'] = rodovias_gdf['elevation_max'] - rodovias_gdf['elevation_min']

    print("Estatísticas de elevação calculadas com sucesso.")
except Exception as e:
    print(f"Erro ao calcular estatísticas de elevação: {e}")
    print("Continuando sem estatísticas de elevação.")

# 5. Identificar cruzamentos importantes (para uso posterior no grafo)
print("Identificando potenciais cruzamentos importantes...")
# Para identificar cruzamentos, precisaríamos analisar intersecções entre vias
# Esta é uma análise complexa que pode ser computacionalmente intensiva
# Vamos simplificar criando um indicador para vias que provavelmente têm muitos cruzamentos

# Categorizar vias por comprimento
bins = [0, 0.1, 0.5, 1, 5, float('inf')]
labels = ['Muito curta', 'Curta', 'Média', 'Longa', 'Muito longa']
rodovias_gdf['categoria_comprimento'] = pd.cut(rodovias_gdf['comprimento_km'], bins=bins, labels=labels)

# Exibir estatísticas das novas colunas
print("\nEstatísticas das novas colunas:")
print("Comprimento (km):")
print(f"  Mínimo: {rodovias_gdf['comprimento_km'].min():.4f}")
print(f"  Máximo: {rodovias_gdf['comprimento_km'].max():.4f}")
print(f"  Médio:  {rodovias_gdf['comprimento_km'].mean():.4f}")
print(f"  Total:  {rodovias_gdf['comprimento_km'].sum():.4f}")

print("\nÍndice de Importância:")
print(f"  Mínimo: {rodovias_gdf['indice_importancia'].min():.4f}")
print(f"  Máximo: {rodovias_gdf['indice_importancia'].max():.4f}")
print(f"  Médio:  {rodovias_gdf['indice_importancia'].mean():.4f}")

if 'elevation_range' in rodovias_gdf.columns:
    print("\nVariação de Elevação (m):")
    print(f"  Mínima: {rodovias_gdf['elevation_range'].min():.2f}")
    print(f"  Máxima: {rodovias_gdf['elevation_range'].max():.2f}")
    print(f"  Média:  {rodovias_gdf['elevation_range'].mean():.2f}")

print("\nDistribuição de Categorias de Comprimento:")
print(rodovias_gdf['categoria_comprimento'].value_counts())

# Salvar a camada atualizada no GeoPackage
print("\nSalvando camada atualizada no GeoPackage...")
try:
    # Substituir a camada existente
    rodovias_gdf.to_file(gpkg_path, layer="rodovias", driver="GPKG", index=False)
    print("Camada 'rodovias' atualizada com sucesso!")

    # Verificar se as alterações foram salvas
    updated_rodovias = gpd.read_file(gpkg_path, layer="rodovias")
    print(f"Camada verificada: {len(updated_rodovias)} registros com {len(updated_rodovias.columns)} colunas")
    print("Novas colunas:", [col for col in updated_rodovias.columns if col not in rodovias_gdf.columns.tolist()[:9]])

except Exception as e:
    print(f"Erro ao salvar a camada atualizada: {e}")
    print(f"O backup está disponível em {backup_path}")

print("\nProcesso concluído!")

import geopandas as gpd
import pandas as pd
import numpy as np
import os
from datetime import datetime

# Configurar o caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Criar uma cópia de backup do arquivo original
backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_path = f'/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_backup_{backup_time}.gpkg'

print(f"Criando backup em {backup_path}...")
import shutil
shutil.copy2(gpkg_path, backup_path)
print("Backup criado com sucesso.")

# Carregar a camada "uso_terra_ocupacao" do GeoPackage
print("Carregando a camada 'uso_terra_ocupacao'...")
uso_gdf = gpd.read_file(gpkg_path, layer="uso_terra_ocupacao")
print(f"Camada carregada com {len(uso_gdf)} registros.")

# Verificar as colunas atuais
print("Colunas originais (primeiras 10):", uso_gdf.columns.tolist()[:10])
print("Sistema de coordenadas:", uso_gdf.crs)

# 1. Adicionar fator de atenuação de sinal com base no tipo de uso do solo
print("Calculando fatores de atenuação de sinal...")
# Definir fatores de atenuação para diferentes tipos de uso do solo
# Valores hipotéticos: menor = menos atenuação, maior = mais atenuação
atenuacao_class = {
    'urban': 0.3,      # Áreas urbanas - atenuação média
    'industrial': 0.5, # Áreas industriais - alta atenuação
    'forest': 0.8,     # Florestas - muito alta atenuação
    'vegetation': 0.6, # Vegetação - alta atenuação
    'water': 0.2,      # Água - baixa atenuação
    'rural': 0.4,      # Rural - atenuação média-baixa
    'agriculture': 0.3, # Agricultura - atenuação média-baixa
    'recreation': 0.4,  # Recreação - atenuação média
    'other': 0.3        # Outros - valor padrão
}

# Aplicar fatores de atenuação
uso_gdf['fator_atenuacao'] = uso_gdf['class'].map(atenuacao_class).fillna(0.3)

# Ajustar atenuação com base em uso específico (landuse)
# Alguns tipos específicos de uso podem ter características diferentes
landuse_ajuste = {
    'residential': 0.05,    # Áreas residenciais têm um pouco mais de atenuação
    'commercial': -0.05,    # Áreas comerciais têm um pouco menos de atenuação (prédios mais altos)
    'grass': -0.2,          # Grama tem menos atenuação que vegetação densa
    'forest': 0.1           # Floresta densa tem mais atenuação
}

# Aplicar ajustes onde aplicável
for landuse, ajuste in landuse_ajuste.items():
    mask = uso_gdf['landuse'] == landuse
    if mask.any():
        uso_gdf.loc[mask, 'fator_atenuacao'] = uso_gdf.loc[mask, 'fator_atenuacao'] + ajuste

# Garantir que o fator de atenuação esteja entre 0 e 1
uso_gdf['fator_atenuacao'] = np.clip(uso_gdf['fator_atenuacao'], 0, 1)

# 2. Calcular índice de fragmentação (relação perímetro/área)
# Áreas mais fragmentadas tendem a ter mais variação na propagação de sinal
print("Calculando índice de fragmentação...")

# Converter para sistema UTM para cálculos precisos
if not uso_gdf.crs or not uso_gdf.crs.is_projected:
    utm_crs = "EPSG:32723"  # UTM Zone 23S
    uso_utm = uso_gdf.to_crs(utm_crs)
    perimetro = uso_utm.geometry.length
    # A área já está em km² na tabela
    area_m2 = uso_gdf['area_km2'] * 1_000_000  # Converter km² para m²
else:
    perimetro = uso_gdf.geometry.length
    area_m2 = uso_gdf['area_km2'] * 1_000_000  # Converter km² para m²

# Índice de fragmentação: maior valor = mais fragmentado
uso_gdf['indice_fragmentacao'] = perimetro / (2 * np.sqrt(np.pi * area_m2))

# 3. Categorização dos polígonos por tamanho
print("Categorizando áreas por tamanho...")
bins = [0, 0.01, 0.1, 1, 10, float('inf')]
labels = ['Muito pequena', 'Pequena', 'Média', 'Grande', 'Muito grande']
uso_gdf['categoria_area'] = pd.cut(uso_gdf['area_km2'], bins=bins, labels=labels)

# 4. Adicionar coluna para potencial de desenvolvimento (baseado no histórico de uso)
print("Analisando potencial de desenvolvimento...")
# Verificar se as colunas históricas existem
hist_cols = [col for col in uso_gdf.columns if col.startswith('USO20')]
if hist_cols:
    # Verificar se houve mudança no uso do solo (valores diferentes nas colunas USO*)
    uso_gdf['mudanca_uso'] = uso_gdf[hist_cols].nunique(axis=1) > 1

    # Verificar tendência de urbanização (aumento no valor das colunas USO*)
    # Assumindo que valores maiores indicam mais urbanização/desenvolvimento
    try:
        uso_gdf['tendencia_urbanizacao'] = (
            uso_gdf[hist_cols[-1]] - uso_gdf[hist_cols[0]]
        ) > 0
    except Exception as e:
        print(f"Erro ao calcular tendência de urbanização: {e}")
        uso_gdf['tendencia_urbanizacao'] = False
else:
    print("Colunas históricas não encontradas. Pulando análise de desenvolvimento.")
    uso_gdf['mudanca_uso'] = False
    uso_gdf['tendencia_urbanizacao'] = False

# 5. Criar um índice composto de adequação para ERBs
print("Criando índice de adequação para ERBs...")
# Fatores que favorecem a instalação de ERBs:
# - Baixa atenuação de sinal (inverso do fator de atenuação)
# - Baixa fragmentação do terreno
# - Presença de atividade humana (trabalhadores)
# - Elevação relativa (vantagem para locais mais altos)

# Normalizar variáveis para escala 0-1
def normalize(series):
    min_val = series.min()
    max_val = series.max()
    if max_val > min_val:
        return (series - min_val) / (max_val - min_val)
    else:
        return series * 0 + 0.5  # Valor médio se não há variação

# Inverso da atenuação (maior é melhor)
atenuacao_norm = 1 - uso_gdf['fator_atenuacao']

# Fragmentação (menor é melhor, então invertemos)
fragmentacao_norm = 1 - normalize(uso_gdf['indice_fragmentacao'])

# Atividade humana (baseada em trabalhadores, se disponível)
if 'est_trabalhadores' in uso_gdf.columns and not uso_gdf['est_trabalhadores'].isna().all():
    # Usar o logaritmo para reduzir o impacto de valores extremos
    trabalhadores_log = np.log1p(uso_gdf['est_trabalhadores'])
    trabalhadores_norm = normalize(trabalhadores_log)
else:
    # Se não há dados de trabalhadores, usar um valor neutro
    trabalhadores_norm = 0.5

# Elevação (maior é melhor, para alcance)
if 'elevation' in uso_gdf.columns and not uso_gdf['elevation'].isna().all():
    elevacao_norm = normalize(uso_gdf['elevation'])
else:
    elevacao_norm = 0.5

# Calcular índice composto
uso_gdf['indice_adequacao_erb'] = (
    0.4 * atenuacao_norm +          # 40% - Fator de atenuação (mais importante)
    0.2 * fragmentacao_norm +       # 20% - Fragmentação do terreno
    0.2 * trabalhadores_norm +      # 20% - Atividade humana
    0.2 * elevacao_norm             # 20% - Elevação
)

# Categorizar o índice
bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
labels = ['Muito baixa', 'Baixa', 'Média', 'Alta', 'Muito alta']
uso_gdf['adequacao_erb_cat'] = pd.cut(uso_gdf['indice_adequacao_erb'], bins=bins, labels=labels)

# Exibir estatísticas das novas colunas
print("\nEstatísticas das novas colunas:")
print("Fator de Atenuação:")
print(f"  Mínimo: {uso_gdf['fator_atenuacao'].min():.4f}")
print(f"  Máximo: {uso_gdf['fator_atenuacao'].max():.4f}")
print(f"  Médio:  {uso_gdf['fator_atenuacao'].mean():.4f}")

print("\nÍndice de Fragmentação:")
print(f"  Mínimo: {uso_gdf['indice_fragmentacao'].min():.4f}")
print(f"  Máximo: {uso_gdf['indice_fragmentacao'].max():.4f}")
print(f"  Médio:  {uso_gdf['indice_fragmentacao'].mean():.4f}")

print("\nÍndice de Adequação para ERBs:")
print(f"  Mínimo: {uso_gdf['indice_adequacao_erb'].min():.4f}")
print(f"  Máximo: {uso_gdf['indice_adequacao_erb'].max():.4f}")
print(f"  Médio:  {uso_gdf['indice_adequacao_erb'].mean():.4f}")

print("\nDistribuição de Categorias de Adequação para ERBs:")
print(uso_gdf['adequacao_erb_cat'].value_counts())

print("\nDistribuição de Categorias de Área:")
print(uso_gdf['categoria_area'].value_counts())

if 'mudanca_uso' in uso_gdf.columns:
    print("\nPolígonos com mudança de uso do solo:", uso_gdf['mudanca_uso'].sum())
    print("Polígonos com tendência de urbanização:", uso_gdf['tendencia_urbanizacao'].sum())

# Salvar a camada atualizada no GeoPackage
print("\nSalvando camada atualizada no GeoPackage...")
try:
    # Substituir a camada existente
    uso_gdf.to_file(gpkg_path, layer="uso_terra_ocupacao", driver="GPKG", index=False)
    print("Camada 'uso_terra_ocupacao' atualizada com sucesso!")

    # Verificar se as alterações foram salvas
    updated_uso = gpd.read_file(gpkg_path, layer="uso_terra_ocupacao")
    print(f"Camada verificada: {len(updated_uso)} registros com {len(updated_uso.columns)} colunas")
    new_cols = [col for col in updated_uso.columns if col not in uso_gdf.columns.tolist()[:10]]
    print(f"Novas colunas adicionadas: {len(new_cols)}")

except Exception as e:
    print(f"Erro ao salvar a camada atualizada: {e}")
    print(f"O backup está disponível em {backup_path}")

print("\nProcesso concluído!")

import geopandas as gpd
import pandas as pd
import numpy as np
import os
from datetime import datetime

# Configurar o caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Criar uma cópia de backup do arquivo original
backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_path = f'/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_backup_{backup_time}.gpkg'

print(f"Criando backup em {backup_path}...")
import shutil
shutil.copy2(gpkg_path, backup_path)
print("Backup criado com sucesso.")

# Carregar a camada "setores_censitarios" do GeoPackage
print("Carregando a camada 'setores_censitarios'...")
setores_gdf = gpd.read_file(gpkg_path, layer="setores_censitarios")
print(f"Camada carregada com {len(setores_gdf)} registros.")

# Verificar as colunas atuais
print("Colunas originais:", setores_gdf.columns.tolist())
print("Sistema de coordenadas:", setores_gdf.crs)

# Definir sistema UTM para cálculos precisos
utm_crs = "EPSG:32723"  # UTM Zone 23S para região de Sorocaba
print(f"Convertendo para sistema projetado ({utm_crs}) para cálculos...")
setores_utm = setores_gdf.to_crs(utm_crs)

# 1. Adicionar métricas de forma e localização
print("Calculando métricas de forma...")
# Perímetro em km
setores_gdf['perimetro_km'] = setores_utm.geometry.length / 1000

# Compacidade (4π × Area/Perímetro²) - 1 para círculo perfeito, menor para formas mais complexas
setores_gdf['compacidade'] = (4 * np.pi * setores_gdf['area_km2']) / (setores_gdf['perimetro_km'] ** 2)

# Calcular centroides corretamente no sistema UTM e converter de volta
centroides_utm = setores_utm.geometry.centroid
centroides = centroides_utm.to_crs(setores_gdf.crs)
setores_gdf['centroide_x'] = centroides.x
setores_gdf['centroide_y'] = centroides.y

# 2. Calcular métricas de demanda de conectividade
print("Calculando métricas de demanda de conectividade...")

# Densidade populacional (habitantes por km²)
setores_gdf['densidade_pop'] = setores_gdf['est_populacao'] / setores_gdf['area_km2']

# Adicionar pesos específicos por faixa etária para uso de serviços móveis
# Hipótese: pessoas mais jovens (15-59) usam mais serviços móveis
# Valores arbitrários baseados em possíveis padrões de uso
pesos_faixa_etaria = {
    'est_pop_0_14': 0.3,    # Crianças e adolescentes - uso moderado
    'est_pop_15_29': 1.0,   # Jovens adultos - uso intenso
    'est_pop_30_59': 0.8,   # Adultos - uso alto
    'est_pop_60_mais': 0.4  # Idosos - uso moderado
}

# Calcular demanda ponderada por faixa etária
pop_ponderada = 0
for col, peso in pesos_faixa_etaria.items():
    if col in setores_gdf.columns:
        pop_ponderada += setores_gdf[col] * peso

# Normalizar pela população total
setores_gdf['indice_demanda_conectividade'] = pop_ponderada / setores_gdf['est_populacao']
# Tratar casos onde não há população
setores_gdf['indice_demanda_conectividade'] = setores_gdf['indice_demanda_conectividade'].fillna(0)

# 3. Analisar variação temporal de população
print("Analisando variação temporal de população...")
# Calcular amplitude de variação (diferença entre máximo e mínimo da população ao longo do dia)
colunas_pop_temporal = ['pop_atual_0800', 'pop_atual_1200', 'pop_atual_1500', 'pop_atual_1900', 'pop_atual_2300']
if all(col in setores_gdf.columns for col in colunas_pop_temporal):
    # Criar um DataFrame temporário apenas com as colunas de população por horário
    pop_temporal = setores_gdf[colunas_pop_temporal]

    # Calcular variação absoluta (max - min)
    setores_gdf['variacao_pop_abs'] = pop_temporal.max(axis=1) - pop_temporal.min(axis=1)

    # Calcular variação relativa ((max - min) / média)
    setores_gdf['variacao_pop_rel'] = setores_gdf['variacao_pop_abs'] / pop_temporal.mean(axis=1)
    # Tratar infinitos e NaNs
    setores_gdf['variacao_pop_rel'] = setores_gdf['variacao_pop_rel'].replace([np.inf, -np.inf], np.nan).fillna(0)

    # Identificar tipo de área baseado na variação populacional
    # Critérios:
    # - Residencial: População alta à noite, baixa durante o dia
    # - Comercial: População alta durante o dia, baixa à noite
    # - Mista: Pouca variação ao longo do dia

    # População diurna média (8h, 12h, 15h)
    pop_diurna = setores_gdf[['pop_atual_0800', 'pop_atual_1200', 'pop_atual_1500']].mean(axis=1)

    # População noturna média (19h, 23h)
    pop_noturna = setores_gdf[['pop_atual_1900', 'pop_atual_2300']].mean(axis=1)

    # Razão diurna/noturna (>1 = mais pessoas durante o dia, <1 = mais pessoas à noite)
    setores_gdf['razao_dia_noite'] = pop_diurna / pop_noturna
    # Tratar infinitos e NaNs
    setores_gdf['razao_dia_noite'] = setores_gdf['razao_dia_noite'].replace([np.inf, -np.inf], np.nan).fillna(1)

    # Classificar áreas com base na razão e na variação
    conditions = [
        (setores_gdf['razao_dia_noite'] < 0.7) & (setores_gdf['variacao_pop_rel'] > 0.3),  # Residencial
        (setores_gdf['razao_dia_noite'] > 1.3) & (setores_gdf['variacao_pop_rel'] > 0.3),  # Comercial
        (setores_gdf['variacao_pop_rel'] <= 0.3)                                          # Mista
    ]
    choices = ['Residencial', 'Comercial', 'Mista']
    setores_gdf['tipo_area'] = np.select(conditions, choices, default='Indefinida')

    print("Análise de variação temporal concluída.")
else:
    print("Colunas de população temporal não encontradas. Pulando análise de variação.")

# 4. Criar um índice de prioridade para cobertura (continuação)
print("Criando índice de prioridade para cobertura...")
# Fatores que influenciam a prioridade:
# - Densidade populacional
# - Variação temporal (setores com grande variação precisam de capacidade extra)
# - Presença de população economicamente ativa (PEA)

# Normalizar variáveis para escala 0-1
def normalize(series):
    min_val = series.min()
    max_val = series.max()
    if max_val > min_val:
        return (series - min_val) / (max_val - min_val)
    else:
        return series * 0 + 0.5  # Valor médio se não há variação

# Densidade populacional normalizada
densidade_norm = normalize(setores_gdf['densidade_pop'])

# Variação populacional normalizada (se disponível)
if 'variacao_pop_rel' in setores_gdf.columns:
    variacao_norm = normalize(setores_gdf['variacao_pop_rel'])
else:
    variacao_norm = 0.5  # Valor neutro se não temos dados

# PEA normalizada (população economicamente ativa)
if 'est_pop_pea' in setores_gdf.columns:
    # Calcular densidade de PEA (PEA por km²)
    setores_gdf['densidade_pea'] = setores_gdf['est_pop_pea'] / setores_gdf['area_km2']
    pea_norm = normalize(setores_gdf['densidade_pea'])
else:
    pea_norm = 0.5  # Valor neutro se não temos dados

# Calcular índice composto de prioridade
setores_gdf['prioridade_cobertura'] = (
    0.4 * densidade_norm +      # 40% - Densidade populacional (mais importante)
    0.3 * pea_norm +            # 30% - População economicamente ativa
    0.3 * variacao_norm         # 30% - Variação populacional (demanda dinâmica)
)

# Categorizar o índice
bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
labels = ['Muito baixa', 'Baixa', 'Média', 'Alta', 'Muito alta']
setores_gdf['categoria_prioridade'] = pd.cut(setores_gdf['prioridade_cobertura'], bins=bins, labels=labels)

# 5. Adicionar índice de desafio para implementação
print("Calculando índice de desafio para implementação...")
# Fatores que tornam a implementação mais desafiadora:
# - Forma complexa (baixa compacidade)
# - Terreno não-urbano
# - Área grande

# Compacidade inversa (formas mais complexas são mais desafiadoras)
compacidade_inv = 1 - normalize(setores_gdf['compacidade'])

# Fator urbano (áreas rurais são mais desafiadoras)
setores_gdf['fator_urbano'] = setores_gdf['is_urban'].astype(float)
urbano_norm = normalize(setores_gdf['fator_urbano'])
urbano_inv = 1 - urbano_norm  # Inverter para que rural = alto desafio

# Área (áreas maiores são mais desafiadoras)
area_norm = normalize(setores_gdf['area_km2'])

# Calcular índice composto de desafio
setores_gdf['indice_desafio'] = (
    0.4 * compacidade_inv +     # 40% - Complexidade da forma
    0.4 * urbano_inv +          # 40% - Ruralidade
    0.2 * area_norm             # 20% - Tamanho da área
)

# Categorizar o índice
setores_gdf['categoria_desafio'] = pd.cut(setores_gdf['indice_desafio'], bins=bins, labels=labels)

# 6. Criar matriz de decisão: prioridade vs. desafio
print("Criando matriz de decisão estratégica...")
# Esta matriz ajuda a priorizar áreas para implementação de ERBs

# Convertendo categorias para valores numéricos (1-5)
cat_to_num = {'Muito baixa': 1, 'Baixa': 2, 'Média': 3, 'Alta': 4, 'Muito alta': 5}
setores_gdf['prioridade_num'] = setores_gdf['categoria_prioridade'].map(cat_to_num)
setores_gdf['desafio_num'] = setores_gdf['categoria_desafio'].map(cat_to_num)

# Criar matriz de decisão (combinação de prioridade e desafio)
# Alta prioridade + Baixo desafio = Implementação imediata
# Alta prioridade + Alto desafio = Análise detalhada
# Baixa prioridade + Baixo desafio = Segunda fase
# Baixa prioridade + Alto desafio = Não prioritário

# Mapear para estratégias de implementação
estrategia_matriz = {
    # Prioridade alta (4-5)
    (4, 1): 'Implementação imediata', (4, 2): 'Implementação imediata',
    (5, 1): 'Implementação imediata', (5, 2): 'Implementação imediata',
    (4, 3): 'Análise detalhada', (5, 3): 'Análise detalhada',
    (4, 4): 'Análise detalhada', (4, 5): 'Análise detalhada',
    (5, 4): 'Análise detalhada', (5, 5): 'Análise detalhada',

    # Prioridade média (3)
    (3, 1): 'Segunda fase', (3, 2): 'Segunda fase',
    (3, 3): 'Segunda fase',
    (3, 4): 'Análise detalhada', (3, 5): 'Análise detalhada',

    # Prioridade baixa (1-2)
    (1, 1): 'Não prioritário', (1, 2): 'Não prioritário',
    (2, 1): 'Não prioritário', (2, 2): 'Não prioritário',
    (1, 3): 'Não prioritário', (2, 3): 'Não prioritário',
    (1, 4): 'Não prioritário', (1, 5): 'Não prioritário',
    (2, 4): 'Não prioritário', (2, 5): 'Não prioritário'
}

# Aplicar matriz de decisão
setores_gdf['estrategia_implementacao'] = setores_gdf.apply(
    lambda row: estrategia_matriz.get((row['prioridade_num'], row['desafio_num']), 'Indefinida'),
    axis=1
)

# Exibir estatísticas das novas colunas
print("\nEstatísticas das novas colunas:")
print("Compacidade:")
print(f"  Mínima: {setores_gdf['compacidade'].min():.4f}")
print(f"  Máxima: {setores_gdf['compacidade'].max():.4f}")
print(f"  Média:  {setores_gdf['compacidade'].mean():.4f}")

print("\nDensidade populacional (hab/km²):")
print(f"  Mínima: {setores_gdf['densidade_pop'].min():.2f}")
print(f"  Máxima: {setores_gdf['densidade_pop'].max():.2f}")
print(f"  Média:  {setores_gdf['densidade_pop'].mean():.2f}")

if 'variacao_pop_rel' in setores_gdf.columns:
    print("\nVariação populacional relativa:")
    print(f"  Mínima: {setores_gdf['variacao_pop_rel'].min():.2f}")
    print(f"  Máxima: {setores_gdf['variacao_pop_rel'].max():.2f}")
    print(f"  Média:  {setores_gdf['variacao_pop_rel'].mean():.2f}")

print("\nÍndice de prioridade de cobertura:")
print(f"  Mínimo: {setores_gdf['prioridade_cobertura'].min():.4f}")
print(f"  Máximo: {setores_gdf['prioridade_cobertura'].max():.4f}")
print(f"  Médio:  {setores_gdf['prioridade_cobertura'].mean():.4f}")

print("\nDistribuição de estratégias de implementação:")
print(setores_gdf['estrategia_implementacao'].value_counts())

if 'tipo_area' in setores_gdf.columns:
    print("\nDistribuição de tipos de área:")
    print(setores_gdf['tipo_area'].value_counts())

# Salvar a camada atualizada no GeoPackage
print("\nSalvando camada atualizada no GeoPackage...")
try:
    # Substituir a camada existente
    setores_gdf.to_file(gpkg_path, layer="setores_censitarios", driver="GPKG", index=False)
    print("Camada 'setores_censitarios' atualizada com sucesso!")

    # Verificar se as alterações foram salvas
    updated_setores = gpd.read_file(gpkg_path, layer="setores_censitarios")
    print(f"Camada verificada: {len(updated_setores)} registros com {len(updated_setores.columns)} colunas")
    new_cols = [col for col in updated_setores.columns if col not in setores_gdf.columns.tolist()[:19]]
    print(f"Novas colunas adicionadas: {len(new_cols)}")

except Exception as e:
    print(f"Erro ao salvar a camada atualizada: {e}")
    print(f"O backup está disponível em {backup_path}")

print("\nProcesso concluído!")

import geopandas as gpd
import pandas as pd
import numpy as np
import os
from datetime import datetime
from shapely.geometry import MultiPolygon, Polygon
from tqdm.notebook import tqdm

# Configurar o caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# Criar uma cópia de backup do arquivo original
backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_path = f'/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado_backup_{backup_time}.gpkg'

print(f"Criando backup em {backup_path}...")
import shutil
shutil.copy2(gpkg_path, backup_path)
print("Backup criado com sucesso.")

# Listar todas as camadas disponíveis no GeoPackage
import fiona
available_layers = fiona.listlayers(gpkg_path)
print(f"Camadas disponíveis no GeoPackage: {len(available_layers)}")
print(available_layers)

# Primeiro, vamos carregar e processar a camada de setores censitários para criar o limite
print("\nCarregando camada de setores censitários...")
setores_gdf = gpd.read_file(gpkg_path, layer="setores_censitarios")
print(f"Camada carregada com {len(setores_gdf)} registros.")
print("Sistema de coordenadas:", setores_gdf.crs)

# Criar um único polígono que representa o limite externo de todos os setores
print("Criando limite externo dos setores censitários...")
try:
    limite_setores = setores_gdf.geometry.union_all()
    limite_gdf = gpd.GeoDataFrame(geometry=[limite_setores], crs=setores_gdf.crs)
    tipo_geom = limite_setores.geom_type
    print(f"Limite externo criado com sucesso. Tipo de geometria: {tipo_geom}")
    limite_gdf.geometry = limite_gdf.geometry.simplify(tolerance=10)
    print(f"Limite simplificado para melhorar performance.")
except Exception as e:
    print(f"Erro ao criar limite: {e}")
    try:
        dissolved = setores_gdf.dissolve()
        limite_gdf = gpd.GeoDataFrame(geometry=[dissolved.geometry.iloc[0]], crs=setores_gdf.crs)
        print("Limite criado usando abordagem alternativa.")
    except Exception as e2:
        print(f"Erro na abordagem alternativa: {e2}")
        raise

# Função para processar cada camada
def processar_camada(layer_name, limite_gdf):
    # Pular processamento das camadas climáticas completamente
    if "clima" in layer_name.lower():
        print(f"\nPreservando camada climática: {layer_name} sem processamento")
        return

    if layer_name == "setores_censitarios":
        print(f"Pulando camada '{layer_name}' (camada base de limite)")
        return

    print(f"\nProcessando camada: {layer_name}")

    # Carregar a camada
    try:
        gdf = gpd.read_file(gpkg_path, layer=layer_name)
        print(f"  Camada carregada com {len(gdf)} registros")

        # Verificar se a camada tem geometria
        if 'geometry' not in gdf.columns or gdf.geometry.isna().all():
            print(f"  A camada '{layer_name}' não tem geometrias válidas. Pulando...")
            return

        # Verificar/ajustar sistema de coordenadas
        if gdf.crs != limite_gdf.crs:
            print(f"  Convertendo sistema de coordenadas de {gdf.crs} para {limite_gdf.crs}")
            gdf = gdf.to_crs(limite_gdf.crs)

        # Contar registros originais
        registros_originais = len(gdf)

        try:
            # Aplicar filtro espacial (manter apenas geometrias que intersectam o limite)
            print("  Aplicando filtro espacial...")

            # Abordagem mais segura: primeiro selecionar intersecções, depois recortar
            # Passo 1: Identificar quais geometrias intersectam o limite
            intersects_mask = gdf.geometry.intersects(limite_gdf.geometry.iloc[0])
            gdf_intersecting = gdf[intersects_mask].copy()

            # Passo 2: Recortar essas geometrias para obter apenas as partes dentro do limite
            if len(gdf_intersecting) > 0:
                # Usar clip que é geralmente mais estável que overlay
                gdf_filtered = gpd.clip(gdf_intersecting, limite_gdf)
            else:
                # Se nada intersecta, criar DataFrame vazio com a mesma estrutura
                gdf_filtered = gdf.iloc[0:0].copy()

            # Contar registros após filtro
            registros_filtrados = len(gdf_filtered)
            registros_removidos = registros_originais - registros_filtrados

            print(f"  Registros originais: {registros_originais}")
            print(f"  Registros após filtro: {registros_filtrados}")
            print(f"  Registros removidos: {registros_removidos}", end="")

            if registros_originais > 0:
                percentual = (registros_removidos/registros_originais*100)
                print(f" ({percentual:.2f}% do total)")
            else:
                print("")

            # Se houve alguma remoção, salvar a camada atualizada
            if registros_removidos > 0:
                # Salvar camada filtrada no GeoPackage
                gdf_filtered.to_file(gpkg_path, layer=layer_name, driver="GPKG", index=False)
                print(f"  Camada '{layer_name}' atualizada com sucesso!")
            else:
                print(f"  Nenhum registro removido da camada '{layer_name}'")

        except Exception as e:
            print(f"  Erro durante o filtro espacial: {e}")
            print("  Tentando abordagem alternativa para esta camada...")

            # Abordagem alternativa simples: selecionar apenas as geometrias que intersectam
            try:
                # Selecionar apenas as geometrias que intersectam com o limite
                gdf_filtered = gdf[gdf.geometry.intersects(limite_gdf.geometry.iloc[0])]

                # Salvar o resultado, mesmo que não seja um recorte perfeito
                registros_filtrados = len(gdf_filtered)
                registros_removidos = registros_originais - registros_filtrados

                print(f"  Usando abordagem alternativa:")
                print(f"  Registros após filtro simples: {registros_filtrados}")
                print(f"  Registros removidos: {registros_removidos} ({registros_removidos/registros_originais*100:.2f}% do total)")

                # Salvar resultado
                gdf_filtered.to_file(gpkg_path, layer=layer_name, driver="GPKG", index=False)
                print(f"  Camada '{layer_name}' atualizada com abordagem alternativa!")

            except Exception as e2:
                print(f"  Erro na abordagem alternativa: {e2}")
                print(f"  A camada '{layer_name}' não pôde ser processada.")

    except Exception as e:
        print(f"  Erro ao carregar camada '{layer_name}': {e}")

# Processar cada camada do GeoPackage
print("\nIniciando processamento de todas as camadas...")
for layer_name in tqdm(available_layers, desc="Processando camadas"):
    processar_camada(layer_name, limite_gdf)

print("\nProcessamento concluído!")

# Verificar camadas após processamento
print("\nVerificando camadas após processamento:")
for layer_name in available_layers:
    if layer_name != "setores_censitarios":
        try:
            gdf = gpd.read_file(gpkg_path, layer=layer_name)
            print(f"Camada '{layer_name}': {len(gdf)} registros")
        except Exception as e:
            print(f"Erro ao verificar camada '{layer_name}': {e}")

print("\nProcesso de limpeza finalizado com sucesso!")

import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.geometry import Point, box
import os
from tqdm.notebook import tqdm
import pyproj
from shapely.ops import transform
from functools import partial

# Caminho para o arquivo GeoPackage
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

# 1. Carregar setores censitários
print("Carregando setores censitários...")
setores = gpd.read_file(gpkg_path, layer="setores_censitarios")
print(f"Setores carregados: {len(setores)}")

# Calculando limites da área de estudo
xmin, ymin, xmax, ymax = setores.total_bounds
print(f"Limites da área (UTM): {xmin}, {ymin}, {xmax}, {ymax}")

# 2. Carregar dados climáticos – somente colunas necessárias (sem geometria inicialmente)
print("Carregando dados climáticos (modo eficiente)...")
temp_parquet = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/temp/clima_pontos.parquet'
if os.path.exists(temp_parquet):
    clima_df = pd.read_parquet(
        temp_parquet,
        columns=['x', 'y', 'z', 'nivel_vertical', 'precipitacao',
                 'TEMPERATURA DO PONTO DE ORVALHO (°C)',
                 'RADIACAO GLOBAL (Kj/m²)',
                 'VENTO, RAJADA MAXIMA (m/s)']
    )
else:
    print("Arquivo Parquet não encontrado. Criando arquivo temporário otimizado...")
    clima_df = gpd.read_file(
        gpkg_path, layer="clima_pontos_grade",
        columns=['x', 'y', 'z', 'nivel_vertical', 'precipitacao',
                 'TEMPERATURA DO PONTO DE ORVALHO (°C)',
                 'RADIACAO GLOBAL (Kj/m²)',
                 'VENTO, RAJADA MAXIMA (m/s)', 'geometry']
    )
    os.makedirs('/content/drive/MyDrive/GrafosGeoespaciais/MBA/temp', exist_ok=True)
    clima_df.to_parquet(temp_parquet)

print(f"Dados climáticos carregados: {len(clima_df)} pontos")

# 3. Criar grade climática representativa
utm_to_sirgas = partial(
    pyproj.transform,
    pyproj.Proj(init='epsg:31983'),  # UTM
    pyproj.Proj(init='epsg:4674')     # SIRGAS 2000
)

envelope_utm = box(xmin, ymin, xmax, ymax)
envelope_sirgas = transform(utm_to_sirgas, envelope_utm)
sirgas_bounds = envelope_sirgas.bounds
print(f"Limites transformados (SIRGAS): {sirgas_bounds}")

x_min, y_min, x_max, y_max = sirgas_bounds
clima_area = clima_df[
    (clima_df['x'] >= x_min) &
    (clima_df['x'] <= x_max) &
    (clima_df['y'] >= y_min) &
    (clima_df['y'] <= y_max)
].copy()

print(f"Pontos dentro da área: {len(clima_area)}")

if len(clima_area) == 0:
    print("Criando grade simplificada...")
    x_vals = clima_df['x'].unique()
    y_vals = clima_df['y'].unique()
    avg_x_dist = np.mean(np.diff(np.sort(x_vals[:20])))
    avg_y_dist = np.mean(np.diff(np.sort(y_vals[:20])))
    print(f"Distância média entre pontos: {avg_x_dist}, {avg_y_dist}")

    grid_x = np.linspace(x_min, x_max, 10)
    grid_y = np.linspace(y_min, y_max, 10)
    grid_points = []
    for x in grid_x:
        for y in grid_y:
            for nivel in [0, 5, 9]:
                nivel_pts = clima_df[clima_df['nivel_vertical'] == nivel].copy()
                if nivel_pts.empty:
                    continue
                # Atribuir a coluna 'dist' usando .loc em uma cópia
                nivel_pts.loc[:, 'dist'] = ((nivel_pts['x'] - x)**2 + (nivel_pts['y'] - y)**2)**0.5
                closest = nivel_pts.loc[nivel_pts['dist'].idxmin()].copy()
                point_data = {
                    'x': x,
                    'y': y,
                    'z': closest['z'],
                    'nivel_vertical': nivel,
                    'precipitacao': closest['precipitacao'],
                    'TEMPERATURA DO PONTO DE ORVALHO (°C)': closest['TEMPERATURA DO PONTO DE ORVALHO (°C)'],
                    'RADIACAO GLOBAL (Kj/m²)': closest['RADIACAO GLOBAL (Kj/m²)'],
                    'VENTO, RAJADA MAXIMA (m/s)': closest['VENTO, RAJADA MAXIMA (m/s)']
                }
                grid_points.append(point_data)
    clima_area = pd.DataFrame(grid_points)
    print(f"Grade simplificada criada com {len(clima_area)} pontos")

# 4. Criar geometria para os pontos filtrados
clima_area = clima_area.copy()
clima_area['geometry'] = [Point(x, y) for x, y in zip(clima_area['x'], clima_area['y'])]
clima_gdf = gpd.GeoDataFrame(clima_area, geometry='geometry', crs='EPSG:4674')

# 5. Criar camadas para diferentes níveis
niveis_interesse = [0, 5, 9]
for nivel in niveis_interesse:
    pts_nivel = clima_gdf[clima_gdf['nivel_vertical'] == nivel].copy()
    if pts_nivel.empty:
        print(f"Sem dados para nível {nivel}. Pulando...")
        continue
    nivel_desc = "superficie" if nivel == 0 else ("medio" if nivel == 5 else "alto")
    pts_nivel = pts_nivel.copy()
    pts_nivel.loc[:, 'nivel_desc'] = nivel_desc  # uso de .loc em DataFrame copiado
    camada_nome = f"clima_{nivel_desc}"
    pts_nivel.to_file(gpkg_path, layer=camada_nome, driver="GPKG")
    print(f"Camada '{camada_nome}' criada com {len(pts_nivel)} pontos")

# 6. Processar setores para associar atributos climáticos
print("Criando camada de setores com atributos climáticos...")
var_clima = [
    'precipitacao',
    'TEMPERATURA DO PONTO DE ORVALHO (°C)',
    'RADIACAO GLOBAL (Kj/m²)',
    'VENTO, RAJADA MAXIMA (m/s)'
]

def process_sector_block(sectors_block):
    sectors_result = sectors_block.copy()
    sectors_4674 = sectors_block.to_crs('EPSG:4674').copy()
    for idx, setor in sectors_4674.iterrows():
        geom_simple = setor.geometry.simplify(0.001)
        bounds = geom_simple.bounds
        for nivel in niveis_interesse:
            pts_nivel = clima_gdf[clima_gdf['nivel_vertical'] == nivel].copy()
            pts_bbox = pts_nivel[
                (pts_nivel.geometry.x >= bounds[0]) &
                (pts_nivel.geometry.x <= bounds[2]) &
                (pts_nivel.geometry.y >= bounds[1]) &
                (pts_nivel.geometry.y <= bounds[3])
            ].copy()
            if not pts_bbox.empty:
                pts_setor = pts_bbox[pts_bbox.within(geom_simple)].copy()
                if len(pts_setor) < 3:
                    pts_setor = pts_bbox.copy()
            else:
                pts_temp = pts_nivel.copy()
                pts_temp.loc[:, 'dist_to_center'] = pts_temp.geometry.distance(geom_simple.centroid)
                pts_setor = pts_temp.nsmallest(5, 'dist_to_center').copy()
            if not pts_setor.empty:
                for var in var_clima:
                    sectors_result.loc[idx, f"{var}_n{nivel}"] = pts_setor[var].mean()
    return sectors_result

BLOCK_SIZE = 100
num_blocks = int(np.ceil(len(setores) / BLOCK_SIZE))
results = []
for i in tqdm(range(num_blocks), desc="Processando blocos de setores"):
    start_idx = i * BLOCK_SIZE
    end_idx = min(start_idx + BLOCK_SIZE, len(setores))
    sector_block = setores.iloc[start_idx:end_idx].copy()
    result_block = process_sector_block(sector_block)
    results.append(result_block)
setores_clima = pd.concat(results)
setores_clima_gdf = gpd.GeoDataFrame(setores_clima, geometry='geometry', crs=setores.crs)
setores_clima_gdf.to_file(gpkg_path, layer="setores_clima", driver="GPKG")
print("Camada 'setores_clima' criada com sucesso")

# 7. Criar camada resumida para visualização rápida
print("Criando camada resumida para visualização...")
cols_viz = [col for col in setores_clima_gdf.columns if 'precipitacao' in col or 'TEMPERATURA' in col]
setores_viz = setores_clima_gdf[['geometry'] + cols_viz].copy()
if 'precipitacao_n0' in setores_viz.columns:
    setores_viz = setores_viz.copy()
    setores_viz.loc[:, 'classe_precip'] = pd.cut(
        setores_viz['precipitacao_n0'],
        bins=[0, 0.05, 0.1, 0.15, 0.2],
        labels=['Baixa', 'Média', 'Alta', 'Muito Alta']
    )
setores_viz.to_file(gpkg_path, layer="clima_viz", driver="GPKG")
print("Camada 'clima_viz' criada com sucesso")

print("\nProcessamento concluído! Camadas climáticas criadas com sucesso.")

import geopandas as gpd
import json
import numpy as np
import os
import fiona

def serialize_record(record):
    """
    Converte objetos de geometria para WKT, se existirem,
    e converte valores numéricos do tipo numpy para tipos nativos do Python.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def analyze_layer(file_path, layer):
    """
    Realiza a análise detalhada de uma camada específica do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG.
        layer: Nome da camada a ser analisada.

    Returns:
        Um dicionário contendo o relatório de análise.
    """
    print(f"Analisando a camada: {layer}")
    gdf = gpd.read_file(file_path, layer=layer)

    report = {}
    report['layer_name'] = layer
    report['num_records'] = len(gdf)
    report['columns'] = list(gdf.columns)
    report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

    # Seleciona uma amostra aleatória de até 30 registros
    num_samples = min(30, len(gdf))
    sample_df = gdf.sample(n=num_samples, random_state=42) if len(gdf) > 0 else gdf
    sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
    report['sample_records'] = sample_records

    # Estatísticas para colunas numéricas
    numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
    if numeric_cols:
        report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
    else:
        report['numeric_summary'] = {}

    # Informações sobre a geometria
    if 'geometry' in gdf.columns:
        report['geometry_type'] = str(gdf.geometry.geom_type.value_counts().to_dict())
        report['geometry_is_valid'] = str(gdf.geometry.is_valid.value_counts().to_dict())
        report['geometry_bounds'] = str(gdf.total_bounds.tolist())

    # Contagem de valores nulos por coluna
    report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

    # Resumo das colunas categóricas (top 10 valores)
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
    if categorical_cols:
        unique_values = {}
        for col in categorical_cols:
            if col != 'geometry':
                unique_counts = gdf[col].value_counts().head(10).to_dict()
                unique_values[col] = {
                    'count': len(gdf[col].unique()),
                    'top_values': unique_counts
                }
        report['categorical_summary'] = unique_values

    return report

def main():
    # Caminho do arquivo que contém as camadas consolidadas
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'

    # Lista de camadas a serem analisadas
    layers_to_keep = [
        "merged_layer",
        "analise_consolidada",
        "voronoi",
        "setores_com_edificacoes",
        "edificacoes_prioritarias",
        "hidrografia",
        "rodovias",
        "ferrovias",
        "curvas_nivel_com_elevacao",
        "clima_pontos_grade",
        "clima_arestas_grade",
        "uso_terra_ocupacao",
        "setores_censitarios",
        "edificacoes"
    ]

    # Pasta para salvar os relatórios enriquecidos (cria se não existir)
    output_dir = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analise_json/enriquecido'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Lista as camadas disponíveis no arquivo
    all_layers = fiona.listlayers(input_file)
    print("Camadas encontradas no arquivo original:")
    for lyr in all_layers:
        print("  -", lyr)

    # Itera sobre cada camada desejada e gera um relatório individual
    for layer in layers_to_keep:
        if layer not in all_layers:
            print(f"Aviso: a camada '{layer}' não foi encontrada. Pulando...")
            continue

        report = analyze_layer(input_file, layer)
        report_file = os.path.join(output_dir, f'analysis_report_{layer}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=4)
        print(f"Relatório para a camada '{layer}' salvo em: {report_file}")

if __name__ == "__main__":
    main()

import os
import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.errors import TopologicalError
from sklearn.preprocessing import MinMaxScaler

# Define o sistema de referência alvo para garantir consistência (por exemplo, EPSG:4326)
TARGET_CRS = "EPSG:4326"

def scale_numeric_columns(gdf, numeric_cols):
    """
    Aplica normalização MinMax a todas as colunas numéricas e acrescenta colunas com o sufixo '_norm'.
    Essa etapa facilita a integração dos atributos com a GNN.
    """
    scaler = MinMaxScaler()
    try:
        # Ajusta e transforma os dados numéricos
        scaled_values = scaler.fit_transform(gdf[numeric_cols])
        # Cria um DataFrame com as colunas normalizadas
        df_scaled = pd.DataFrame(scaled_values,
                                 columns=[col + '_norm' for col in numeric_cols],
                                 index=gdf.index)
        # Concatena os valores normalizados com o GeoDataFrame original
        gdf = pd.concat([gdf, df_scaled], axis=1)
    except Exception as e:
        print("Erro durante a normalização:", e)
    return gdf

def preprocess_layer(layer_name, input_file, output_file):
    """
    Carrega, pré-processa e salva uma camada específica do arquivo GeoPackage.
    As etapas incluem:
      - Carregamento e reprojeção para TARGET_CRS;
      - Validação e correção das geometrias;
      - Tratamento de valores nulos (numéricos e categóricos);
      - Operações específicas para cada camada (ex.: filtro de elevação, cálculo de centroides,
        criação de índice composto);
      - Normalização dos atributos numéricos para integração com a GNN.
    Cada camada processada é salva no arquivo 'mba_preparado.gpkg' com o mesmo nome da camada.
    """
    print(f"\n--- Processando camada: {layer_name} ---")

    # Tenta carregar a camada
    try:
        gdf = gpd.read_file(input_file, layer=layer_name)
        print(f"Camada '{layer_name}' carregada com {len(gdf)} registros.")
    except Exception as e:
        print(f"Erro ao ler a camada {layer_name}: {e}")
        return

    # Reprojetar para TARGET_CRS, se necessário
    if gdf.crs != TARGET_CRS:
        try:
            gdf = gdf.to_crs(TARGET_CRS)
            print(f"Camada '{layer_name}' re-projetada para {TARGET_CRS}.")
        except Exception as e:
            print(f"Erro na reprojeção da camada {layer_name}: {e}")

    # Validação e correção de geometria
    try:
        # O buffer(0) é uma técnica comum para corrigir geometrias inválidas
        gdf['geometry'] = gdf['geometry'].buffer(0)
        print("Geometrias validadas e corrigidas (se necessário).")
    except (ValueError, TopologicalError) as e:
        print(f"Erro na validação das geometrias da camada {layer_name}: {e}")

    # Identificação dos tipos de colunas
    numeric_cols = gdf.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()

    # Preenchimento de valores nulos em colunas numéricas (usando mediana)
    for col in numeric_cols:
        if gdf[col].isnull().sum() > 0:
            mediana = gdf[col].median()
            gdf[col].fillna(mediana, inplace=True)
            print(f"Coluna numérica '{col}': preenchidos {gdf[col].isnull().sum()} nulos com a mediana ({mediana}).")

    # Preenchimento de valores nulos em colunas categóricas (usando 'desconhecido')
    for col in categorical_cols:
        if gdf[col].isnull().sum() > 0:
            gdf[col].fillna("desconhecido", inplace=True)
            print(f"Coluna categórica '{col}': valores nulos preenchidos com 'desconhecido'.")

    # Operações específicas para cada camada
    if layer_name == "curvas_nivel_com_elevacao":
        # Exemplo: Remover registros com elevações fora do intervalo plausível (0 a 10000)
        antes = len(gdf)
        gdf = gdf[(gdf['elevation'] >= 0) & (gdf['elevation'] <= 10000)]
        removidos = antes - len(gdf)
        print(f"Curvas de nível: removidos {removidos} registros com elevação fora do intervalo plausível.")

    elif layer_name == "hidrografia":
        # Padronizar valores na coluna 'waterway' e tratar campos nulos
        if 'waterway' in gdf.columns:
            gdf['waterway'] = gdf['waterway'].replace({None: "stream", "": "stream"})
            print("Hidrografia: padronizados os valores da coluna 'waterway'.")

    elif layer_name == "setores_censitarios":
        # Calcular centroides para incorporar informações espaciais nos atributos da GNN
        try:
            centroids = gdf.geometry.centroid
            gdf['centroid_x'] = centroids.x
            gdf['centroid_y'] = centroids.y
            print("Setores censitários: centroides calculados e adicionados ('centroid_x' e 'centroid_y').")
        except Exception as e:
            print(f"Erro ao calcular centroides na camada {layer_name}: {e}")

    elif layer_name == "uso_terra_ocupacao":
        # Criar um índice composto de uso do solo a partir das colunas USO2000, USO2010, etc.
        uso_cols = ['USO2000', 'USO2010', 'USO2012', 'USO2014', 'USO2016', 'USO2018']
        if all(col in gdf.columns for col in uso_cols):
            gdf['uso_medio'] = gdf[uso_cols].mean(axis=1)
            print("Uso do solo: coluna 'uso_medio' criada a partir das colunas de uso.")

    # Normalização dos atributos numéricos para facilitar a integração com a GNN
    if numeric_cols:
        gdf = scale_numeric_columns(gdf, numeric_cols)
        print("Atributos numéricos normalizados (novas colunas com sufixo '_norm' criadas).")

    # Salvar (ou atualizar) a camada processada no arquivo de saída
    try:
        # Se o arquivo de saída já existir, cada camada será atualizada com o mesmo nome.
        gdf.to_file(output_file, layer=layer_name, driver="GPKG")
        print(f"Camada '{layer_name}' salva/atualizada em '{output_file}'.")
    except Exception as e:
        print(f"Erro ao salvar a camada {layer_name}: {e}")

if __name__ == "__main__":
    # Definição dos arquivos de entrada e saída
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'
    output_file = 'mba_preparado.gpkg'

    # Lista de camadas a serem processadas nesta etapa (4 de um total de 14)
    layers_to_process = [
        "curvas_nivel_com_elevacao",
        "hidrografia",
        "setores_censitarios",
        "uso_terra_ocupacao"
    ]

    # Processa cada camada individualmente
    for layer in layers_to_process:
        preprocess_layer(layer, input_file, output_file)

import os
import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.errors import TopologicalError
from sklearn.preprocessing import MinMaxScaler

# Define o sistema de referência alvo para garantir consistência (por exemplo, EPSG:4326)
TARGET_CRS = "EPSG:4326"

def scale_numeric_columns(gdf, numeric_cols):
    """
    Aplica normalização MinMax a todas as colunas numéricas e acrescenta colunas com o sufixo '_norm'.
    Essa etapa facilita a integração dos atributos com a GNN.
    """
    scaler = MinMaxScaler()
    try:
        # Ajusta e transforma os dados numéricos
        scaled_values = scaler.fit_transform(gdf[numeric_cols])
        # Cria um DataFrame com as colunas normalizadas
        df_scaled = pd.DataFrame(scaled_values,
                                 columns=[col + '_norm' for col in numeric_cols],
                                 index=gdf.index)
        # Concatena os valores normalizados com o GeoDataFrame original
        gdf = pd.concat([gdf, df_scaled], axis=1)
    except Exception as e:
        print("Erro durante a normalização:", e)
    return gdf

def preprocess_layer(layer_name, input_file, output_file):
    """
    Carrega, pré-processa e salva uma camada específica do arquivo GeoPackage.
    As etapas incluem:
      - Carregamento e reprojeção para TARGET_CRS;
      - Validação e correção das geometrias;
      - Tratamento de valores nulos (numéricos e categóricos);
      - Operações específicas para cada camada (ex.: filtros, cálculos de centroides, índices compostos);
      - Normalização dos atributos numéricos para integração com a GNN.
    Cada camada processada é salva no arquivo 'mba_preparado.gpkg' com o mesmo nome da camada.
    """
    print(f"\n--- Processando camada: {layer_name} ---")

    # Tenta carregar a camada
    try:
        gdf = gpd.read_file(input_file, layer=layer_name)
        print(f"Camada '{layer_name}' carregada com {len(gdf)} registros.")
    except Exception as e:
        print(f"Erro ao ler a camada {layer_name}: {e}")
        return

    # Reprojetar para TARGET_CRS, se necessário
    if gdf.crs != TARGET_CRS:
        try:
            gdf = gdf.to_crs(TARGET_CRS)
            print(f"Camada '{layer_name}' re-projetada para {TARGET_CRS}.")
        except Exception as e:
            print(f"Erro na reprojeção da camada {layer_name}: {e}")

    # Validação e correção de geometria
    try:
        # O buffer(0) é uma técnica comum para corrigir geometrias inválidas
        gdf['geometry'] = gdf['geometry'].buffer(0)
        print("Geometrias validadas e corrigidas (se necessário).")
    except (ValueError, TopologicalError) as e:
        print(f"Erro na validação das geometrias da camada {layer_name}: {e}")

    # Identificação dos tipos de colunas
    numeric_cols = gdf.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()

    # Preenchimento de valores nulos em colunas numéricas (usando mediana)
    for col in numeric_cols:
        if gdf[col].isnull().sum() > 0:
            mediana = gdf[col].median()
            gdf[col].fillna(mediana, inplace=True)
            print(f"Coluna numérica '{col}': preenchidos {gdf[col].isnull().sum()} nulos com a mediana ({mediana}).")

    # Preenchimento de valores nulos em colunas categóricas (usando 'desconhecido')
    for col in categorical_cols:
        if gdf[col].isnull().sum() > 0:
            gdf[col].fillna("desconhecido", inplace=True)
            print(f"Coluna categórica '{col}': valores nulos preenchidos com 'desconhecido'.")

    # Operações específicas para cada camada
    if layer_name == "curvas_nivel_com_elevacao":
        # Remover registros com elevações fora do intervalo plausível (0 a 10000)
        antes = len(gdf)
        gdf = gdf[(gdf['elevation'] >= 0) & (gdf['elevation'] <= 10000)]
        removidos = antes - len(gdf)
        print(f"Curvas de nível: removidos {removidos} registros com elevação fora do intervalo plausível.")

    elif layer_name == "hidrografia":
        # Padronizar valores na coluna 'waterway'
        if 'waterway' in gdf.columns:
            gdf['waterway'] = gdf['waterway'].replace({None: "stream", "": "stream"})
            print("Hidrografia: padronizados os valores da coluna 'waterway'.")

    elif layer_name == "setores_censitarios":
        # Calcular centroides para incorporar informações espaciais
        try:
            centroids = gdf.geometry.centroid
            gdf['centroid_x'] = centroids.x
            gdf['centroid_y'] = centroids.y
            print("Setores censitários: centroides calculados e adicionados ('centroid_x' e 'centroid_y').")
        except Exception as e:
            print(f"Erro ao calcular centroides na camada {layer_name}: {e}")

    elif layer_name == "uso_terra_ocupacao":
        # Criar um índice composto de uso do solo a partir das colunas de uso
        uso_cols = ['USO2000', 'USO2010', 'USO2012', 'USO2014', 'USO2016', 'USO2018']
        if all(col in gdf.columns for col in uso_cols):
            gdf['uso_medio'] = gdf[uso_cols].mean(axis=1)
            print("Uso do solo: coluna 'uso_medio' criada a partir das colunas de uso.")

    # ------------------ Novas Camadas ------------------
    elif layer_name == "edificacoes_prioritarias":
        # Filtrar registros com elevação plausível (0 a 10000)
        antes = len(gdf)
        gdf = gdf[(gdf['elevation'] >= 0) & (gdf['elevation'] <= 10000)]
        removidos = antes - len(gdf)
        print(f"Edificações Prioritárias: removidos {removidos} registros com elevação fora do intervalo plausível.")
        # Padronizar o texto em 'building_type' para letras minúsculas (se existir)
        if 'building_type' in gdf.columns:
            gdf['building_type'] = gdf['building_type'].str.lower()
            print("Edificações Prioritárias: 'building_type' padronizado para letras minúsculas.")
        # Converter 'priority' para inteiro, se necessário
        if 'priority' in gdf.columns:
            gdf['priority'] = gdf['priority'].astype(int)

    elif layer_name == "setores_com_edificacoes":
        # Caso os centroides não estejam presentes, calcular e adicioná-los
        if not ('centroide_x' in gdf.columns and 'centroide_y' in gdf.columns):
            centroids = gdf.geometry.centroid
            gdf['centroide_x'] = centroids.x
            gdf['centroide_y'] = centroids.y
            print("Setores com edificações: centroides calculados e adicionados.")
        # Calcular um índice de densidade de edificações se não existir (building_count / area_km2)
        if 'building_count' in gdf.columns and 'area_km2' in gdf.columns:
            gdf['building_density_calc'] = gdf.apply(lambda row: row['building_count'] / row['area_km2'] if row['area_km2'] > 0 else 0, axis=1)
            print("Setores com edificações: 'building_density_calc' calculado.")

    elif layer_name == "edificacoes":
        # Preencher colunas de contagem de alunos com 0, pois representam quantidades
        for col in ["alunos_manha", "alunos_tarde_noite"]:
            if col in gdf.columns:
                gdf[col].fillna(0, inplace=True)
                print(f"Edificações: Coluna '{col}' preenchida com 0 para valores nulos.")
        # Converter a coluna 'edificacao_prioritaria' para boolean, se existir
        if 'edificacao_prioritaria' in gdf.columns:
            gdf['edificacao_prioritaria'] = gdf['edificacao_prioritaria'].astype(bool)
            print("Edificações: Coluna 'edificacao_prioritaria' convertida para boolean.")
        # Opcional: padronizar o campo 'building_type' para letras minúsculas
        if 'building_type' in gdf.columns:
            gdf['building_type'] = gdf['building_type'].str.lower()
            print("Edificações: 'building_type' padronizado para letras minúsculas.")

    # -----------------------------------------------------
    # Normalização dos atributos numéricos para integração com a GNN
    if numeric_cols:
        gdf = scale_numeric_columns(gdf, numeric_cols)
        print("Atributos numéricos normalizados (novas colunas com sufixo '_norm' criadas).")

    # Salvar (ou atualizar) a camada processada no arquivo de saída
    try:
        # Cada camada é salva (ou atualizada) com o mesmo nome no arquivo 'mba_preparado.gpkg'
        gdf.to_file(output_file, layer=layer_name, driver="GPKG")
        print(f"Camada '{layer_name}' salva/atualizada em '{output_file}'.")
    except Exception as e:
        print(f"Erro ao salvar a camada {layer_name}: {e}")

if __name__ == "__main__":
    # Definição dos arquivos de entrada e saída
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'
    output_file = 'mba_preparado.gpkg'

    # Lista das 3 novas camadas a serem processadas
    layers_to_process = [
        "edificacoes_prioritarias",
        "setores_com_edificacoes",
        "edificacoes"
    ]

    # Processa cada uma das novas camadas individualmente
    for layer in layers_to_process:
        preprocess_layer(layer, input_file, output_file)

import os
import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.errors import TopologicalError
from sklearn.preprocessing import MinMaxScaler

# Sistema de referência alvo (ex.: EPSG:4326)
TARGET_CRS = "EPSG:4326"

def scale_numeric_columns(gdf, numeric_cols):
    """
    Aplica normalização MinMax a todas as colunas numéricas e acrescenta colunas com o sufixo '_norm'.
    Essa etapa facilita a integração dos atributos com a GNN.
    """
    scaler = MinMaxScaler()
    try:
        scaled_values = scaler.fit_transform(gdf[numeric_cols])
        df_scaled = pd.DataFrame(scaled_values,
                                 columns=[col + '_norm' for col in numeric_cols],
                                 index=gdf.index)
        gdf = pd.concat([gdf, df_scaled], axis=1)
    except Exception as e:
        print("Erro durante a normalização:", e)
    return gdf

def preprocess_layer(layer_name, input_file, output_file):
    """
    Carrega, pré-processa e salva uma camada específica do arquivo GeoPackage.
    As etapas incluem:
      - Carregamento e reprojeção para TARGET_CRS;
      - Validação e correção das geometrias;
      - Tratamento de valores nulos (numéricos e categóricos);
      - Operações específicas para cada camada (ex.: filtragem, padronização, cálculo de índices ou centroides);
      - Normalização dos atributos numéricos para integração com a GNN.
    Cada camada processada é salva/atualizada no arquivo de saída.
    """
    print(f"\n--- Processando camada: {layer_name} ---")

    try:
        gdf = gpd.read_file(input_file, layer=layer_name)
        print(f"Camada '{layer_name}' carregada com {len(gdf)} registros.")
    except Exception as e:
        print(f"Erro ao ler a camada {layer_name}: {e}")
        return

    # Reprojeção, se necessário
    if gdf.crs != TARGET_CRS:
        try:
            gdf = gdf.to_crs(TARGET_CRS)
            print(f"Camada '{layer_name}' re-projetada para {TARGET_CRS}.")
        except Exception as e:
            print(f"Erro na reprojeção da camada {layer_name}: {e}")

    # Validação e correção de geometria
    try:
        gdf['geometry'] = gdf['geometry'].buffer(0)
        print("Geometrias validadas e corrigidas (se necessário).")
    except (ValueError, TopologicalError) as e:
        print(f"Erro na validação das geometrias da camada {layer_name}: {e}")

    # Identificação de colunas numéricas e categóricas
    numeric_cols = gdf.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()

    # Preenchimento de nulos: numéricos com mediana e categóricos com 'desconhecido'
    for col in numeric_cols:
        if gdf[col].isnull().sum() > 0:
            mediana = gdf[col].median()
            gdf[col].fillna(mediana, inplace=True)
            print(f"Coluna numérica '{col}': preenchidos nulos com a mediana ({mediana}).")
    for col in categorical_cols:
        if gdf[col].isnull().sum() > 0:
            gdf[col].fillna("desconhecido", inplace=True)
            print(f"Coluna categórica '{col}': nulos preenchidos com 'desconhecido'.")

    # ---------------- Operações Específicas para as novas camadas ----------------
    if layer_name == "analise_consolidada":
        # Garantir que os atributos numéricos estejam em faixa plausível
        antes = len(gdf)
        # Exemplo: remover registros com densidade ou potência negativa (se houver)
        gdf = gdf[(gdf['densidade_media'] >= 0) & (gdf['potencia_media'] >= 0)]
        removidos = antes - len(gdf)
        print(f"Analise Consolidada: removidos {removidos} registros com valores numéricos fora do esperado.")
        # Padronizar textos para minúsculas
        for col in ["vulnerabilidade", "classificacao_cobertura"]:
            if col in gdf.columns:
                gdf[col] = gdf[col].str.lower()
                print(f"Analise Consolidada: '{col}' padronizado para minúsculas.")

    elif layer_name == "merged_layer":
        # Padronizar colunas de texto importantes
        text_cols = ["Status.state", "NomeEntidade", "Operadora", "Municipio.NomeMunicipio", "tipo_area", "categoria_potencia", "tecnologia_principal"]
        for col in text_cols:
            if col in gdf.columns:
                gdf[col] = gdf[col].str.lower()
                print(f"Merged Layer: '{col}' padronizado para minúsculas.")
        # Se existirem colunas de frequência ou ângulos, garantir que sejam numéricas (já tratados na rotina de nulos)

    elif layer_name == "voronoi":
        # Verificar se os centroides já existem; caso contrário, calcular a partir da geometria
        if not ('centroide_x' in gdf.columns and 'centroide_y' in gdf.columns):
            centroids = gdf.geometry.centroid
            gdf['centroide_x'] = centroids.x
            gdf['centroide_y'] = centroids.y
            print("Voronoi: centroides calculados e adicionados ('centroide_x' e 'centroide_y').")
        # Verificar se compacidade é válida; opcionalmente, remover registros com compacidade fora de faixa [0,1]
        antes = len(gdf)
        gdf = gdf[(gdf['compacidade'] >= 0) & (gdf['compacidade'] <= 1)]
        removidos = antes - len(gdf)
        if removidos > 0:
            print(f"Voronoi: removidos {removidos} registros com compacidade fora do intervalo [0,1].")

    # -----------------------------------------------------------------------------
    # Normalização dos atributos numéricos para integração com a GNN
    if numeric_cols:
        gdf = scale_numeric_columns(gdf, numeric_cols)
        print("Atributos numéricos normalizados (colunas '_norm' criadas).")

    # Salvar/atualizar a camada processada no arquivo de saída
    try:
        gdf.to_file(output_file, layer=layer_name, driver="GPKG")
        print(f"Camada '{layer_name}' salva/atualizada em '{output_file}'.")
    except Exception as e:
        print(f"Erro ao salvar a camada {layer_name}: {e}")

if __name__ == "__main__":
    # Arquivo de entrada e saída
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'
    output_file = 'mba_preparado.gpkg'

    # Lista das novas camadas a serem processadas
    layers_to_process = [
        "analise_consolidada",
        "merged_layer",
        "voronoi"
    ]

    # Processa cada camada
    for layer in layers_to_process:
        preprocess_layer(layer, input_file, output_file)

import os
import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.errors import TopologicalError
from sklearn.preprocessing import MinMaxScaler

# Sistema de referência alvo para uniformizar os dados
TARGET_CRS = "EPSG:4326"

def scale_numeric_columns(gdf, numeric_cols):
    """
    Aplica normalização MinMax a todas as colunas numéricas e adiciona novas colunas com o sufixo '_norm'.
    Essa etapa garante que os atributos tenham escalas compatíveis para a integração com a GNN.
    """
    scaler = MinMaxScaler()
    try:
        scaled_values = scaler.fit_transform(gdf[numeric_cols])
        df_scaled = pd.DataFrame(scaled_values,
                                 columns=[col + '_norm' for col in numeric_cols],
                                 index=gdf.index)
        gdf = pd.concat([gdf, df_scaled], axis=1)
    except Exception as e:
        print("Erro durante a normalização:", e)
    return gdf

def preprocess_layer(layer_name, input_file, output_file):
    """
    Processa e salva uma camada específica do GeoPackage.

    Etapas:
      - Leitura da camada e reprojeção para TARGET_CRS;
      - Validação e correção das geometrias;
      - Preenchimento de valores nulos: numéricos com mediana e categóricos com 'desconhecido';
      - Operações específicas para cada camada:
            * Ferrovias: filtragem de elevações plausíveis, padronização de campos textuais.
            * Rodovias: filtragem (por exemplo, de elevações se aplicável), padronização dos campos de texto.
      - Normalização dos atributos numéricos.
      - Salvamento/atualização da camada processada no arquivo de saída.
    """
    print(f"\n--- Processando camada: {layer_name} ---")

    try:
        gdf = gpd.read_file(input_file, layer=layer_name)
        print(f"Camada '{layer_name}' carregada com {len(gdf)} registros.")
    except Exception as e:
        print(f"Erro ao ler a camada {layer_name}: {e}")
        return

    # Reprojeção
    if gdf.crs != TARGET_CRS:
        try:
            gdf = gdf.to_crs(TARGET_CRS)
            print(f"Camada '{layer_name}' re-projetada para {TARGET_CRS}.")
        except Exception as e:
            print(f"Erro na reprojeção da camada {layer_name}: {e}")

    # Validação e correção de geometria
    try:
        gdf['geometry'] = gdf['geometry'].buffer(0)
        print("Geometrias validadas e corrigidas (se necessário).")
    except (ValueError, TopologicalError) as e:
        print(f"Erro na validação das geometrias da camada {layer_name}: {e}")

    # Identificação de colunas numéricas e categóricas
    numeric_cols = gdf.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()

    # Preenchimento de valores nulos
    for col in numeric_cols:
        if gdf[col].isnull().sum() > 0:
            mediana = gdf[col].median()
            gdf[col].fillna(mediana, inplace=True)
            print(f"Coluna numérica '{col}': preenchidos nulos com a mediana ({mediana}).")
    for col in categorical_cols:
        if gdf[col].isnull().sum() > 0:
            gdf[col].fillna("desconhecido", inplace=True)
            print(f"Coluna categórica '{col}': nulos preenchidos com 'desconhecido'.")

    # Operações específicas por camada
    if layer_name == "ferrovias":
        # Filtragem: remover registros com elevações fora de um intervalo plausível (0 a 10000)
        antes = len(gdf)
        gdf = gdf[(gdf['elevation'] >= 0) & (gdf['elevation'] <= 10000)]
        removidos = antes - len(gdf)
        print(f"Ferrovias: removidos {removidos} registros com elevação fora do intervalo plausível.")
        # Padronizar campos de texto
        for col in ["railway", "rail_class", "name"]:
            if col in gdf.columns:
                gdf[col] = gdf[col].str.lower()
                print(f"Ferrovias: '{col}' padronizado para minúsculas.")

    elif layer_name == "rodovias":
        # Filtragem: se houver registros com 'comprimento_km' negativo ou zero, removê-los
        if "comprimento_km" in gdf.columns:
            antes = len(gdf)
            gdf = gdf[gdf["comprimento_km"] > 0]
            removidos = antes - len(gdf)
            if removidos > 0:
                print(f"Rodovias: removidos {removidos} registros com 'comprimento_km' <= 0.")
        # Padronizar campos de texto importantes
        text_cols = ["highway", "road_class", "name", "categoria_comprimento"]
        for col in text_cols:
            if col in gdf.columns:
                gdf[col] = gdf[col].str.lower()
                print(f"Rodovias: '{col}' padronizado para minúsculas.")
        # Se houver campos de elevação, filtrar valores plausíveis (por exemplo, 0 a 10000)
        if "elevation" in gdf.columns:
            antes = len(gdf)
            gdf = gdf[(gdf['elevation'] >= 0) & (gdf['elevation'] <= 10000)]
            removidos = antes - len(gdf)
            if removidos > 0:
                print(f"Rodovias: removidos {removidos} registros com 'elevation' fora do intervalo plausível.")

    # Normalização dos atributos numéricos para integração com a GNN
    if numeric_cols:
        gdf = scale_numeric_columns(gdf, numeric_cols)
        print("Atributos numéricos normalizados (novas colunas com sufixo '_norm' criadas).")

    # Salvamento incremental
    try:
        gdf.to_file(output_file, layer=layer_name, driver="GPKG")
        print(f"Camada '{layer_name}' salva/atualizada em '{output_file}'.")
    except Exception as e:
        print(f"Erro ao salvar a camada {layer_name}: {e}")

if __name__ == "__main__":
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'
    output_file = 'mba_preparado.gpkg'

    layers_to_process = ["ferrovias", "rodovias"]

    for layer in layers_to_process:
        preprocess_layer(layer, input_file, output_file)

import os
import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.errors import TopologicalError
from sklearn.preprocessing import MinMaxScaler

# Sistema de referência alvo (por exemplo, EPSG:4326)
TARGET_CRS = "EPSG:4326"

def scale_numeric_columns(gdf, numeric_cols):
    """
    Aplica normalização MinMax às colunas numéricas e cria novas colunas com sufixo '_norm'.
    """
    scaler = MinMaxScaler()
    try:
        scaled_values = scaler.fit_transform(gdf[numeric_cols])
        df_scaled = pd.DataFrame(scaled_values,
                                 columns=[col + "_norm" for col in numeric_cols],
                                 index=gdf.index)
        gdf = pd.concat([gdf, df_scaled], axis=1)
    except Exception as e:
        print("Erro durante a normalização:", e)
    return gdf

def preprocess_layer(layer_name, input_file, output_file):
    """
    Carrega, pré-processa e salva uma camada específica do arquivo GeoPackage.
    As etapas incluem:
      - Carregamento da camada e reprojeção para TARGET_CRS;
      - Validação e correção das geometrias;
      - Preenchimento de valores nulos (numéricos com mediana e categóricos com 'desconhecido');
      - Normalização dos atributos numéricos para facilitar a integração com a GNN.
    Cada camada processada é salva/atualizada no arquivo 'mba_preparado.gpkg' com o mesmo nome da camada.
    """
    print(f"\n--- Processando camada: {layer_name} ---")

    # Carrega a camada
    try:
        gdf = gpd.read_file(input_file, layer=layer_name)
        print(f"Camada '{layer_name}' carregada com {len(gdf)} registros.")
    except Exception as e:
        print(f"Erro ao ler a camada {layer_name}: {e}")
        return

    # Reprojetar para TARGET_CRS, se necessário
    if gdf.crs != TARGET_CRS:
        try:
            gdf = gdf.to_crs(TARGET_CRS)
            print(f"Camada '{layer_name}' re-projetada para {TARGET_CRS}.")
        except Exception as e:
            print(f"Erro na reprojeção da camada {layer_name}: {e}")

    # Validação e correção de geometria
    try:
        gdf['geometry'] = gdf['geometry'].buffer(0)
        print("Geometrias validadas e corrigidas (se necessário).")
    except (ValueError, TopologicalError) as e:
        print(f"Erro na validação das geometrias da camada {layer_name}: {e}")

    # Identificar colunas numéricas e categóricas
    numeric_cols = gdf.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()

    # Preencher valores nulos em colunas numéricas com a mediana
    for col in numeric_cols:
        if gdf[col].isnull().sum() > 0:
            mediana = gdf[col].median()
            gdf[col].fillna(mediana, inplace=True)
            print(f"Coluna numérica '{col}': nulos preenchidos com mediana ({mediana}).")

    # Preencher valores nulos em colunas categóricas com 'desconhecido'
    for col in categorical_cols:
        if gdf[col].isnull().sum() > 0:
            gdf[col].fillna("desconhecido", inplace=True)
            print(f"Coluna categórica '{col}': nulos preenchidos com 'desconhecido'.")

    # Operações específicas para as camadas climáticas (se necessário)
    if layer_name == "clima_arestas_grade":
        print("Processamento específico para a camada de arestas climáticas realizado.")
    elif layer_name == "clima_pontos_grade":
        print("Processamento específico para a camada de pontos climáticos realizado.")

    # Normalizar os atributos numéricos
    gdf = scale_numeric_columns(gdf, numeric_cols)
    print("Atributos numéricos normalizados (novas colunas com sufixo '_norm' criadas).")

    # Salvar/atualizar a camada processada
    try:
        gdf.to_file(output_file, layer=layer_name, driver="GPKG")
        print(f"Camada '{layer_name}' salva/atualizada em '{output_file}'.")
    except Exception as e:
        print(f"Erro ao salvar a camada {layer_name}: {e}")

if __name__ == "__main__":
    input_file = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/mba_consolidado.gpkg'
    output_file = 'mba_preparado.gpkg'
    layers_to_process = ["clima_arestas_grade", "clima_pontos_grade"]

    for layer in layers_to_process:
        preprocess_layer(layer, input_file, output_file)

import geopandas as gpd
import json
import numpy as np
import os
import fiona

def serialize_record(record):
    """
    Converte objetos de geometria para WKT, se existirem,
    e converte valores numéricos do tipo numpy para tipos nativos do Python.
    """
    serialized_record = {}
    for key, value in record.items():
        if hasattr(value, 'wkt'):
            serialized_record[key] = value.wkt
        elif isinstance(value, (np.int64, np.float64)):
            serialized_record[key] = float(value) if isinstance(value, np.float64) else int(value)
        else:
            serialized_record[key] = value
    return serialized_record

def analyze_layer(file_path, layer):
    """
    Realiza a análise detalhada de uma camada específica do arquivo GPKG.

    Args:
        file_path: Caminho para o arquivo GPKG.
        layer: Nome da camada a ser analisada.

    Returns:
        Um dicionário contendo o relatório de análise.
    """
    print(f"Analisando a camada: {layer}")
    gdf = gpd.read_file(file_path, layer=layer)

    report = {}
    report['layer_name'] = layer
    report['num_records'] = len(gdf)
    report['columns'] = list(gdf.columns)
    report['dtypes'] = {col: str(dtype) for col, dtype in gdf.dtypes.items()}

    # Seleciona uma amostra aleatória de até 30 registros
    num_samples = min(30, len(gdf))
    sample_df = gdf.sample(n=num_samples, random_state=42) if len(gdf) > 0 else gdf
    sample_records = [serialize_record(rec) for rec in sample_df.to_dict(orient='records')]
    report['sample_records'] = sample_records

    # Estatísticas para colunas numéricas
    numeric_cols = gdf.select_dtypes(include=['number']).columns.tolist()
    if numeric_cols:
        report['numeric_summary'] = gdf[numeric_cols].describe().to_dict()
    else:
        report['numeric_summary'] = {}

    # Informações sobre a geometria
    if 'geometry' in gdf.columns:
        report['geometry_type'] = gdf.geometry.geom_type.value_counts().to_dict()
        report['geometry_is_valid'] = gdf.geometry.is_valid.value_counts().to_dict()
        report['geometry_bounds'] = gdf.total_bounds.tolist()

    # Contagem de valores nulos por coluna
    report['null_values'] = {col: int(gdf[col].isna().sum()) for col in gdf.columns}

    # Resumo das colunas categóricas (top 10 valores)
    categorical_cols = gdf.select_dtypes(include=['object']).columns.tolist()
    if categorical_cols:
        unique_values = {}
        for col in categorical_cols:
            if col != 'geometry':
                unique_counts = gdf[col].value_counts().head(10).to_dict()
                unique_values[col] = {
                    'unique_count': int(len(gdf[col].unique())),
                    'top_values': unique_counts
                }
        report['categorical_summary'] = unique_values

    return report

def overall_analysis(file_path, layers):
    """
    Gera um relatório global que agrega, para todas as camadas analisadas,
    informações como número total de camadas, total de registros e lista de relatórios.
    """
    overall = {}
    overall["total_layers"] = len(layers)
    overall["total_records"] = 0
    overall["layers"] = {}

    for layer in layers:
        try:
            report = analyze_layer(file_path, layer)
            overall["layers"][layer] = report
            overall["total_records"] += report["num_records"]
        except Exception as e:
            print(f"Erro ao analisar a camada {layer}: {e}")

    return overall

def main():
    # Caminho do arquivo preparado
    input_file = '/content/mba_preparado.gpkg'

    # Diretório de saída para os relatórios
    output_dir = '/content/drive/MyDrive/GrafosGeoespaciais/MBA/analise_json/enriquecido'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Lista todas as camadas disponíveis no arquivo
    all_layers = fiona.listlayers(input_file)
    print("Camadas encontradas no arquivo preparado:")
    for lyr in all_layers:
        print("  -", lyr)

    # Gera e salva um relatório individual para cada camada
    for layer in all_layers:
        try:
            report = analyze_layer(input_file, layer)
            report_file = os.path.join(output_dir, f'analysis_report_{layer}.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=4)
            print(f"Relatório para a camada '{layer}' salvo em: {report_file}")
        except Exception as e:
            print(f"Erro ao processar a camada '{layer}': {e}")

    # Gera um relatório global agregando todas as camadas
    overall_report = overall_analysis(input_file, all_layers)
    overall_report_file = os.path.join(output_dir, 'analysis_report_overall.json')
    with open(overall_report_file, 'w', encoding='utf-8') as f:
        json.dump(overall_report, f, ensure_ascii=False, indent=4)
    print(f"\nRelatório global salvo em: {overall_report_file}")

if __name__ == "__main__":
    main()

import os
import json
import geopandas as gpd
import fiona

# Caminho para o geopackage preparado
gpkg_path = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/mba_preparado.gpkg"

# Listar as camadas disponíveis no geopackage
layers = fiona.listlayers(gpkg_path)
print("Camadas encontradas:", layers)

# Dicionário com definições customizadas para algumas camadas
custom_features = {
    "curvas_nivel_com_elevacao": ["elevation", "elevation_norm"],
    "hidrografia": ["cotrecho_norm", "nuordemcda_norm", "nunivotto_norm", "nustrahler_norm", "elevation_norm"],
    "setores_censitarios": [
        "est_populacao_norm",
        "densidade_pop_norm",
        "indice_demanda_conectividade_norm",
        "compacidade_norm",
        "area_km2_norm",
        "is_urban_norm"
    ]
}

# Dicionário para armazenar as features extraídas de cada camada
result = {}

# Itera sobre cada camada do geopackage
for layer in layers:
    print(f"\nProcessando camada: {layer}")
    try:
        # Lê a camada usando GeoPandas
        gdf = gpd.read_file(gpkg_path, layer=layer)
        print(f"  Registros na camada: {len(gdf)}")

        # Define as colunas de features:
        if layer in custom_features:
            feature_cols = custom_features[layer]
            print(f"  Utilizando features customizadas: {feature_cols}")
        else:
            # Se não há customização, tenta selecionar todas as colunas que terminam com '_norm'
            feature_cols = [col for col in gdf.columns if col.endswith("_norm")]
            if not feature_cols:
                # Se nenhuma coluna termina com _norm, seleciona as colunas numéricas
                feature_cols = gdf.select_dtypes(include=["int64", "float64"]).columns.tolist()
            print(f"  Utilizando features padrão: {feature_cols}")

        # Remove registros com valores nulos nas colunas selecionadas
        initial_count = len(gdf)
        gdf = gdf.dropna(subset=feature_cols)
        if len(gdf) < initial_count:
            print(f"  Removidos {initial_count - len(gdf)} registros com valores nulos.")

        # Extrai as features como uma lista de dicionários (um por registro)
        features_list = gdf[feature_cols].to_dict(orient="records")

        # Armazena o resultado com informações resumidas
        result[layer] = {
            "num_records": len(gdf),
            "feature_columns": feature_cols,
            "features": features_list
        }

        print(f"  Camada '{layer}' processada com {len(gdf)} registros.")
    except Exception as e:
        print(f"  ERRO ao processar a camada {layer}: {e}")

# Define o caminho para salvar o JSON (na mesma pasta do geopackage)
output_path = os.path.join(os.path.dirname(gpkg_path), "mba_features.json")
with open(output_path, "w") as f:
    json.dump(result, f, indent=4)

print("\nExtração completa. JSON salvo em:", output_path)

import os

pasta = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho"
arquivos = os.listdir(pasta)

print("Arquivos na pasta:")
for arquivo in arquivos:
    print(arquivo)

import os
import geopandas as gpd
import pandas as pd
import numpy as np
import torch

# ===============================
# Função para garantir que o diretório exista
# ===============================
def ensure_dir(filepath):
    directory = os.path.dirname(filepath)
    if not os.path.exists(directory):
        os.makedirs(directory)

# ===============================
# Função utilitária para reprojeção
# ===============================
def reproject_for_join(gdf, target_epsg=3857):
    """
    Se o GeoDataFrame estiver em um CRS geográfico, reprojeta para um CRS projetado (default: EPSG:3857).
    """
    if gdf.crs is None:
        raise ValueError("GeoDataFrame não possui CRS definido.")
    if gdf.crs.is_geographic:
        return gdf.to_crs(epsg=target_epsg)
    return gdf

# ===============================
# Funções de extração de features por camada
# ===============================
def extrair_features_hidrografia(gdf):
    gdf['slope_norm'] = gdf['elevation_norm'] / (gdf['cotrecho_norm'] + 1e-6)
    gdf['flow_potential'] = gdf['nustrahler_norm'] * (1 - gdf['elevation_norm'])
    gdf['is_stream'] = (gdf['waterway'].str.lower() == 'stream').astype(int)
    gdf['is_river'] = (gdf['waterway'].str.lower() == 'river').astype(int)
    gdf['is_canal'] = (gdf['waterway'].str.lower() == 'canal').astype(int)
    scale_factor = 1000.0
    gdf['elev_deviation'] = gdf['elevation'] - (gdf['elevation_norm'] * scale_factor)
    gdf['length_to_level_ratio'] = gdf['cotrecho_norm'] / (gdf['nunivotto_norm'] + 1e-6)
    return gdf

def extrair_features_curvas(gdf):
    mean_elev = gdf['elevation'].mean()
    gdf['diff_elev_mean'] = gdf['elevation'] - mean_elev
    gdf['ratio_elev_mean'] = gdf['elevation'] / (mean_elev + 1e-6)
    return gdf

def extrair_features_setores(gdf):
    gdf['area_perimetro_ratio'] = gdf['area_km2'] / (gdf['perimetro_km'] + 1e-6)
    gdf['pop_density_calc'] = gdf['est_populacao'] / (gdf['area_km2'] + 1e-6)
    return gdf

def extrair_features_uso_terra(gdf):
    gdf['trend_uso'] = gdf['USO2018_norm'] - gdf['USO2000_norm']
    cols_uso = ['USO2000_norm', 'USO2010_norm', 'USO2012_norm', 'USO2014_norm', 'USO2016_norm', 'USO2018_norm']
    gdf['media_uso_norm'] = gdf[cols_uso].mean(axis=1)
    return gdf

def extrair_features_edificacoes(gdf):
    gdf['total_alunos_norm'] = gdf['alunos_manha_norm'] + gdf['alunos_tarde_noite_norm']
    gdf['ratio_alunos'] = gdf['alunos_manha_norm'] / (gdf['alunos_tarde_noite_norm'] + 1e-6)
    return gdf

def extrair_features_edificacoes_prioritarias(gdf):
    mapping = {"educational": 1, "public": 2, "health": 3}
    gdf['building_type_code'] = gdf['building_type'].map(mapping).fillna(0)
    return gdf

def extrair_features_setores_edificacoes(gdf):
    gdf['area_perimetro_ratio'] = gdf['area_km2'] / (gdf['perimetro_km'] + 1e-6)
    return gdf

def drop_index_right(gdf):
    if "index_right" in gdf.columns:
        gdf = gdf.drop(columns=["index_right"])
    return gdf

def join_edificacoes_setores(edif_gdf, setores_gdf):
    edif_gdf = drop_index_right(edif_gdf)
    setores_gdf = drop_index_right(setores_gdf)
    edif_proj = reproject_for_join(edif_gdf)
    setores_proj = reproject_for_join(setores_gdf)
    joined = gpd.sjoin_nearest(edif_proj, setores_proj, how="left", distance_col="dist_setor")
    return joined

def join_with_priority(edif_gdf, prior_gdf):
    edif_gdf = drop_index_right(edif_gdf)
    prior_gdf = drop_index_right(prior_gdf)
    edif_proj = reproject_for_join(edif_gdf)
    prior_proj = reproject_for_join(prior_gdf)
    joined = gpd.sjoin_nearest(edif_proj, prior_proj[['priority_norm', 'elevation_norm', 'geometry']],
                               how="left", distance_col="dist_priority", lsuffix="_edif", rsuffix="_prior")
    joined['is_prioritaria'] = joined['dist_priority'] < 10  # threshold ajustável
    return joined

def extrair_features_erbs_consolidada(gdf):
    gdf['ratio_densidade_potencia'] = gdf['densidade_media_norm'] / (gdf['potencia_media_norm'] + 1e-6)
    def map_vulnerabilidade(text):
        text = str(text).lower()
        if "ideal" in text:
            return 1
        elif "sem cobertura" in text:
            return -1
        else:
            return 0
    gdf['vulnerabilidade_encoded'] = gdf['vulnerabilidade'].apply(map_vulnerabilidade)
    return gdf

def extrair_features_erbs_merged(gdf):
    gdf['diff_freq'] = gdf['FreqTxMHz_norm'] - gdf['FreqRxMHz_norm']
    gdf['ratio_freq'] = gdf['FreqTxMHz_norm'] / (gdf['FreqRxMHz_norm'] + 1e-6)
    gdf['indice_tecnico'] = (gdf['GanhoAntena_norm'] + gdf['PotenciaTransmissorWatts_norm']) / 2
    return gdf

def join_erbs_layers(gdf_cons, gdf_merged):
    gdf_cons = drop_index_right(gdf_cons)
    gdf_merged = drop_index_right(gdf_merged)
    cons_proj = reproject_for_join(gdf_cons)
    merged_proj = reproject_for_join(gdf_merged)
    gdf_join = gpd.sjoin_nearest(cons_proj, merged_proj, how="left", distance_col="dist_merged")
    return gdf_join

def join_with_voronoi(gdf_erbs, gdf_voronoi):
    gdf_erbs = drop_index_right(gdf_erbs)
    gdf_voronoi = drop_index_right(gdf_voronoi)
    erbs_proj = reproject_for_join(gdf_erbs)
    voronoi_proj = reproject_for_join(gdf_voronoi)
    cols_voronoi = ['area_km2_norm', 'perimetro_km_norm', 'compacidade_norm', 'geometry']
    gdf_joined = gpd.sjoin_nearest(erbs_proj, voronoi_proj[cols_voronoi], how="left", distance_col="dist_voronoi")
    return gdf_joined

def extrair_features_rodovias(gdf):
    gdf['delta_elev_norm'] = gdf['elevation_max_norm'] - gdf['elevation_min_norm']
    gdf['ratio_elev_mean'] = gdf['elevation_mean_norm'] / (gdf['elevation_norm'] + 1e-6)
    gdf['ratio_comprimento_importancia'] = gdf['comprimento_km_norm'] / (gdf['importancia_via_norm'] + 1e-6)
    return gdf

def extrair_features_ferrovias(gdf):
    gdf['log_elevation_norm'] = np.log(gdf['elevation_norm'] + 1e-6)
    return gdf

def extrair_features_pontos(gdf):
    gdf['dif_elevacao_norm'] = gdf['z_norm'] - gdf['elevacao_base_norm']
    gdf['ratio_elevacao'] = gdf['z_norm'] / (gdf['elevacao_base_norm'] + 1e-6)
    return gdf

def extrair_features_arestas(gdf_arestas, gdf_pontos):
    cols_ponto = ['ponto_id',
                  'TEMPERATURA DO PONTO DE ORVALHO (°C)_norm',
                  'VENTO, RAJADA MAXIMA (m/s)_norm',
                  'precipitacao_norm',
                  'RADIACAO GLOBAL (Kj/m²)_norm']
    gdf_origin = gdf_pontos[cols_ponto].rename(columns={
        'ponto_id': 'from_id',
        'TEMPERATURA DO PONTO DE ORVALHO (°C)_norm': 'temp_orig',
        'VENTO, RAJADA MAXIMA (m/s)_norm': 'vento_orig',
        'precipitacao_norm': 'precipitacao_orig',
        'RADIACAO GLOBAL (Kj/m²)_norm': 'radiacao_orig'
    })
    gdf_arestas = gdf_arestas.merge(gdf_origin, on='from_id', how='left')
    gdf_dest = gdf_pontos[cols_ponto].rename(columns={
        'ponto_id': 'to_id',
        'TEMPERATURA DO PONTO DE ORVALHO (°C)_norm': 'temp_dest',
        'VENTO, RAJADA MAXIMA (m/s)_norm': 'vento_dest',
        'precipitacao_norm': 'precipitacao_dest',
        'RADIACAO GLOBAL (Kj/m²)_norm': 'radiacao_dest'
    })
    gdf_arestas = gdf_arestas.merge(gdf_dest, on='to_id', how='left')
    gdf_arestas['delta_temp'] = gdf_arestas['temp_dest'] - gdf_arestas['temp_orig']
    gdf_arestas['delta_vento'] = gdf_arestas['vento_dest'] - gdf_arestas['vento_orig']
    gdf_arestas['delta_precipitacao'] = gdf_arestas['precipitacao_dest'] - gdf_arestas['precipitacao_orig']
    gdf_arestas['delta_radiacao'] = gdf_arestas['radiacao_dest'] - gdf_arestas['radiacao_orig']
    gdf_arestas['rel_delta_temp'] = gdf_arestas['delta_temp'] / (gdf_arestas['temp_orig'] + 1e-6)
    gdf_arestas['rel_delta_vento'] = gdf_arestas['delta_vento'] / (gdf_arestas['vento_orig'] + 1e-6)
    return gdf_arestas

# ===============================
# Funções genéricas de salvamento
# ===============================
def salvar_dados(gdf, caminho):
    ensure_dir(caminho)
    gdf.drop(columns="geometry", errors="ignore").to_csv(caminho, index=False)
    print(f"Arquivo CSV salvo em: {caminho}")

def salvar_features_pytorch(gdf, caminho):
    ensure_dir(caminho)
    df_numeric = gdf.select_dtypes(include=[np.number])
    tensor = torch.tensor(df_numeric.to_numpy(), dtype=torch.float32)
    torch.save(tensor, caminho)
    print(f"Features salvos em formato PyTorch em: {caminho}")

# ===============================
# Pipeline principal e consolidação final
# ===============================
def main():
    gpkg_path = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/mba_preparado.gpkg"

    # Prefixos para salvar os arquivos individuais
    base_csv = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/feature_extraction"
    base_pt = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/feature_extraction"

    # Lista para armazenar os DataFrames individuais (com a coluna "layer")
    consolidated_dfs = []

    # Dicionário para armazenar os dados CSV por camada (cada valor será uma lista de registros)
    csv_data = {}

    # Função auxiliar para armazenar os registros de uma camada
    def add_to_csv_data(gdf):
        layer = gdf["layer"].iloc[0]
        records = gdf.drop(columns="geometry", errors="ignore").to_dict(orient="records")
        if layer in csv_data:
            csv_data[layer].extend(records)
        else:
            csv_data[layer] = records

    # --- Camada "hidrografia" ---
    print("Processando camada 'hidrografia'...")
    gdf = gpd.read_file(gpkg_path, layer="hidrografia")
    gdf = extrair_features_hidrografia(gdf)
    gdf["layer"] = "hidrografia"
    salvar_dados(gdf, f"{base_csv}_hidrografia.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_hidrografia.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "curvas_nivel_com_elevacao" ---
    print("Processando camada 'curvas_nivel_com_elevacao'...")
    gdf = gpd.read_file(gpkg_path, layer="curvas_nivel_com_elevacao")
    gdf = extrair_features_curvas(gdf)
    gdf["layer"] = "curvas_nivel_com_elevacao"
    salvar_dados(gdf, f"{base_csv}_curvas_nivel_com_elevacao.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_curvas_nivel_com_elevacao.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "setores_censitarios" ---
    print("Processando camada 'setores_censitarios'...")
    gdf = gpd.read_file(gpkg_path, layer="setores_censitarios")
    gdf = extrair_features_setores(gdf)
    gdf["layer"] = "setores_censitarios"
    salvar_dados(gdf, f"{base_csv}_setores_censitarios.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_setores_censitarios.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "uso_terra_ocupacao" ---
    print("Processando camada 'uso_terra_ocupacao'...")
    gdf = gpd.read_file(gpkg_path, layer="uso_terra_ocupacao")
    gdf = extrair_features_uso_terra(gdf)
    gdf["layer"] = "uso_terra_ocupacao"
    salvar_dados(gdf, f"{base_csv}_uso_terra_ocupacao.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_uso_terra_ocupacao.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "edificacoes" ---
    print("Processando camada 'edificacoes'...")
    gdf = gpd.read_file(gpkg_path, layer="edificacoes")
    gdf = extrair_features_edificacoes(gdf)
    gdf["layer"] = "edificacoes"
    salvar_dados(gdf, f"{base_csv}_edificacoes.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_edificacoes.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "setores_com_edificacoes" ---
    print("Processando camada 'setores_com_edificacoes'...")
    gdf = gpd.read_file(gpkg_path, layer="setores_com_edificacoes")
    gdf = extrair_features_setores_edificacoes(gdf)
    gdf["layer"] = "setores_com_edificacoes"
    salvar_dados(gdf, f"{base_csv}_setores_com_edificacoes.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_setores_com_edificacoes.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "edificacoes_prioritarias" ---
    print("Processando camada 'edificacoes_prioritarias'...")
    gdf = gpd.read_file(gpkg_path, layer="edificacoes_prioritarias")
    gdf = extrair_features_edificacoes_prioritarias(gdf)
    gdf["layer"] = "edificacoes_prioritarias"
    salvar_dados(gdf, f"{base_csv}_edificacoes_prioritarias.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_edificacoes_prioritarias.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Junções para edificações ---
    print("Realizando junção entre 'edificacoes' e 'setores_com_edificacoes'...")
    gdf_joined = join_edificacoes_setores(consolidated_dfs[-4], consolidated_dfs[-3])
    print("Realizando junção para identificar edificações prioritárias...")
    gdf_enriquecidas = join_with_priority(gdf_joined, consolidated_dfs[-1])
    gdf_enriquecidas["layer"] = "edificacoes_enriquecidas"
    salvar_dados(gdf_enriquecidas, f"{base_csv}_edificacoes_enriquecidas.csv")
    salvar_features_pytorch(gdf_enriquecidas, f"{base_pt}_edificacoes_enriquecidas.pt")
    consolidated_dfs.append(gdf_enriquecidas)
    add_to_csv_data(gdf_enriquecidas)

    # --- Camada dos ERBs ---
    print("Processando ERBs...")
    gdf_cons = gpd.read_file(gpkg_path, layer="analise_consolidada")
    gdf_merged = gpd.read_file(gpkg_path, layer="merged_layer")
    gdf_voronoi = gpd.read_file(gpkg_path, layer="voronoi")
    gdf_cons = extrair_features_erbs_consolidada(gdf_cons)
    gdf_merged = extrair_features_erbs_merged(gdf_merged)
    gdf_erbs = join_erbs_layers(gdf_cons, gdf_merged)
    gdf_erbs = join_with_voronoi(gdf_erbs, gdf_voronoi)
    gdf_erbs["layer"] = "ERBs"
    salvar_dados(gdf_erbs, f"{base_csv}_ERBs.csv")
    salvar_features_pytorch(gdf_erbs, f"{base_pt}_ERBs.pt")
    consolidated_dfs.append(gdf_erbs)
    add_to_csv_data(gdf_erbs)

    # --- Camada "rodovias" ---
    print("Processando camada 'rodovias'...")
    gdf = gpd.read_file(gpkg_path, layer="rodovias")
    gdf = extrair_features_rodovias(gdf)
    gdf["layer"] = "rodovias"
    salvar_dados(gdf, f"{base_csv}_rodovias.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_rodovias.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "ferrovias" ---
    print("Processando camada 'ferrovias'...")
    gdf = gpd.read_file(gpkg_path, layer="ferrovias")
    gdf = extrair_features_ferrovias(gdf)
    gdf["layer"] = "ferrovias"
    salvar_dados(gdf, f"{base_csv}_ferrovias.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_ferrovias.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "clima_pontos_grade" ---
    print("Processando camada 'clima_pontos_grade'...")
    gdf = gpd.read_file(gpkg_path, layer="clima_pontos_grade")
    gdf = extrair_features_pontos(gdf)
    gdf["layer"] = "clima_pontos_grade"
    salvar_dados(gdf, f"{base_csv}_clima_pontos_grade.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_clima_pontos_grade.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Camada "clima_arestas_grade" ---
    print("Processando camada 'clima_arestas_grade'...")
    gdf = gpd.read_file(gpkg_path, layer="clima_arestas_grade")
    gdf = extrair_features_arestas(gdf, consolidated_dfs[-1])  # usando gdf_pontos já processado
    gdf["layer"] = "clima_arestas_grade"
    salvar_dados(gdf, f"{base_csv}_clima_arestas_grade.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_clima_arestas_grade.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    # --- Consolidação final ---
    print("Realizando consolidação de todas as camadas...")
    consolidated_df = pd.concat([df.drop(columns="geometry", errors="ignore") for df in consolidated_dfs],
                                ignore_index=True, sort=False)
    # Cria o tensor consolidado de features (apenas as colunas numéricas)
    df_numeric = consolidated_df.select_dtypes(include=[np.number])
    node_features = torch.tensor(df_numeric.to_numpy(), dtype=torch.float32)

    # Cria o mapeamento global dos nós por camada (ordenando as chaves alfabeticamente)
    mapping = {}
    start = 0
    for nt in sorted(csv_data.keys()):
        count = len(csv_data[nt])
        mapping[nt] = (start, start + count)
        start += count
    total_nodes = start
    print(f"Total de nós (conforme CSV): {total_nodes}")
    print("Mapping dos nós por camada:")
    print(mapping)

    # Cria o dicionário final para extração de features
    output = {
        "csv": csv_data,
        "node_features": node_features,
        "node_mapping": mapping
    }

    # Salva o dicionário final
    output_path = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "feature_extraction_dict.pt")
    torch.save(output, output_path)
    print(f"\nDicionário de extração de features salvo em: {output_path}")

if __name__ == "__main__":
    main()

import os
import time
import torch
import geopandas as gpd
import numpy as np
from sklearn.neighbors import NearestNeighbors

# --- Configuration ---
BASE_PATH = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho"
FEATURE_DICT_PATH = os.path.join(BASE_PATH, "feature_extraction_dict.pt")
GPKG_PATH = os.path.join(BASE_PATH, "mba_preparado.gpkg")
OUTPUT_NODE_FEATURES_PATH = os.path.join(BASE_PATH, "final_node_features.pt")
OUTPUT_EDGE_INDEX_PATH = os.path.join(BASE_PATH, "final_edge_index.pt")
OUTPUT_EDGE_WEIGHTS_PATH = os.path.join(BASE_PATH, "final_edge_weights.pt")
OUTPUT_NODE_MAPPING_PATH = os.path.join(BASE_PATH, "final_node_mapping.pt")

# k-NN parameters
K_INTRA = 5   # Número de vizinhos dentro da mesma camada
K_INTER = 1   # Número de vizinhos entre camadas diferentes
ADD_SELF_LOOPS = False  # Adiciona laços nos nós se necessário
MAKE_UNDIRECTED = True  # Cria arestas simétricas (grafo não direcionado)

# CRS para cálculo dos centróides (recomendado CRS projetado)
CRS_PROJECTED = "EPSG:3857"


# --- Função para calcular centróides ---
def calculate_centroids(gdf, target_crs):
    """
    Calcula os centróides de um GeoDataFrame após reprojetar para o CRS de destino.
    Retorna uma tupla com (array de coordenadas, índices originais válidos).
    """
    if gdf.empty or "geometry" not in gdf.columns:
        return np.empty((0, 2)), np.array([], dtype=int)

    original_indices = gdf.index.to_numpy()
    valid_mask = ~gdf.geometry.isna() & ~gdf.geometry.is_empty
    gdf_valid = gdf[valid_mask].copy()

    if gdf_valid.empty:
        return np.empty((0, 2)), np.array([], dtype=int)

    try:
        gdf_proj = gdf_valid.to_crs(target_crs)
        centroids = np.array([(geom.x, geom.y) for geom in gdf_proj.geometry])
        valid_indices = original_indices[valid_mask]
        return centroids, valid_indices
    except Exception as e:
        print(f"  Warning: Falha ao reprojetar/calcular centróides: {e}")
        return np.empty((0, 2)), np.array([], dtype=int)


# --- Função para carregar o dicionário de extração e adaptar o mapeamento ---
def load_feature_dict(feature_dict_path):
    try:
        feat_dict = torch.load(feature_dict_path)
        node_features = feat_dict["node_features"]
        raw_mapping = feat_dict["node_mapping"]  # Ex.: {layer_name: (start, end)}

        # Converte cada tupla em um dicionário com chaves 'inicio' e 'fim'
        node_mapping = {}
        for layer, tup in raw_mapping.items():
            # Se já estiver no formato dicionário, mantém
            if isinstance(tup, dict) and "inicio" in tup and "fim" in tup:
                node_mapping[layer] = tup
            else:
                node_mapping[layer] = {"inicio": tup[0], "fim": tup[1]}

        num_total_nodes = node_features.shape[0]
        print(f"Carregado dicionário de features com {num_total_nodes} nós no total.")
        return node_features, node_mapping
    except FileNotFoundError:
        print(f"Erro: Dicionário de extração não encontrado em {feature_dict_path}")
        exit()
    except Exception as e:
        print(f"Erro ao carregar o dicionário de extração: {e}")
        exit()


# --- Função para processar cada camada e calcular centróides válidos ---
def process_layer(layer_name, mapping_info):
    print(f"\nProcessando camada: '{layer_name}'")
    start_idx = mapping_info["inicio"]
    end_idx = mapping_info["fim"]
    num_expected = end_idx - start_idx

    try:
        gdf = gpd.read_file(GPKG_PATH, layer=layer_name)
        if len(gdf) != num_expected:
            print(f"  Aviso: Camada '{layer_name}' possui {len(gdf)} registros, mas o mapeamento espera {num_expected}.")
            num_expected = len(gdf)
        centroids, valid_local_indices = calculate_centroids(gdf, CRS_PROJECTED)
        num_valid = centroids.shape[0]
        print(f"  Encontrados {num_valid} geometria(s) válida(s) na camada '{layer_name}'.")

        # Mapeamento: de índice local (dentro do gdf) para índice global
        local_to_global = {int(local_idx): start_idx + int(local_idx) for local_idx in valid_local_indices}
        global_to_centroid_idx = {start_idx + int(local_idx): i for i, local_idx in enumerate(valid_local_indices)}

        # Lista dos índices globais dos nós válidos (na mesma ordem dos centróides)
        valid_global_indices = [local_to_global[int(idx)] for idx in valid_local_indices]

        return {
            "centroids": centroids,
            "num_valid": num_valid,
            "local_to_global": local_to_global,
            "global_to_centroid_idx": global_to_centroid_idx,
            "valid_global_indices": valid_global_indices
        }
    except Exception as e:
        print(f"  Erro ao processar camada '{layer_name}': {e}")
        return {
            "centroids": np.empty((0, 2)),
            "num_valid": 0,
            "local_to_global": {},
            "global_to_centroid_idx": {},
            "valid_global_indices": []
        }


# --- Função para construir arestas intra-camada usando K-NN ---
def build_intra_edges(layer_name, layer_data, k_intra, make_undirected):
    edges = []
    weights = []
    centroids = layer_data["centroids"]
    num_valid = layer_data["num_valid"]
    valid_global_indices = layer_data["valid_global_indices"]

    if num_valid <= 1:
        print(f"  Camada '{layer_name}': Menos de 2 nós válidos, pulando intra-KNN.")
        return edges, weights

    actual_k = min(k_intra, num_valid - 1)
    if actual_k <= 0:
        print(f"  Camada '{layer_name}': k_intra ajustado para 0, pulando.")
        return edges, weights

    try:
        nbrs = NearestNeighbors(n_neighbors=actual_k + 1, algorithm='ball_tree').fit(centroids)
        distances, indices = nbrs.kneighbors(centroids)
        count_edges = 0
        for i in range(num_valid):
            source = valid_global_indices[i]
            for n in range(1, actual_k + 1):  # Ignora o próprio nó (índice 0)
                j = indices[i, n]
                target = valid_global_indices[j]
                dist = distances[i, n]
                edges.append((source, target))
                weights.append(dist)
                count_edges += 1
                if make_undirected:
                    edges.append((target, source))
                    weights.append(dist)
                    count_edges += 1
        print(f"  Camada '{layer_name}': Adicionadas {count_edges} arestas intra-camada.")
    except Exception as e:
        print(f"  Erro no KNN intra-camada para '{layer_name}': {e}")

    return edges, weights


# --- Função para construir arestas inter-camada usando K-NN ---
def build_inter_edges(layerA_name, dataA, layerB_name, dataB, k_inter, make_undirected):
    edges = []
    weights = []

    if dataA["num_valid"] == 0 or dataB["num_valid"] == 0:
        print(f"  Pulando KNN intercamada entre '{layerA_name}' e '{layerB_name}' (nó(s) inválido(s)).")
        return edges, weights

    valid_global_A = dataA["valid_global_indices"]
    valid_global_B = dataB["valid_global_indices"]
    centroids_A = dataA["centroids"]
    centroids_B = dataB["centroids"]

    actual_k = min(k_inter, dataB["num_valid"])
    if actual_k <= 0:
        print(f"  Pulando KNN de '{layerA_name}' para '{layerB_name}' (k_inter = 0).")
        return edges, weights

    try:
        nbrs_B = NearestNeighbors(n_neighbors=actual_k, algorithm='ball_tree').fit(centroids_B)
        distances_AB, indices_AB = nbrs_B.kneighbors(centroids_A)
        count_edges = 0
        for i in range(dataA["num_valid"]):
            source = valid_global_A[i]
            for n in range(actual_k):
                j = indices_AB[i, n]
                target = valid_global_B[j]
                dist = distances_AB[i, n]
                edges.append((source, target))
                weights.append(dist)
                count_edges += 1
                if make_undirected:
                    edges.append((target, source))
                    weights.append(dist)
                    count_edges += 1
        print(f"  Arestas inter-camada '{layerA_name}' <-> '{layerB_name}': {count_edges} arestas adicionadas.")
    except Exception as e:
        print(f"  Erro no KNN inter-camada de '{layerA_name}' para '{layerB_name}': {e}")

    return edges, weights


# --- Função principal ---
def main():
    start_time = time.time()

    # Carrega o dicionário de features e o mapeamento de nós
    node_features, node_mapping = load_feature_dict(FEATURE_DICT_PATH)
    num_total_nodes = node_features.shape[0]

    # Salva os nós carregados (opcional)
    torch.save(node_features, OUTPUT_NODE_FEATURES_PATH)
    torch.save(node_mapping, OUTPUT_NODE_MAPPING_PATH)
    print(f"Nós e mapeamento salvos em:\n  {OUTPUT_NODE_FEATURES_PATH}\n  {OUTPUT_NODE_MAPPING_PATH}")

    # Processa cada camada para calcular os centróides válidos
    layers_data = {}
    for layer_name, mapping_info in node_mapping.items():
        layer_data = process_layer(layer_name, mapping_info)
        layers_data[layer_name] = layer_data

    print(f"\nCálculo dos centróides finalizado em {time.time() - start_time:.2f} segundos.")

    # Construção das arestas usando k-NN
    all_edges = []
    all_weights = []

    # Arestas intra-camada
    print("\nConstruindo arestas intra-camada...")
    for layer_name, data in layers_data.items():
        edges, weights = build_intra_edges(layer_name, data, K_INTRA, MAKE_UNDIRECTED)
        all_edges.extend(edges)
        all_weights.extend(weights)

    # Arestas inter-camada: itera por pares distintos de camadas
    print("\nConstruindo arestas inter-camada...")
    layer_names = list(layers_data.keys())
    for i in range(len(layer_names)):
        for j in range(i + 1, len(layer_names)):
            layerA = layer_names[i]
            layerB = layer_names[j]
            edges, weights = build_inter_edges(layerA, layers_data[layerA], layerB, layers_data[layerB], K_INTER, MAKE_UNDIRECTED)
            all_edges.extend(edges)
            all_weights.extend(weights)

    print(f"\nConstrução das arestas concluída em {time.time() - start_time:.2f} segundos.")

    # Adiciona self-loops se configurado
    if ADD_SELF_LOOPS:
        print(f"\nAdicionando self-loops para todos os {num_total_nodes} nós...")
        for i in range(num_total_nodes):
            all_edges.append((i, i))
            all_weights.append(0.0)

    # Remoção de arestas duplicadas
    num_edges_before = len(all_edges)
    if num_edges_before > 0:
        edge_set = set(all_edges)
        unique_edges_map = {edge: i for i, edge in enumerate(all_edges) if edge in edge_set}
        sorted_unique_edges = sorted(edge_set, key=lambda edge: unique_edges_map[edge])
        unique_weights = [all_weights[unique_edges_map[edge]] for edge in sorted_unique_edges]
        all_edges = sorted_unique_edges
        all_weights = unique_weights
        print(f"Removidas {num_edges_before - len(all_edges)} arestas duplicadas.")

    # Converte as arestas para tensor do PyTorch
    if not all_edges:
        print("Aviso: Nenhuma aresta foi criada, salvando edge_index vazio.")
        edge_index = torch.empty((2, 0), dtype=torch.long)
        edge_weights = torch.empty((0,), dtype=torch.float)
    else:
        edge_index = torch.tensor(all_edges, dtype=torch.long).t().contiguous()
        edge_weights = torch.tensor(all_weights, dtype=torch.float)
        print(f"edge_index: shape {edge_index.shape}")
        print(f"edge_weights: shape {edge_weights.shape}")

    # Salva os resultados finais
    torch.save(edge_index, OUTPUT_EDGE_INDEX_PATH)
    torch.save(edge_weights, OUTPUT_EDGE_WEIGHTS_PATH)
    print(f"\nArestas salvas em:\n  {OUTPUT_EDGE_INDEX_PATH}\n  {OUTPUT_EDGE_WEIGHTS_PATH}")
    print(f"\nConstrução do grafo concluída em {time.time() - start_time:.2f} segundos.")


if __name__ == "__main__":
    main()

import os
import torch
import json
import numpy as np
from torch.serialization import safe_globals
from torch_geometric.data import HeteroData

# Caminhos dos arquivos
base_path = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho"
node_features_path = os.path.join(base_path, "node_features.pt")
full_adj_path = os.path.join(base_path, "full_adjacency.pt")
feature_extraction_path = os.path.join(base_path, "feature_extraction_dict.pt")
hetero_graph_path = os.path.join(base_path, "hetero_graph_data.pt")

###############################################################################
# Carregamento dos dados
###############################################################################
# Carrega o tensor global de features dos nós
node_features = torch.load(node_features_path)  # shape: [total_nós, num_features]

# Carrega o dicionário de adjacência utilizando safe_globals para permitir "numpy.core.multiarray.scalar"
with safe_globals(["numpy.core.multiarray.scalar"]):
    full_adj = torch.load(full_adj_path, weights_only=False)

# Carrega o dicionário de extração de features (espera-se a chave "csv")
feature_extraction = torch.load(feature_extraction_path)
if "csv" not in feature_extraction:
    raise ValueError("Estrutura inesperada em feature_extraction.pt. Esperava-se a chave 'csv'.")
csv_data = feature_extraction["csv"]

###############################################################################
# Mapeamento dos nós por camada (usando as chaves do CSV em ordem alfabética)
###############################################################################
node_types = sorted(csv_data.keys())
mapping = {}  # mapping[node_type] = (start, end) com end exclusivo
start = 0
for nt in node_types:
    count = len(csv_data[nt])
    mapping[nt] = (start, start + count)
    start += count
total_nodes = start
print(f"Total de nós (conforme CSV): {total_nodes}")
print("Mapping dos nós por camada:")
print(mapping)

###############################################################################
# Criação do Grafo Heterogêneo
###############################################################################
data = HeteroData()

# Atribuição dos nós para cada camada, particionando o tensor global de features
for nt in node_types:
    s, e = mapping[nt]
    data[nt].x = node_features[s:e]
    print(f"Camada '{nt}': {data[nt].x.shape[0]} nós com {data[nt].x.shape[1]} features.")

# Armazena metadados extra (opcional)
data.feature_extraction = feature_extraction
data.node_mapping = mapping

###############################################################################
# Adição das arestas
###############################################################################
# (1) Arestas intra-camada: converte índices locais para globais usando o mapeamento
for nt, adj in full_adj.get("intra", {}).items():
    if nt not in mapping:
        print(f"Aviso: Não há mapeamento para a camada {nt}. Ignorando arestas intra.")
        continue
    s, _ = mapping[nt]
    edges = []
    for i, neighbors in adj.items():
        for j in neighbors:
            edges.append((s + int(i), s + int(j)))
    if edges:
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        data[(nt, 'intra', nt)].edge_index = edge_index
        print(f"Arestas intra para '{nt}': {edge_index.shape[1]} arestas.")
    else:
        print(f"Aviso: Nenhuma aresta intra para '{nt}'.")

# (2) Arestas inter-camada: para cada par "nt_A|nt_B", converte os índices locais para globais
for key, edges in full_adj.get("inter", {}).items():
    try:
        nt_A, nt_B = key.split("|")
    except Exception as e:
        print(f"Erro ao dividir a chave '{key}': {e}")
        continue
    if nt_A not in mapping or nt_B not in mapping:
        print(f"Aviso: Mapeamento não encontrado para uma das camadas em '{key}'.")
        continue
    sA, _ = mapping[nt_A]
    sB, _ = mapping[nt_B]
    global_edges = []
    for (i, j) in edges:
        global_edges.append((sA + int(i), sB + int(j)))
    if global_edges:
        edge_index = torch.tensor(global_edges, dtype=torch.long).t().contiguous()
        data[(nt_A, 'inter', nt_B)].edge_index = edge_index
        print(f"Arestas inter entre '{nt_A}' -> '{nt_B}': {edge_index.shape[1]} arestas.")
    else:
        print(f"Aviso: Nenhuma aresta inter para '{key}'.")

###############################################################################
# Salvamento do Grafo Heterogêneo
###############################################################################
torch.save(data, hetero_graph_path)
print(f"\nGrafo heterogêneo criado e salvo em '{hetero_graph_path}'")

import torch
from torch.serialization import safe_globals
from torch_geometric.data.data import DataEdgeAttr

# Use o contexto safe_globals para permitir a classe DataEdgeAttr
with safe_globals([DataEdgeAttr]):
    data = torch.load("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/graph_data.pt", weights_only=False)

# Exibe um resumo do grafo carregado
print("Grafo PyG:")
print(data)
print("Número de nós:", data.num_nodes)
print("Número de arestas:", data.num_edges)
print("Dimensão das features (x):", data.x.shape)
print("Chaves do objeto Data:", data.keys)

# -*- coding: utf-8 -*-
"""CURSOR_corrigido.py

Versão ajustada do pipeline para corrigir erros no cálculo dos centróides, no processamento de camadas derivadas
e na criação do grafo heterogêneo.
"""

import os
import geopandas as gpd
import pandas as pd
import numpy as np
import torch
import time
from sklearn.neighbors import NearestNeighbors
from torch.serialization import safe_globals
from torch_geometric.data import HeteroData

# ===============================
# Função para garantir que o diretório exista
# ===============================
def ensure_dir(filepath):
    directory = os.path.dirname(filepath)
    if not os.path.exists(directory):
        os.makedirs(directory)

# ===============================
# Função utilitária para reprojeção
# ===============================
def reproject_for_join(gdf, target_epsg=3857):
    if gdf.crs is None:
        raise ValueError("GeoDataFrame não possui CRS definido.")
    if gdf.crs.is_geographic:
        return gdf.to_crs(epsg=target_epsg)
    return gdf

# ===============================
# Funções de extração de features por camada
# ===============================
def extrair_features_hidrografia(gdf):
    gdf['slope_norm'] = gdf['elevation_norm'] / (gdf['cotrecho_norm'] + 1e-6)
    gdf['flow_potential'] = gdf['nustrahler_norm'] * (1 - gdf['elevation_norm'])
    gdf['is_stream'] = (gdf['waterway'].str.lower() == 'stream').astype(int)
    gdf['is_river'] = (gdf['waterway'].str.lower() == 'river').astype(int)
    gdf['is_canal'] = (gdf['waterway'].str.lower() == 'canal').astype(int)
    scale_factor = 1000.0
    gdf['elev_deviation'] = gdf['elevation'] - (gdf['elevation_norm'] * scale_factor)
    gdf['length_to_level_ratio'] = gdf['cotrecho_norm'] / (gdf['nunivotto_norm'] + 1e-6)
    return gdf

def extrair_features_curvas(gdf):
    mean_elev = gdf['elevation'].mean()
    gdf['diff_elev_mean'] = gdf['elevation'] - mean_elev
    gdf['ratio_elev_mean'] = gdf['elevation'] / (mean_elev + 1e-6)
    return gdf

def extrair_features_setores(gdf):
    gdf['area_perimetro_ratio'] = gdf['area_km2'] / (gdf['perimetro_km'] + 1e-6)
    gdf['pop_density_calc'] = gdf['est_populacao'] / (gdf['area_km2'] + 1e-6)
    return gdf

def extrair_features_uso_terra(gdf):
    gdf['trend_uso'] = gdf['USO2018_norm'] - gdf['USO2000_norm']
    cols_uso = ['USO2000_norm', 'USO2010_norm', 'USO2012_norm', 'USO2014_norm', 'USO2016_norm', 'USO2018_norm']
    gdf['media_uso_norm'] = gdf[cols_uso].mean(axis=1)
    return gdf

def extrair_features_edificacoes(gdf):
    gdf['total_alunos_norm'] = gdf['alunos_manha_norm'] + gdf['alunos_tarde_noite_norm']
    gdf['ratio_alunos'] = gdf['alunos_manha_norm'] / (gdf['alunos_tarde_noite_norm'] + 1e-6)
    return gdf

def extrair_features_edificacoes_prioritarias(gdf):
    mapping = {"educational": 1, "public": 2, "health": 3}
    gdf['building_type_code'] = gdf['building_type'].map(mapping).fillna(0)
    return gdf

def extrair_features_setores_edificacoes(gdf):
    gdf['area_perimetro_ratio'] = gdf['area_km2'] / (gdf['perimetro_km'] + 1e-6)
    return gdf

def drop_index_right(gdf):
    if "index_right" in gdf.columns:
        gdf = gdf.drop(columns=["index_right"])
    return gdf

def join_edificacoes_setores(edif_gdf, setores_gdf):
    edif_gdf = drop_index_right(edif_gdf)
    setores_gdf = drop_index_right(setores_gdf)
    edif_proj = reproject_for_join(edif_gdf)
    setores_proj = reproject_for_join(setores_gdf)
    joined = gpd.sjoin_nearest(edif_proj, setores_proj, how="left", distance_col="dist_setor")
    return joined

def join_with_priority(edif_gdf, prior_gdf):
    edif_gdf = drop_index_right(edif_gdf)
    prior_gdf = drop_index_right(prior_gdf)
    edif_proj = reproject_for_join(edif_gdf)
    prior_proj = reproject_for_join(prior_gdf)
    joined = gpd.sjoin_nearest(edif_proj, prior_proj[['priority_norm', 'elevation_norm', 'geometry']],
                               how="left", distance_col="dist_priority", lsuffix="_edif", rsuffix="_prior")
    joined['is_prioritaria'] = joined['dist_priority'] < 10
    return joined

def extrair_features_erbs_consolidada(gdf):
    gdf['ratio_densidade_potencia'] = gdf['densidade_media_norm'] / (gdf['potencia_media_norm'] + 1e-6)
    def map_vulnerabilidade(text):
        text = str(text).lower()
        if "ideal" in text:
            return 1
        elif "sem cobertura" in text:
            return -1
        else:
            return 0
    gdf['vulnerabilidade_encoded'] = gdf['vulnerabilidade'].apply(map_vulnerabilidade)
    return gdf

def extrair_features_erbs_merged(gdf):
    gdf['diff_freq'] = gdf['FreqTxMHz_norm'] - gdf['FreqRxMHz_norm']
    gdf['ratio_freq'] = gdf['FreqTxMHz_norm'] / (gdf['FreqRxMHz_norm'] + 1e-6)
    gdf['indice_tecnico'] = (gdf['GanhoAntena_norm'] + gdf['PotenciaTransmissorWatts_norm']) / 2
    return gdf

def join_erbs_layers(gdf_cons, gdf_merged):
    gdf_cons = drop_index_right(gdf_cons)
    gdf_merged = drop_index_right(gdf_merged)
    cons_proj = reproject_for_join(gdf_cons)
    merged_proj = reproject_for_join(gdf_merged)
    gdf_join = gpd.sjoin_nearest(cons_proj, merged_proj, how="left", distance_col="dist_merged")
    return gdf_join

def join_with_voronoi(gdf_erbs, gdf_voronoi):
    gdf_erbs = drop_index_right(gdf_erbs)
    gdf_voronoi = drop_index_right(gdf_voronoi)
    erbs_proj = reproject_for_join(gdf_erbs)
    voronoi_proj = reproject_for_join(gdf_voronoi)
    cols_voronoi = ['area_km2_norm', 'perimetro_km_norm', 'compacidade_norm', 'geometry']
    gdf_joined = gpd.sjoin_nearest(erbs_proj, voronoi_proj[cols_voronoi], how="left", distance_col="dist_voronoi")
    return gdf_joined

def extrair_features_rodovias(gdf):
    gdf['delta_elev_norm'] = gdf['elevation_max_norm'] - gdf['elevation_min_norm']
    gdf['ratio_elev_mean'] = gdf['elevation_mean_norm'] / (gdf['elevation_norm'] + 1e-6)
    gdf['ratio_comprimento_importancia'] = gdf['comprimento_km_norm'] / (gdf['importancia_via_norm'] + 1e-6)
    return gdf

def extrair_features_ferrovias(gdf):
    gdf['log_elevation_norm'] = np.log(gdf['elevation_norm'] + 1e-6)
    return gdf

def extrair_features_pontos(gdf):
    gdf['dif_elevacao_norm'] = gdf['z_norm'] - gdf['elevacao_base_norm']
    gdf['ratio_elevacao'] = gdf['z_norm'] / (gdf['elevacao_base_norm'] + 1e-6)
    return gdf

def extrair_features_arestas(gdf_arestas, gdf_pontos):
    cols_ponto = ['ponto_id',
                  'TEMPERATURA DO PONTO DE ORVALHO (°C)_norm',
                  'VENTO, RAJADA MAXIMA (m/s)_norm',
                  'precipitacao_norm',
                  'RADIACAO GLOBAL (Kj/m²)_norm']
    gdf_origin = gdf_pontos[cols_ponto].rename(columns={
        'ponto_id': 'from_id',
        'TEMPERATURA DO PONTO DE ORVALHO (°C)_norm': 'temp_orig',
        'VENTO, RAJADA MAXIMA (m/s)_norm': 'vento_orig',
        'precipitacao_norm': 'precipitacao_orig',
        'RADIACAO GLOBAL (Kj/m²)_norm': 'radiacao_orig'
    })
    gdf_arestas = gdf_arestas.merge(gdf_origin, on='from_id', how='left')
    gdf_dest = gdf_pontos[cols_ponto].rename(columns={
        'ponto_id': 'to_id',
        'TEMPERATURA DO PONTO DE ORVALHO (°C)_norm': 'temp_dest',
        'VENTO, RAJADA MAXIMA (m/s)_norm': 'vento_dest',
        'precipitacao_norm': 'precipitacao_dest',
        'RADIACAO GLOBAL (Kj/m²)_norm': 'radiacao_dest'
    })
    gdf_arestas = gdf_arestas.merge(gdf_dest, on='to_id', how='left')
    gdf_arestas['delta_temp'] = gdf_arestas['temp_dest'] - gdf_arestas['temp_orig']
    gdf_arestas['delta_vento'] = gdf_arestas['vento_dest'] - gdf_arestas['vento_orig']
    gdf_arestas['delta_precipitacao'] = gdf_arestas['precipitacao_dest'] - gdf_arestas['precipitacao_orig']
    gdf_arestas['delta_radiacao'] = gdf_arestas['radiacao_dest'] - gdf_arestas['radiacao_orig']
    gdf_arestas['rel_delta_temp'] = gdf_arestas['delta_temp'] / (gdf_arestas['temp_orig'] + 1e-6)
    gdf_arestas['rel_delta_vento'] = gdf_arestas['delta_vento'] / (gdf_arestas['vento_orig'] + 1e-6)
    return gdf_arestas

# ===============================
# Funções genéricas de salvamento
# ===============================
def salvar_dados(gdf, caminho):
    ensure_dir(caminho)
    gdf.drop(columns="geometry", errors="ignore").to_csv(caminho, index=False)
    print(f"Arquivo CSV salvo em: {caminho}")

def salvar_features_pytorch(gdf, caminho):
    ensure_dir(caminho)
    df_numeric = gdf.select_dtypes(include=[np.number])
    tensor = torch.tensor(df_numeric.to_numpy(), dtype=torch.float32)
    torch.save(tensor, caminho)
    print(f"Features salvos em formato PyTorch em: {caminho}")

# ===============================
# Pipeline de extração de features (primeira etapa)
# ===============================
def main_extraicao():
    gpkg_path = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/mba_preparado.gpkg"
    base_csv = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/feature_extraction"
    base_pt = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho/feature_extraction"
    consolidated_dfs = []
    csv_data = {}

    def add_to_csv_data(gdf):
        layer = gdf["layer"].iloc[0]
        records = gdf.drop(columns="geometry", errors="ignore").to_dict(orient="records")
        csv_data.setdefault(layer, []).extend(records)

    print("Processando camada 'hidrografia'...")
    gdf = gpd.read_file(gpkg_path, layer="hidrografia")
    gdf = extrair_features_hidrografia(gdf)
    gdf["layer"] = "hidrografia"
    salvar_dados(gdf, f"{base_csv}_hidrografia.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_hidrografia.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'curvas_nivel_com_elevacao'...")
    gdf = gpd.read_file(gpkg_path, layer="curvas_nivel_com_elevacao")
    gdf = extrair_features_curvas(gdf)
    gdf["layer"] = "curvas_nivel_com_elevacao"
    salvar_dados(gdf, f"{base_csv}_curvas_nivel_com_elevacao.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_curvas_nivel_com_elevacao.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'setores_censitarios'...")
    gdf = gpd.read_file(gpkg_path, layer="setores_censitarios")
    gdf = extrair_features_setores(gdf)
    gdf["layer"] = "setores_censitarios"
    salvar_dados(gdf, f"{base_csv}_setores_censitarios.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_setores_censitarios.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'uso_terra_ocupacao'...")
    gdf = gpd.read_file(gpkg_path, layer="uso_terra_ocupacao")
    gdf = extrair_features_uso_terra(gdf)
    gdf["layer"] = "uso_terra_ocupacao"
    salvar_dados(gdf, f"{base_csv}_uso_terra_ocupacao.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_uso_terra_ocupacao.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'edificacoes'...")
    gdf = gpd.read_file(gpkg_path, layer="edificacoes")
    gdf = extrair_features_edificacoes(gdf)
    gdf["layer"] = "edificacoes"
    salvar_dados(gdf, f"{base_csv}_edificacoes.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_edificacoes.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'setores_com_edificacoes'...")
    gdf = gpd.read_file(gpkg_path, layer="setores_com_edificacoes")
    gdf = extrair_features_setores_edificacoes(gdf)
    gdf["layer"] = "setores_com_edificacoes"
    salvar_dados(gdf, f"{base_csv}_setores_com_edificacoes.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_setores_com_edificacoes.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'edificacoes_prioritarias'...")
    gdf = gpd.read_file(gpkg_path, layer="edificacoes_prioritarias")
    gdf = extrair_features_edificacoes_prioritarias(gdf)
    gdf["layer"] = "edificacoes_prioritarias"
    salvar_dados(gdf, f"{base_csv}_edificacoes_prioritarias.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_edificacoes_prioritarias.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Realizando junção entre 'edificacoes' e 'setores_com_edificacoes'...")
    gdf_joined = join_edificacoes_setores(consolidated_dfs[-4], consolidated_dfs[-3])
    print("Realizando junção para identificar edificações prioritárias...")
    gdf_enriquecidas = join_with_priority(gdf_joined, consolidated_dfs[-1])
    gdf_enriquecidas["layer"] = "edificacoes_enriquecidas"
    salvar_dados(gdf_enriquecidas, f"{base_csv}_edificacoes_enriquecidas.csv")
    salvar_features_pytorch(gdf_enriquecidas, f"{base_pt}_edificacoes_enriquecidas.pt")
    consolidated_dfs.append(gdf_enriquecidas)
    add_to_csv_data(gdf_enriquecidas)

    print("Processando ERBs...")
    # Aqui a camada real para ERBs é 'analise_consolidada'
    gdf_cons = gpd.read_file(gpkg_path, layer="analise_consolidada")
    gdf_merged = gpd.read_file(gpkg_path, layer="merged_layer")
    gdf_voronoi = gpd.read_file(gpkg_path, layer="voronoi")
    gdf_cons = extrair_features_erbs_consolidada(gdf_cons)
    gdf_merged = extrair_features_erbs_merged(gdf_merged)
    gdf_erbs = join_erbs_layers(gdf_cons, gdf_merged)
    gdf_erbs = join_with_voronoi(gdf_erbs, gdf_voronoi)
    gdf_erbs["layer"] = "ERBs"
    salvar_dados(gdf_erbs, f"{base_csv}_ERBs.csv")
    salvar_features_pytorch(gdf_erbs, f"{base_pt}_ERBs.pt")
    consolidated_dfs.append(gdf_erbs)
    add_to_csv_data(gdf_erbs)

    print("Processando camada 'rodovias'...")
    gdf = gpd.read_file(gpkg_path, layer="rodovias")
    gdf = extrair_features_rodovias(gdf)
    gdf["layer"] = "rodovias"
    salvar_dados(gdf, f"{base_csv}_rodovias.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_rodovias.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'ferrovias'...")
    gdf = gpd.read_file(gpkg_path, layer="ferrovias")
    gdf = extrair_features_ferrovias(gdf)
    gdf["layer"] = "ferrovias"
    salvar_dados(gdf, f"{base_csv}_ferrovias.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_ferrovias.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'clima_pontos_grade'...")
    gdf = gpd.read_file(gpkg_path, layer="clima_pontos_grade")
    gdf = extrair_features_pontos(gdf)
    gdf["layer"] = "clima_pontos_grade"
    salvar_dados(gdf, f"{base_csv}_clima_pontos_grade.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_clima_pontos_grade.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Processando camada 'clima_arestas_grade'...")
    gdf = gpd.read_file(gpkg_path, layer="clima_arestas_grade")
    gdf = extrair_features_arestas(gdf, consolidated_dfs[-1])
    gdf["layer"] = "clima_arestas_grade"
    salvar_dados(gdf, f"{base_csv}_clima_arestas_grade.csv")
    salvar_features_pytorch(gdf, f"{base_pt}_clima_arestas_grade.pt")
    consolidated_dfs.append(gdf)
    add_to_csv_data(gdf)

    print("Realizando consolidação de todas as camadas...")
    consolidated_df = pd.concat([df.drop(columns="geometry", errors="ignore") for df in consolidated_dfs],
                                ignore_index=True, sort=False)
    df_numeric = consolidated_df.select_dtypes(include=[np.number])
    node_features = torch.tensor(df_numeric.to_numpy(), dtype=torch.float32)
    mapping = {}
    start = 0
    for nt in sorted(csv_data.keys()):
        count = len(csv_data[nt])
        mapping[nt] = (start, start + count)
        start += count
    total_nodes = start
    print(f"Total de nós (conforme CSV): {total_nodes}")
    print("Mapping dos nós por camada:")
    print(mapping)
    output = {
        "csv": csv_data,
        "node_features": node_features,
        "node_mapping": mapping
    }
    output_path = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "feature_extraction_dict.pt")
    torch.save(output, output_path)
    print(f"\nDicionário de extração de features salvo em: {output_path}")

if __name__ == "__main__":
    main_extraicao()

# ===============================
# Segunda etapa: Construção do grafo (cálculo de centróides, arestas, etc.)
# ===============================
def calculate_centroids(gdf, target_crs):
    if gdf.empty or "geometry" not in gdf.columns:
        return np.empty((0, 2)), np.array([], dtype=int)
    original_indices = gdf.index.to_numpy()
    valid_mask = ~gdf.geometry.isna() & ~gdf.geometry.is_empty
    gdf_valid = gdf[valid_mask].copy()
    if gdf_valid.empty:
        return np.empty((0, 2)), np.array([], dtype=int)
    try:
        gdf_proj = gdf_valid.to_crs(target_crs)
        centroids = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf_proj.geometry])
        valid_indices = original_indices[valid_mask]
        return centroids, valid_indices
    except Exception as e:
        print(f"  Warning: Falha ao reprojetar/calcular centróides: {e}")
        return np.empty((0, 2)), np.array([], dtype=int)

def load_feature_dict(feature_dict_path):
    try:
        feat_dict = torch.load(feature_dict_path)
        node_features = feat_dict["node_features"]
        raw_mapping = feat_dict["node_mapping"]
        node_mapping = {}
        for layer, tup in raw_mapping.items():
            if isinstance(tup, dict) and "inicio" in tup and "fim" in tup:
                node_mapping[layer] = tup
            else:
                node_mapping[layer] = {"inicio": tup[0], "fim": tup[1]}
        num_total_nodes = node_features.shape[0]
        print(f"Carregado dicionário de features com {num_total_nodes} nós no total.")
        return node_features, node_mapping
    except FileNotFoundError:
        print(f"Erro: Dicionário de extração não encontrado em {feature_dict_path}")
        exit()
    except Exception as e:
        print(f"Erro ao carregar o dicionário de extração: {e}")
        exit()

def process_layer(layer_name, mapping_info):
    print(f"\nProcessando camada: '{layer_name}'")
    start_idx = mapping_info["inicio"]
    end_idx = mapping_info["fim"]
    num_expected = end_idx - start_idx
    alias_mapping = {
        "ERBs": "analise_consolidada",
        "edificacoes_enriquecidas": None
    }
    if layer_name in alias_mapping:
        if alias_mapping[layer_name] is None:
            print(f"  Aviso: Não há camada correspondente para '{layer_name}' no arquivo. Pulando processamento.")
            return {"centroids": np.empty((0,2)), "num_valid": 0, "local_to_global": {}, "global_to_centroid_idx": {}, "valid_global_indices": []}
        else:
            actual_layer = alias_mapping[layer_name]
    else:
        actual_layer = layer_name
    try:
        gdf = gpd.read_file(GPKG_PATH, layer=actual_layer)
        if len(gdf) != num_expected:
            print(f"  Aviso: Camada '{layer_name}' possui {len(gdf)} registros, mas o mapeamento espera {num_expected}.")
            num_expected = len(gdf)
        centroids, valid_local_indices = calculate_centroids(gdf, CRS_PROJECTED)
        num_valid = centroids.shape[0]
        print(f"  Encontrados {num_valid} geometria(s) válida(s) na camada '{layer_name}'.")
        local_to_global = {int(local_idx): start_idx + int(local_idx) for local_idx in valid_local_indices}
        global_to_centroid_idx = {start_idx + int(local_idx): i for i, local_idx in enumerate(valid_local_indices)}
        valid_global_indices = [local_to_global[int(idx)] for idx in valid_local_indices]
        return {
            "centroids": centroids,
            "num_valid": num_valid,
            "local_to_global": local_to_global,
            "global_to_centroid_idx": global_to_centroid_idx,
            "valid_global_indices": valid_global_indices
        }
    except Exception as e:
        print(f"  Erro ao processar camada '{layer_name}': {e}")
        return {"centroids": np.empty((0, 2)), "num_valid": 0, "local_to_global": {}, "global_to_centroid_idx": {}, "valid_global_indices": []}

# Parâmetros e caminhos
GPKG_PATH = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "mba_preparado.gpkg")
CRS_PROJECTED = "EPSG:3857"
K_INTRA = 5
K_INTER = 1
ADD_SELF_LOOPS = False
MAKE_UNDIRECTED = True
OUTPUT_NODE_FEATURES_PATH = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "final_node_features.pt")
OUTPUT_EDGE_INDEX_PATH = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "final_edge_index.pt")
OUTPUT_EDGE_WEIGHTS_PATH = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "final_edge_weights.pt")
OUTPUT_NODE_MAPPING_PATH = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "final_node_mapping.pt")
FEATURE_DICT_PATH = os.path.join("/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho", "feature_extraction_dict.pt")

def build_intra_edges(layer_name, layer_data, k_intra, make_undirected):
    edges = []
    weights = []
    centroids = layer_data["centroids"]
    num_valid = layer_data["num_valid"]
    valid_global_indices = layer_data["valid_global_indices"]
    if num_valid <= 1:
        print(f"  Camada '{layer_name}': Menos de 2 nós válidos, pulando intra-KNN.")
        return edges, weights
    actual_k = min(k_intra, num_valid - 1)
    if actual_k <= 0:
        print(f"  Camada '{layer_name}': k_intra ajustado para 0, pulando.")
        return edges, weights
    try:
        nbrs = NearestNeighbors(n_neighbors=actual_k + 1, algorithm='ball_tree').fit(centroids)
        distances, indices = nbrs.kneighbors(centroids)
        count_edges = 0
        for i in range(num_valid):
            source = valid_global_indices[i]
            for n in range(1, actual_k + 1):
                j = indices[i, n]
                target = valid_global_indices[j]
                dist = distances[i, n]
                edges.append((source, target))
                weights.append(dist)
                count_edges += 1
                if make_undirected:
                    edges.append((target, source))
                    weights.append(dist)
                    count_edges += 1
        print(f"  Camada '{layer_name}': Adicionadas {count_edges} arestas intra-camada.")
    except Exception as e:
        print(f"  Erro no KNN intra-camada para '{layer_name}': {e}")
    return edges, weights

def build_inter_edges(layerA_name, dataA, layerB_name, dataB, k_inter, make_undirected):
    edges = []
    weights = []
    if dataA["num_valid"] == 0 or dataB["num_valid"] == 0:
        print(f"  Pulando KNN intercamada entre '{layerA_name}' e '{layerB_name}' (nó(s) inválido(s)).")
        return edges, weights
    valid_global_A = dataA["valid_global_indices"]
    valid_global_B = dataB["valid_global_indices"]
    centroids_A = dataA["centroids"]
    centroids_B = dataB["centroids"]
    actual_k = min(k_inter, dataB["num_valid"])
    if actual_k <= 0:
        print(f"  Pulando KNN de '{layerA_name}' para '{layerB_name}' (k_inter = 0).")
        return edges, weights
    try:
        nbrs_B = NearestNeighbors(n_neighbors=actual_k, algorithm='ball_tree').fit(centroids_B)
        distances_AB, indices_AB = nbrs_B.kneighbors(centroids_A)
        count_edges = 0
        for i in range(dataA["num_valid"]):
            source = valid_global_A[i]
            for n in range(actual_k):
                j = indices_AB[i, n]
                target = valid_global_B[j]
                dist = distances_AB[i, n]
                edges.append((source, target))
                weights.append(dist)
                count_edges += 1
                if make_undirected:
                    edges.append((target, source))
                    weights.append(dist)
                    count_edges += 1
        print(f"  Arestas inter-camada '{layerA_name}' <-> '{layerB_name}': {count_edges} arestas adicionadas.")
    except Exception as e:
        print(f"  Erro no KNN inter-camada de '{layerA_name}' para '{layerB_name}': {e}")
    return edges, weights

def main_grafo():
    start_time = time.time()
    node_features, node_mapping = load_feature_dict(FEATURE_DICT_PATH)
    num_total_nodes = node_features.shape[0]
    torch.save(node_features, OUTPUT_NODE_FEATURES_PATH)
    torch.save(node_mapping, OUTPUT_NODE_MAPPING_PATH)
    print(f"Nós e mapeamento salvos em:\n  {OUTPUT_NODE_FEATURES_PATH}\n  {OUTPUT_NODE_MAPPING_PATH}")
    layers_data = {}
    for layer_name, mapping_tuple in node_mapping.items():
        mapping_info = {"inicio": mapping_tuple["inicio"], "fim": mapping_tuple["fim"]}
        layer_data = process_layer(layer_name, mapping_info)
        layers_data[layer_name] = layer_data
    print(f"\nCálculo dos centróides finalizado em {time.time() - start_time:.2f} segundos.")
    all_edges = []
    all_weights = []
    print("\nConstruindo arestas intra-camada...")
    for layer_name, data in layers_data.items():
        edges, weights = build_intra_edges(layer_name, data, K_INTRA, MAKE_UNDIRECTED)
        all_edges.extend(edges)
        all_weights.extend(weights)
    print("\nConstruindo arestas inter-camada...")
    layer_names = list(layers_data.keys())
    for i in range(len(layer_names)):
        for j in range(i + 1, len(layer_names)):
            layerA = layer_names[i]
            layerB = layer_names[j]
            edges, weights = build_inter_edges(layerA, layers_data[layerA], layerB, layers_data[layerB], K_INTER, MAKE_UNDIRECTED)
            all_edges.extend(edges)
            all_weights.extend(weights)
    print(f"\nConstrução das arestas concluída em {time.time() - start_time:.2f} segundos.")
    if ADD_SELF_LOOPS:
        print(f"\nAdicionando self-loops para todos os {num_total_nodes} nós...")
        for i in range(num_total_nodes):
            all_edges.append((i, i))
            all_weights.append(0.0)
    num_edges_before = len(all_edges)
    if num_edges_before > 0:
        edge_set = set(all_edges)
        unique_edges_map = {edge: i for i, edge in enumerate(all_edges) if edge in edge_set}
        sorted_unique_edges = sorted(edge_set, key=lambda edge: unique_edges_map[edge])
        unique_weights = [all_weights[unique_edges_map[edge]] for edge in sorted_unique_edges]
        all_edges = sorted_unique_edges
        all_weights = unique_weights
        print(f"Removidas {num_edges_before - len(all_edges)} arestas duplicadas.")
    if not all_edges:
        print("Aviso: Nenhuma aresta foi criada, salvando edge_index vazio.")
        edge_index = torch.empty((2, 0), dtype=torch.long)
        edge_weights = torch.empty((0,), dtype=torch.float)
    else:
        edge_index = torch.tensor(all_edges, dtype=torch.long).t().contiguous()
        edge_weights = torch.tensor(all_weights, dtype=torch.float)
        print(f"edge_index: shape {edge_index.shape}")
        print(f"edge_weights: shape {edge_weights.shape}")
    torch.save(edge_index, OUTPUT_EDGE_INDEX_PATH)
    torch.save(edge_weights, OUTPUT_EDGE_WEIGHTS_PATH)
    print(f"\nArestas salvas em:\n  {OUTPUT_EDGE_INDEX_PATH}\n  {OUTPUT_EDGE_WEIGHTS_PATH}")
    print(f"\nConstrução do grafo concluída em {time.time() - start_time:.2f} segundos.")

if __name__ == "__main__":
    main_grafo()

# ===============================
# Terceira etapa: Criação do Grafo Heterogêneo
# ===============================
# Nesta etapa, utilizamos os arquivos gerados na etapa anterior.
base_path = "/content/drive/MyDrive/GrafosGeoespaciais/MBA/grapho"
node_features_path = os.path.join(base_path, "final_node_features.pt")
edge_index_path = os.path.join(base_path, "final_edge_index.pt")
edge_weights_path = os.path.join(base_path, "final_edge_weights.pt")
feature_extraction_path = os.path.join(base_path, "feature_extraction_dict.pt")
hetero_graph_path = os.path.join(base_path, "hetero_graph_data.pt")

print("\nCriando grafo heterogêneo...")
node_features = torch.load(node_features_path)
edge_index = torch.load(edge_index_path)
edge_weights = torch.load(edge_weights_path)
feature_extraction = torch.load(feature_extraction_path)
if "csv" not in feature_extraction:
    raise ValueError("Estrutura inesperada em feature_extraction.pt. Esperava-se a chave 'csv'.")
csv_data = feature_extraction["csv"]
node_types = sorted(csv_data.keys())
mapping = {}
start = 0
for nt in node_types:
    count = len(csv_data[nt])
    mapping[nt] = (start, start + count)
    start += count
total_nodes = start
print(f"Total de nós (conforme CSV): {total_nodes}")
print("Mapping dos nós por camada:")
print(mapping)
data = HeteroData()
for nt in node_types:
    s, e = mapping[nt]
    data[nt].x = node_features[s:e]
    print(f"Camada '{nt}': {data[nt].x.shape[0]} nós com {data[nt].x.shape[1]} features.")
data.feature_extraction = feature_extraction
data.node_mapping = mapping
# Aqui, atribuiremos as arestas calculadas de forma homogênea.
data["homogeneous"].edge_index = edge_index
data["homogeneous"].edge_weights = edge_weights
torch.save(data, hetero_graph_path)
print(f"\nGrafo heterogêneo criado e salvo em '{hetero_graph_path}'")

# prompt: Invalid Notebook
# There was an error rendering your Notebook: the 'state' key is missing from 'metadata.widgets'. Add 'state' to each, or remove 'metadata.widgets'.
{
"metadata": {
"kernelspec": { ... },
"language_info": { ... },
"widgets": { "state": {} } # Added missing 'state' key under 'widgets'
},
"nbformat": 4,
"nbformat_minor": 4,
"cells": [...]
}
{
"metadata": {
"kernelspec": { ... },
"language_info": { ... }
},
"nbformat": 4,
"nbformat_minor": 4,
"cells": [...]
}
{
"nbformat_minor": 4,
"cells": [...]
}