# -*- coding: utf-8 -*-
"""ufabc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gq3LWdJcqePXMl8pprYRLVsInOV2-rL9
"""

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""Teoria para um Workflow Prático de Grafos Geoespaciais no Colab
Compreendo que você precisa de uma estrutura teórica para guiar a implementação no Google Colab, não os códigos detalhados. Vou desenvolver um framework conceitual que considere os desafios práticos, a natureza iterativa do desenvolvimento e as limitações do ambiente Colab.
Fundamentos Teóricos do Workflow
Um workflow eficaz para implementação de grafos geoespaciais multicamadas no Colab deve ser fundamentado em princípios de desenvolvimento científico-computacional adaptativo. Este paradigma reconhece que o desenvolvimento real não é sequencial perfeito, mas iterativo e experimental.
Princípios Orientadores

Modularidade com Acoplamento Fraco: Cada componente funcional deve operar independentemente, permitindo testes, ajustes e substituições sem comprometer todo o sistema.
Persistência Estratégica: O ambiente Colab é volátil por natureza. Estratégias de persistência devem ser incorporadas não apenas no final, mas em pontos críticos do processamento.
Validação Progressiva: Cada etapa precisa de mecanismos de validação para confirmar integridade, qualidade e consistência antes de proceder.
Adaptabilidade Computacional: O workflow deve se ajustar dinamicamente às restrições de recursos (memória, processamento, tempo de sessão) do Colab.
Visualização Intermediária: Resultados parciais devem ser visualizáveis para informar decisões de ajuste em etapas subsequentes.

Arquitetura Conceitual do Workflow
1. Camada de Preparação e Configuração
Esta camada estabelece o ambiente e as condições iniciais para execução:

Configuração de Ambiente: Define parâmetros de execução, bibliotecas necessárias e alocação de recursos.
Estratégia de Persistência: Estabelece convenções para armazenamento e recuperação de dados entre sessões.
Parametrização Dinâmica: Permite a modificação de parâmetros sem reestruturação complexa.

2. Camada de Gestão de Dados
Responsável pelo fluxo de dados através do sistema:

Fontes de Dados: Define interfaces para aquisição de dados geoespaciais e metadados.
Transformações Incrementais: Processa dados em estágios reversíveis e verificáveis.
Cache Inteligente: Armazena resultados intermediários com políticas de invalidação claras.

3. Camada de Construção e Análise de Grafos
Implementa a lógica central de grafos geoespaciais:

Construção Incremental: Permite a construção de subgrafos e fusão posterior.
Validação Topológica: Verifica a integridade estrutural em pontos críticos.
Análise Distribuída: Divide análises complexas em operações menores gerenciáveis.

4. Camada de Simulação e Modelagem
Executa análises dinâmicas no grafo construído:

Parametrização de Cenários: Define condições para simulações específicas.
Execução Modular: Permite simulações parciais para verificação incremental.
Recuperação de Falhas: Implementa estratégias para continuar simulações interrompidas.

5. Camada de Visualização e Interpretação
Transforma resultados em insights interpretáveis:

Renderização Progressiva: Visualiza resultados em níveis crescentes de detalhe.
Exportação Seletiva: Permite salvar resultados específicos em formatos apropriados.
Documentação Integrada: Associa metadados interpretativos aos resultados.

Fluxo Operacional Teórico
O fluxo não é estritamente linear, mas adapta-se a ciclos iterativos de desenvolvimento:
Fase 1: Estabelecimento (Preparação Contextual)
Nesta fase inicial, o foco está em criar um ambiente estável e bem definido:

Definição do espaço de trabalho e conectividade com armazenamento persistente
Estabelecimento das convenções de parâmetros e configurações
Verificação de recursos disponíveis (memória, GPU, limites do Colab)
Importação e validação das bibliotecas necessárias

Fase 2: Aquisição e Transformação (Tratamento de Dados)
Esta fase aborda o carregamento e preparação dos dados geoespaciais:

Carregamento seletivo de dados geoespaciais (economizando memória)
Transformações espaciais com verificação de integridade
Harmonização de sistemas de coordenadas e projeções
Criação de estruturas intermediárias de dados com persistência

Ponto de Decisão: Avaliar qualidade e completude dos dados antes de prosseguir.
Fase 3: Modelagem Estrutural (Construção de Grafos)
Nesta fase, os dados geoespaciais são convertidos em estruturas de grafos:

Construção modular por camada (terreno, clima, considerações civis)
Validação topológica por camada antes da integração
Estabelecimento de conexões inter-camadas com verificação
Armazenamento de estados intermediários do grafo

Ponto de Decisão: Avaliar propriedades estruturais do grafo antes de análises complexas.
Fase 4: Análise Estática (Propriedades Topológicas)
Esta fase explora as propriedades estáticas do grafo multicamadas:

Cálculo de centralidades com validação de consistência
Detecção de comunidades com interpretação contextual
Identificação de vulnerabilidades estruturais
Criação de índices compostos para análise de risco

Ponto de Decisão: Avaliar a necessidade de refinamento do grafo com base nos resultados.
Fase 5: Análise Dinâmica (Simulações)
Nesta fase, simulações são executadas para modelar fenômenos dinâmicos:

Simulação de difusão com checkpoints incrementais
Análise de rotas com variação de parâmetros
Modelagem de cenários de falha com estratégias de recuperação
Comparação de resultados entre cenários alternativos

Ponto de Decisão: Avaliar a necessidade de ajustes nos parâmetros de simulação.
Fase 6: Síntese e Comunicação (Visualização)
A fase final transforma os resultados em representações comunicáveis:

Visualização seletiva de resultados críticos
Geração de representações espaciais em múltiplas escalas
Exportação de dados para análises posteriores
Documentação de metodologia e resultados

Considerações sobre Resiliência e Adaptação
Gestão de Falhas e Recuperação
Um aspecto crítico frequentemente ignorado em workflows teóricos é a gestão de falhas:

Pontos de Verificação Estratégicos: Identificar onde falhas são mais prováveis ou custosas
Armazenamento de Estados Intermediários: Salvar resultados parciais em momentos críticos
Modularização de Processos Longos: Dividir operações extensas em unidades recuperáveis
Validação Explícita de Pressupostos: Verificar condições necessárias antes de operações críticas

Otimização de Recursos
O Colab impõe restrições que devem ser consideradas no design do workflow:

Compressão Seletiva: Reduzir dimensionalidade de dados quando apropriado
Processamento em Lotes: Dividir operações grandes em conjuntos gerenciáveis
Escalonamento Adaptativo: Ajustar complexidade das operações conforme disponibilidade de recursos
Descarregamento Estratégico: Mover dados processados para armazenamento externo quando não imediatamente necessários

Iteração e Refinamento
O desenvolvimento real é iterativo, não linear:

Ciclos de Feedback: Incorporar aprendizados de execuções anteriores
Parametrização Explícita: Facilitar experimentação com diferentes configurações
Documentação Progressiva: Registrar decisões e observações durante o desenvolvimento
Separação de Configuração e Implementação: Permitir alterações em parâmetros sem modificar o código fundamental

Implementação no Contexto de Sorocaba-SP
Aplicando o framework teórico ao caso específico de Sorocaba-SP e análise QBRN:
Considerações Específicas

Escala e Granularidade: Determinar a resolução espacial apropriada considerando o equilíbrio entre precisão e desempenho.
Confiabilidade dos Dados: Implementar validação específica para dados críticos (ex: infraestruturas do PRO SUB, padrões climáticos).
Relevância Contextual: Priorizar aspectos do modelo diretamente relevantes para análise de vulnerabilidade QBRN.
Validação com Conhecimento de Domínio: Incorporar pontos de verificação que alinham resultados computacionais com conhecimento especializado.

Abordagem de Implementação

Prototipagem Incremental: Desenvolver primeiro uma versão simplificada mas funcional do workflow completo.
Refinamento Focalizado: Aprimorar componentes específicos baseado em resultados preliminares.
Validação Cruzada: Comparar resultados com metodologias alternativas quando possível.
Documentação Contextual: Registrar não apenas o que foi feito, mas por que decisões específicas foram tomadas.

Conclusão: Um Workflow Adaptativo para o Mundo Real
Um workflow teórico para implementação de grafos geoespaciais multicamadas no Colab deve reconhecer a natureza iterativa e às vezes imprevisível do desenvolvimento científico-computacional. Através de uma arquitetura modular, estratégias robustas de persistência, validação constante e adaptabilidade aos recursos disponíveis, o framework proposto fornece uma base conceitual para implementar com sucesso as análises descritas no Capítulo 6, mesmo com as limitações inerentes ao ambiente Colab.
A chave para o sucesso não está em seguir rigidamente um plano predefinido, mas em estabelecer um sistema resiliente que possa evoluir conforme necessário, mantendo a integridade dos dados e a validade das análises ao longo de todo o processo de desenvolvimento.
"""

# Definir o diretório base do projeto
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'

# Criar a estrutura de diretórios
import os

# Lista de diretórios a serem criados
directories = [
    # Diretório principal
    base_dir,

    # Diretórios de dados
    f"{base_dir}/data",
    f"{base_dir}/data/terreno",
    f"{base_dir}/data/clima",
    f"{base_dir}/data/civil",
    f"{base_dir}/data/limites",

    # Diretórios de configuração
    f"{base_dir}/config",

    # Diretórios de saída
    f"{base_dir}/output",
    f"{base_dir}/output/grafos",
    f"{base_dir}/output/analises",
    f"{base_dir}/output/simulacoes",
    f"{base_dir}/output/visualizacoes"
]

# Criar cada diretório
for directory in directories:
    try:
        os.makedirs(directory, exist_ok=True)
        print(f"Diretório criado: {directory}")
    except Exception as e:
        print(f"Erro ao criar {directory}: {e}")

print("\nEstrutura de diretórios criada com sucesso!")

"""Criando Curvas de Nível a partir de Dados TOPODATA
Vejo que você já baixou e descompactou diversos arquivos TOPODATA para a região 23S48, que cobrem a área de Sorocaba. Esses dados são do projeto TOPODATA do INPE, que fornece dados topográficos derivados do SRTM (Shuttle Radar Topography Mission) com resolução refinada para 30m.
Para criar curvas de nível com intervalo de 50m a partir desses dados e armazenar em um arquivo GeoPackage (.gpkg), precisamos realizar estas etapas:

Carregar o modelo digital de elevação (o arquivo 23S48_ZN.tif)
Gerar curvas de nível com intervalo de 50m
Salvar as curvas como GeoPackage

Aqui está como podemos fazer isso usando GDAL e PyGEOS através do GeoPandas:
"""

pip install fiona

!apt-get update
!apt-get install -y gdal-bin
!pip install geopandas contextily matplotlib osmnx

!pip install rasterio

!apt-get update && apt-get install -y osmium-tool
!pip install osmnx contextily

# Importar bibliotecas necessárias
import os
import geopandas as gpd
import numpy as np
from osgeo import gdal
import matplotlib.pyplot as plt
from shapely.geometry import LineString
import rasterio
from rasterio import features

# Definir caminho para o arquivo de elevação (MDE)
dem_path = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/23S48_ZN.tif'

# Definir caminho para o arquivo de saída (GeoPackage)
output_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/curvas_nivel_50m.gpkg'

# Função para gerar curvas de nível a partir de um MDE
def generate_contours(dem_path, interval=50, base=0):
    """
    Gera curvas de nível a partir de um Modelo Digital de Elevação.

    Parâmetros:
    dem_path (str): Caminho para o arquivo raster de elevação.
    interval (int): Intervalo entre as curvas de nível em metros.
    base (int): Valor base para iniciar as curvas.

    Retorna:
    GeoDataFrame: Curvas de nível como geometrias LineString.
    """
    # Abrir o arquivo raster
    with rasterio.open(dem_path) as src:
        # Ler os dados e a transformação
        data = src.read(1)
        transform = src.transform
        crs = src.crs

        # Definir os níveis para as curvas (baseados no intervalo)
        levels = np.arange(
            base + interval * np.floor(np.min(data[data > 0]) / interval),
            np.ceil(np.max(data) / interval) * interval + interval,
            interval
        )

        # Gerar as curvas de nível
        contours = []
        for level in levels:
            # Extrair curvas de nível para o valor específico
            for geometry, value in features.shapes(
                    (data >= level).astype('uint8'),
                    transform=transform,
                    mask=(data >= level)):
                # Converter para LineString se aplicável
                if value == 1:
                    try:
                        contour = LineString(geometry['coordinates'][0])
                        contours.append({'geometry': contour, 'elevation': level})
                    except:
                        # Ignorar geometrias inválidas
                        pass

        # Criar GeoDataFrame
        if contours:
            gdf = gpd.GeoDataFrame(contours, crs=crs)
            return gdf
        else:
            print("Nenhuma curva de nível gerada.")
            return None

# Gerar as curvas de nível
print("Gerando curvas de nível...")
contours_gdf = generate_contours(dem_path, interval=50)

# Verificar se as curvas foram geradas com sucesso
if contours_gdf is not None:
    # Salvar como GeoPackage
    contours_gdf.to_file(output_gpkg, driver='GPKG', layer='curvas_nivel_50m')
    print(f"Curvas de nível salvas em: {output_gpkg}")

    # Plotar para verificação
    fig, ax = plt.subplots(figsize=(12, 10))
    contours_gdf.plot(ax=ax, column='elevation', cmap='terrain', linewidth=0.5)
    ax.set_title('Curvas de Nível (Intervalo: 50m)')
    plt.show()
else:
    print("Não foi possível gerar as curvas de nível.")

import os

caminho = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno'
arquivos = os.listdir(caminho)

print("Arquivos encontrados:")
for arquivo in arquivos:
    print(arquivo)

!pip install contextily

import os
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import contextily as ctx
import numpy as np
from matplotlib.colors import LinearSegmentedColormap
import warnings

# Definir os caminhos
input_dir = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno'
output_dir = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados'
plot_dir = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots'
csv_dir = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/csv'

# Criar as pastas de saída se não existirem
os.makedirs(output_dir, exist_ok=True)
os.makedirs(plot_dir, exist_ok=True)
os.makedirs(csv_dir, exist_ok=True)

# Definir o sistema de referência padrão (SIRGAS 2000)
SIRGAS_2000 = "EPSG:4674"

def filter_geospatial_data():
    # Carregar o polígono de Sorocaba
    try:
        sorocaba_polygon = gpd.read_file(os.path.join(input_dir, 'sorocaba.gpkg'))
        # Reprojetar para SIRGAS 2000 se necessário
        if sorocaba_polygon.crs != SIRGAS_2000:
            sorocaba_polygon = sorocaba_polygon.to_crs(SIRGAS_2000)
        print(f"Polígono de Sorocaba carregado com sucesso e projetado para {sorocaba_polygon.crs}")

        # Exibir informações sobre o polígono de Sorocaba
        print("\nDescrição do polígono de Sorocaba:")
        print(sorocaba_polygon.describe())
        print("\nColunas do polígono de Sorocaba:", sorocaba_polygon.columns.tolist())
    except Exception as e:
        print(f"Erro ao carregar o polígono de Sorocaba: {e}")
        return

    # Identificar todos os arquivos shp e gpkg na pasta
    input_files = []
    for file in os.listdir(input_dir):
        if file.endswith('.shp') or file.endswith('.gpkg'):
            # Ignorar o arquivo sorocaba.gpkg pois é o delimitador
            if file != 'sorocaba.gpkg':
                input_files.append(file)

    print(f"\nEncontrados {len(input_files)} arquivos para processar.")

    # Processar cada arquivo
    for file in input_files:
        try:
            input_path = os.path.join(input_dir, file)
            base_name = os.path.splitext(file)[0]

            # Para arquivos gpkg, processar todas as camadas
            if file.endswith('.gpkg'):
                # Listar camadas disponíveis
                import fiona
                layers = fiona.listlayers(input_path)

                if len(layers) > 1:
                    print(f"\nMúltiplas camadas encontradas em {file}: {layers}")
                    print(f"Processando todas as {len(layers)} camadas...")

                    # Criar um GeoPackage de saída único para todas as camadas
                    output_path = os.path.join(output_dir, f"sorocaba_{base_name}.gpkg")

                    # Processar cada camada
                    for layer_name in layers:
                        process_layer(input_path, layer_name, output_path, base_name, sorocaba_polygon)
                else:
                    # Arquivos com apenas uma camada são processados normalmente
                    gdf = gpd.read_file(input_path)
                    process_single_layer(gdf, file, base_name, sorocaba_polygon)
            else:
                # Para arquivos shapefile
                gdf = gpd.read_file(input_path)
                process_single_layer(gdf, file, base_name, sorocaba_polygon)

        except Exception as e:
            print(f"Erro ao processar {file}: {e}")
            import traceback
            print(traceback.format_exc())

    print("\nProcessamento concluído. Todos os arquivos foram salvos em formato GeoPackage (SIRGAS 2000) e CSV.")

def process_layer(input_path, layer_name, output_path, base_name, sorocaba_polygon):
    """Processa uma camada específica de um arquivo GeoPackage"""
    print(f"  Processando camada: {layer_name}")

    # Carregar camada específica
    try:
        gdf = gpd.read_file(input_path, layer=layer_name)

        # Verificar e mostrar informações sobre a camada original
        print(f"\n  Informações da camada original '{layer_name}':")
        print(f"  - Número de registros: {len(gdf)}")
        print(f"  - Colunas disponíveis: {gdf.columns.tolist()}")
        print(f"  - Tipos de geometria: {gdf.geom_type.unique().tolist()}")

        # Verificar se o CRS está definido
        if gdf.crs is None:
            print(f"  Aviso: Camada {layer_name} não possui CRS definido. Atribuindo SIRGAS 2000 como padrão.")
            gdf.set_crs(SIRGAS_2000, inplace=True)

        # Reprojetar para SIRGAS 2000
        if gdf.crs != SIRGAS_2000:
            print(f"  Reprojetando camada {layer_name} de {gdf.crs} para {SIRGAS_2000}")
            gdf = gdf.to_crs(SIRGAS_2000)

        # Realizar o clip espacial com preservação de atributos
        try:
            clipped_gdf = gpd.clip(gdf, sorocaba_polygon)

            # Verificar se o resultado não está vazio
            if len(clipped_gdf) > 0:
                # Garantir que o resultado tem um CRS definido
                if clipped_gdf.crs is None:
                    clipped_gdf.set_crs(SIRGAS_2000, inplace=True)

                # Verificar se os atributos foram preservados
                print(f"  - Colunas após recorte: {clipped_gdf.columns.tolist()}")

                # Mostrar estatísticas descritivas
                print(f"\n  Estatísticas descritivas da camada '{layer_name}' após recorte:")
                describe_result = clipped_gdf.describe(include='all')
                print(describe_result)

                # Criar um nome simplificado para a camada
                simplified_layer = layer_name.split('.')[-1] if '.' in layer_name else layer_name

                # Salvar camada no GeoPackage
                clipped_gdf.to_file(output_path, driver='GPKG', layer=simplified_layer)
                print(f"  Camada '{simplified_layer}' salva em: {output_path}")

                # Salvar em CSV (sem a geometria)
                csv_path = os.path.join(csv_dir, f"sorocaba_{base_name}_{simplified_layer}.csv")

                # Criar uma cópia sem a coluna de geometria para CSV
                if 'geometry' in clipped_gdf.columns:
                    # Se usar gdf.drop('geometry', axis=1) diretamente com GeoDataFrame, pode perder metadados
                    csv_df = pd.DataFrame(clipped_gdf.drop(columns='geometry'))
                else:
                    csv_df = pd.DataFrame(clipped_gdf)

                # Adicionar coordenadas do centróide como colunas separadas
                try:
                    # Calcular centróide se possível
                    clipped_gdf['centroid_x'] = clipped_gdf.geometry.centroid.x
                    clipped_gdf['centroid_y'] = clipped_gdf.geometry.centroid.y

                    # Adicionar coordenadas à versão CSV
                    csv_df['centroid_x'] = clipped_gdf['centroid_x']
                    csv_df['centroid_y'] = clipped_gdf['centroid_y']
                except:
                    print(f"  Aviso: Não foi possível calcular centróides para a camada {simplified_layer}")

                # Salvar CSV
                csv_df.to_csv(csv_path, index=False)
                print(f"  Dados tabulares salvos em: {csv_path}")

                # Criar visualização para camadas específicas
                if "curvas_nivel" in base_name or "hidrografia" in base_name or "Natureza" in base_name:
                    layer_plot_name = f"{base_name}_{simplified_layer}"
                    plot_geospatial_data(clipped_gdf, layer_plot_name, layer_type=simplified_layer)
            else:
                print(f"  Aviso: O recorte da camada {layer_name} resultou em conjunto de dados vazio, pulando.")
        except Exception as e:
            print(f"  Erro ao realizar recorte espacial da camada {layer_name}: {e}")
    except Exception as e:
        print(f"  Erro ao processar camada {layer_name}: {e}")

def process_single_layer(gdf, filename, base_name, sorocaba_polygon):
    """Processa uma única camada de dados geoespaciais"""
    output_path = os.path.join(output_dir, f"sorocaba_{base_name}.gpkg")

    # Verificar e mostrar informações sobre o arquivo original
    print(f"\nInformações do arquivo original '{filename}':")
    print(f"- Número de registros: {len(gdf)}")
    print(f"- Colunas disponíveis: {gdf.columns.tolist()}")
    print(f"- Tipos de geometria: {gdf.geom_type.unique().tolist()}")

    # Verificar se o CRS está definido
    if gdf.crs is None:
        print(f"Aviso: {filename} não possui CRS definido. Atribuindo SIRGAS 2000 como padrão.")
        gdf.set_crs(SIRGAS_2000, inplace=True)

    # Reprojetar para SIRGAS 2000
    if gdf.crs != SIRGAS_2000:
        print(f"Reprojetando {filename} de {gdf.crs} para {SIRGAS_2000}")
        gdf = gdf.to_crs(SIRGAS_2000)

    # Realizar o clip espacial
    try:
        clipped_gdf = gpd.clip(gdf, sorocaba_polygon)

        # Verificar se o resultado não está vazio
        if len(clipped_gdf) > 0:
            # Garantir que o resultado tem um CRS definido
            if clipped_gdf.crs is None:
                clipped_gdf.set_crs(SIRGAS_2000, inplace=True)

            # Verificar se os atributos foram preservados
            print(f"- Colunas após recorte: {clipped_gdf.columns.tolist()}")

            # Mostrar estatísticas descritivas
            print(f"\nEstatísticas descritivas de '{filename}' após recorte:")
            describe_result = clipped_gdf.describe(include='all')
            print(describe_result)

            # Salvar sempre como GeoPackage
            clipped_gdf.to_file(output_path, driver='GPKG')
            print(f"Arquivo salvo em formato GeoPackage: {output_path}")

            # Salvar em CSV (sem a geometria)
            csv_path = os.path.join(csv_dir, f"sorocaba_{base_name}.csv")

            # Criar uma cópia sem a coluna de geometria para CSV
            if 'geometry' in clipped_gdf.columns:
                csv_df = pd.DataFrame(clipped_gdf.drop(columns='geometry'))
            else:
                csv_df = pd.DataFrame(clipped_gdf)

            # Adicionar coordenadas do centróide como colunas separadas
            try:
                # Calcular centróide se possível
                clipped_gdf['centroid_x'] = clipped_gdf.geometry.centroid.x
                clipped_gdf['centroid_y'] = clipped_gdf.geometry.centroid.y

                # Adicionar coordenadas à versão CSV
                csv_df['centroid_x'] = clipped_gdf['centroid_x']
                csv_df['centroid_y'] = clipped_gdf['centroid_y']
            except:
                print(f"Aviso: Não foi possível calcular centróides para o arquivo {filename}")

            # Salvar CSV
            csv_df.to_csv(csv_path, index=False)
            print(f"Dados tabulares salvos em: {csv_path}")

            # Criar visualização para arquivos específicos
            if "curvas_nivel" in filename or "hidrografia" in filename or "Natureza" in filename:
                plot_geospatial_data(clipped_gdf, base_name)
        else:
            print(f"Aviso: O recorte de {filename} resultou em conjunto de dados vazio, pulando.")
    except Exception as e:
        print(f"Erro ao realizar recorte espacial de {filename}: {e}")
        import traceback
        print(traceback.format_exc())

def plot_geospatial_data(gdf, filename, layer_type=None):
    """
    Cria visualizações dos dados geoespaciais no estilo da imagem de referência
    """
    fig, ax = plt.subplots(figsize=(12, 10))

    # Ajustar título com informação da camada, se disponível
    title_suffix = f" - {layer_type}" if layer_type else ""

    # Ajustar estilo de visualização com base no tipo de dados
    if "curvas_nivel" in filename:
        # Esquema de cores personalizado para curvas de nível
        colors = ['#00FF00', '#ADFF2F', '#FFFF00', '#FFD700', '#DAA520', '#8B4513', '#800000']
        cmap = LinearSegmentedColormap.from_list('elevation_cmap', colors, N=256)

        # Verificar se existe coluna de elevação
        elevation_col = None
        for col in gdf.columns:
            if col.lower() in ["elevation", "elevacao", "altitude", "z", "height"]:
                elevation_col = col
                break

        if elevation_col:
            gdf.plot(column=elevation_col, cmap=cmap, linewidth=0.5, ax=ax)
            ax.set_title(f"Curvas de Nível (Intervalo: 50m){title_suffix}", fontsize=14)
        else:
            # Se não encontrar coluna de elevação, usar uma sequência
            gdf.plot(cmap=cmap, linewidth=0.5, ax=ax)
            ax.set_title(f"Curvas de Nível{title_suffix}", fontsize=14)

    elif "hidrografia" in filename or "trecho_drenagem" in filename or "curso_dagua" in filename:
        gdf.plot(color='blue', linewidth=0.8, ax=ax)
        ax.set_title(f"Hidrografia de Sorocaba{title_suffix}", fontsize=14)

    elif "area_drenagem" in filename:
        gdf.plot(color='lightblue', alpha=0.7, ax=ax)
        ax.set_title(f"Áreas de Drenagem de Sorocaba{title_suffix}", fontsize=14)

    elif "ponto_drenagem" in filename:
        gdf.plot(color='darkblue', markersize=20, ax=ax)
        ax.set_title(f"Pontos de Drenagem de Sorocaba{title_suffix}", fontsize=14)

    elif "linha_costa" in filename:
        gdf.plot(color='navy', linewidth=1.2, ax=ax)
        ax.set_title(f"Linha de Costa de Sorocaba{title_suffix}", fontsize=14)

    else:
        # Para outros tipos de dados
        gdf.plot(cmap='viridis', ax=ax)
        ax.set_title(f"Dados Geoespaciais: {filename}{title_suffix}", fontsize=14)

    # Ajustar os limites do gráfico
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    ax.grid(False)

    # Salvar a figura
    output_path = os.path.join(plot_dir, f"{filename}_plot.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Visualização salva: {output_path}")
    plt.close()

    # Criar uma versão com mapa base para referência
    try:
        fig, ax = plt.subplots(figsize=(12, 10))

        # Suprimir avisos específicos do contextily sobre zoom
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning, message="The inferred zoom level")

            # Converter para EPSG:4326 para usar com contextily
            gdf_wgs84 = gdf.to_crs(epsg=4326)

            # Estilos específicos para diferentes tipos de camadas
            if "curvas_nivel" in filename and elevation_col:
                gdf_wgs84.plot(column=elevation_col, cmap=cmap, linewidth=0.5, ax=ax, alpha=0.7)
            elif "hidrografia" in filename or "trecho_drenagem" in filename or "curso_dagua" in filename:
                gdf_wgs84.plot(color='blue', linewidth=0.8, ax=ax)
            elif "area_drenagem" in filename:
                gdf_wgs84.plot(color='lightblue', alpha=0.7, ax=ax)
            elif "ponto_drenagem" in filename:
                gdf_wgs84.plot(color='darkblue', markersize=20, ax=ax)
            elif "linha_costa" in filename:
                gdf_wgs84.plot(color='navy', linewidth=1.2, ax=ax)
            else:
                gdf_wgs84.plot(cmap='viridis', ax=ax, alpha=0.7)

            # Definir um zoom apropriado
            zoom = 12  # Um valor razoável para visualização municipal
            ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, zoom=zoom)

        # Salvar a figura com mapa base
        output_path = os.path.join(plot_dir, f"{filename}_com_mapa_base.png")
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"Visualização com mapa base salva: {output_path}")
        plt.close()
    except Exception as e:
        print(f"Erro ao adicionar mapa base: {e}")

# Executar a função
if __name__ == "__main__":
    # Instalar dependências adicionais se necessário
    try:
        import contextily
        import fiona
    except ImportError:
        print("Instalando dependências necessárias...")
        !pip install contextily fiona
        import contextily
        import fiona

    filter_geospatial_data()

import os

caminho = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados'
arquivos = os.listdir(caminho)

print("Arquivos encontrados:")
for arquivo in arquivos:
    print(arquivo)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para processamento de dados OSM para o Projeto de Grafos Geoespaciais
Autor: Luis Felipe Comodo Seelig
Data: Março/2025

Este script processa dados OpenStreetMap usando o arquivo local south-america-latest.osm.pbf,
extraindo as camadas relevantes para a região de Sorocaba.
"""

# Instalação de dependências necessárias
!apt-get update
!apt-get install -y osmium-tool gdal-bin
!pip install geopandas contextily matplotlib

# Importar bibliotecas
import os
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import contextily as ctx
import subprocess
import tempfile
import warnings
from shapely.geometry import box
import time

# Suprimir avisos
warnings.filterwarnings('ignore', category=UserWarning)

# Definir diretórios
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
data_dir = os.path.join(base_dir, 'data/terreno')
osm_file = os.path.join(data_dir, 'south-america-latest.osm.pbf')
sorocaba_file = os.path.join(data_dir, 'sorocaba.gpkg')
output_dir = os.path.join(data_dir, 'osm_layers')
csv_dir = os.path.join(data_dir, 'csv')
plot_dir = os.path.join(data_dir, 'plots')

# Criar diretórios se não existirem
for directory in [output_dir, csv_dir, plot_dir]:
    os.makedirs(directory, exist_ok=True)

print(f"Verificando arquivo OSM: {osm_file}")
if not os.path.exists(osm_file):
    raise FileNotFoundError(f"Arquivo OSM não encontrado: {osm_file}")
else:
    print(f"Arquivo OSM encontrado: {os.path.getsize(osm_file) / (1024*1024):.1f} MB")

print(f"Verificando arquivo de limites: {sorocaba_file}")
if not os.path.exists(sorocaba_file):
    raise FileNotFoundError(f"Arquivo de limites não encontrado: {sorocaba_file}")
else:
    print(f"Arquivo de limites encontrado")

# Verificando instalação do osmium
try:
    result = subprocess.run(['osmium', '--version'], capture_output=True, text=True)
    print(f"Osmium instalado: {result.stdout.strip()}")
except FileNotFoundError:
    print("Osmium não encontrado. Instalando novamente...")
    !apt-get update
    !apt-get install -y osmium-tool

    # Verificar novamente
    try:
        result = subprocess.run(['osmium', '--version'], capture_output=True, text=True)
        print(f"Osmium instalado: {result.stdout.strip()}")
    except:
        print("FALHA NA INSTALAÇÃO DO OSMIUM. Isso pode causar problemas no processamento.")

# Carregar o polígono de Sorocaba
sorocaba_gdf = gpd.read_file(sorocaba_file)
print(f"Polígono de Sorocaba carregado: {sorocaba_gdf.crs}")
print(f"Número de geometrias: {len(sorocaba_gdf)}")

# Extrair os limites da área de Sorocaba com buffer
bbox = sorocaba_gdf.total_bounds
buffer = 0.02  # ~2km em graus
buffer_bbox = [
    bbox[0] - buffer,  # minx
    bbox[1] - buffer,  # miny
    bbox[2] + buffer,  # maxx
    bbox[3] + buffer   # maxy
]

print(f"Limites para extração: {buffer_bbox}")

# Definir as camadas OSM a serem extraídas
osm_layers = {
    'roads': {
        'description': 'Sistema viário principal',
        'osm_key': 'highway',
        'osm_values': 'motorway,trunk,primary,secondary,tertiary,residential',
        'layer_type': 'lines',
        'style': {'color': 'red', 'linewidth': 0.8}
    },
    'waterways': {
        'description': 'Hidrografia (rios e cursos d\'água)',
        'osm_key': 'waterway',
        'osm_values': 'river,stream,canal',
        'layer_type': 'lines',
        'style': {'color': 'blue', 'linewidth': 0.8}
    },
    'buildings': {
        'description': 'Edificações',
        'osm_key': 'building',
        'osm_values': None,  # Todos os valores
        'layer_type': 'polygons',
        'style': {'color': 'gray', 'alpha': 0.7}
    },
    'landuse': {
        'description': 'Uso do solo',
        'osm_key': 'landuse',
        'osm_values': None,  # Todos os valores
        'layer_type': 'polygons',
        'style': {'cmap': 'YlOrBr', 'alpha': 0.7}
    },
    'natural': {
        'description': 'Elementos naturais',
        'osm_key': 'natural',
        'osm_values': 'water,forest,wood,wetland',
        'layer_type': 'polygons',
        'style': {'cmap': 'Greens', 'alpha': 0.7}
    },
    'poi': {
        'description': 'Pontos de interesse',
        'osm_key': 'amenity',
        'osm_values': 'school,hospital,university,police,fire_station',
        'layer_type': 'points',
        'style': {'color': 'purple', 'markersize': 30}
    },
    'railway': {
        'description': 'Sistema ferroviário',
        'osm_key': 'railway',
        'osm_values': 'rail,tram,subway',
        'layer_type': 'lines',
        'style': {'color': 'black', 'linewidth': 1.2}
    }
}

# Função para extrair uma área específica do arquivo OSM usando osmium
def extract_area_from_osm(bbox, output_file):
    """Extrai a área de Sorocaba (com buffer) do arquivo OSM principal"""
    bbox_str = f"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}"

    cmd = [
        'osmium', 'extract',
        '--bbox', bbox_str,
        '--strategy', 'complete_ways',
        '--output', output_file,
        osm_file
    ]

    print(f"Extraindo área de interesse...")
    print(f"Comando: {' '.join(cmd)}")

    try:
        # Execução com timeout de 10 minutos (600 segundos)
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)

        if result.returncode != 0:
            print(f"ERRO ao extrair área: {result.stderr}")
            return False

        if os.path.exists(output_file):
            print(f"Área extraída com sucesso: {output_file} ({os.path.getsize(output_file) / (1024*1024):.1f} MB)")
            return True
        else:
            print(f"Arquivo de saída não encontrado após extração")
            return False
    except subprocess.TimeoutExpired:
        print("Extração demorou demais e foi interrompida (timeout após 10 minutos)")
        return False
    except Exception as e:
        print(f"Erro ao executar osmium extract: {str(e)}")
        return False

# Função para extrair uma camada específica usando ogr2ogr
def extract_layer_from_area(area_file, layer_config, layer_name, output_gpkg):
    """Extrai uma camada específica do arquivo OSM da área"""
    osm_key = layer_config['osm_key']
    osm_values = layer_config['osm_values']
    layer_type = layer_config['layer_type']

    # Mapear tipo de geometria para camada OSM
    if layer_type == 'points':
        osm_layer = 'points'
    elif layer_type == 'lines':
        osm_layer = 'lines'
    else:  # polygons
        osm_layer = 'multipolygons'

    # Construir a condição SQL para filtrar os dados
    if osm_values:
        values_list = osm_values.split(',')
        values_str = ','.join([f"'{v}'" for v in values_list])
        sql_condition = f"{osm_key} IN ({values_str})"
    else:
        sql_condition = f"{osm_key} IS NOT NULL"

    cmd = [
        'ogr2ogr',
        '-f', 'GPKG',
        output_gpkg,
        area_file,
        osm_layer,
        '-where', sql_condition,
        '-nln', layer_name
    ]

    print(f"Extraindo camada {layer_name}...")
    print(f"Comando: {' '.join(cmd)}")

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

        if result.returncode != 0:
            print(f"ERRO ao extrair camada {layer_name}: {result.stderr}")
            return False

        if not os.path.exists(output_gpkg):
            print(f"Arquivo de saída não foi criado: {output_gpkg}")
            return False

        print(f"Camada {layer_name} extraída com sucesso: {output_gpkg}")
        return True
    except subprocess.TimeoutExpired:
        print(f"Extração da camada {layer_name} demorou demais e foi interrompida (timeout após 5 minutos)")
        return False
    except Exception as e:
        print(f"Erro ao executar ogr2ogr para {layer_name}: {str(e)}")
        return False

# Função para recortar dados geográficos com o polígono de Sorocaba
def clip_with_sorocaba(input_gpkg, layer_name, output_gpkg):
    """Recorta os dados com o polígono de Sorocaba"""
    try:
        # Carregar os dados originais
        data_gdf = gpd.read_file(input_gpkg)

        if len(data_gdf) == 0:
            print(f"Camada {layer_name} está vazia. Pulando...")
            return None

        print(f"Camada {layer_name}: {len(data_gdf)} elementos carregados")

        # Verificar e converter CRS se necessário
        if data_gdf.crs is None:
            print(f"AVISO: Camada {layer_name} não possui CRS definido. Assumindo o mesmo de Sorocaba.")
            data_gdf.set_crs(sorocaba_gdf.crs, inplace=True)
        elif data_gdf.crs != sorocaba_gdf.crs:
            print(f"Convertendo CRS de {data_gdf.crs} para {sorocaba_gdf.crs}")
            data_gdf = data_gdf.to_crs(sorocaba_gdf.crs)

        # Realizar o recorte
        clipped_gdf = gpd.clip(data_gdf, sorocaba_gdf)

        if len(clipped_gdf) == 0:
            print(f"Nenhum elemento de {layer_name} encontrado dentro de Sorocaba")
            return None

        print(f"Recorte concluído: {len(clipped_gdf)} elementos")

        # Salvar o resultado
        clipped_gdf.to_file(output_gpkg, driver='GPKG')
        print(f"Dados recortados salvos em: {output_gpkg}")

        # Gerar CSV
        csv_path = os.path.join(csv_dir, f"sorocaba_{layer_name}.csv")

        # Adicionar coordenadas dos centróides
        try:
            clipped_gdf['centroid_x'] = clipped_gdf.geometry.centroid.x
            clipped_gdf['centroid_y'] = clipped_gdf.geometry.centroid.y
        except Exception as e:
            print(f"Aviso: Não foi possível calcular centróides para {layer_name}: {str(e)}")

        # Salvar CSV sem geometria
        csv_data = clipped_gdf.drop(columns='geometry') if 'geometry' in clipped_gdf.columns else clipped_gdf
        csv_data.to_csv(csv_path, index=False)
        print(f"Arquivo CSV salvo em: {csv_path}")

        return output_gpkg

    except Exception as e:
        print(f"ERRO ao recortar camada {layer_name}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# Função para gerar visualizações das camadas
def create_visualization(gpkg_file, layer_name, layer_info):
    """Cria visualizações das camadas processadas"""
    try:
        # Carregar dados
        gdf = gpd.read_file(gpkg_file)

        if len(gdf) == 0:
            print(f"Sem dados para visualizar em {layer_name}")
            return

        # Definir estilo baseado no tipo de camada
        style = layer_info['style']

        # Criar visualização básica
        fig, ax = plt.subplots(figsize=(12, 10))

        # Adicionar título e descrição
        title = f"{layer_info['description']} - Sorocaba"

        # Plotar camada
        if 'cmap' in style:
            # Se tiver coluna correspondente ao tipo de camada, usar como coluna para colorir
            if layer_info['osm_key'] in gdf.columns:
                gdf.plot(ax=ax, column=layer_info['osm_key'], **style)
            else:
                gdf.plot(ax=ax, **style)
        else:
            gdf.plot(ax=ax, **style)

        # Adicionar contorno de Sorocaba
        sorocaba_gdf.boundary.plot(ax=ax, color='black', linewidth=1.5)

        # Configurações do gráfico
        ax.set_title(title, fontsize=14)
        ax.set_xlabel('Longitude')
        ax.set_ylabel('Latitude')
        ax.grid(True, linestyle='--', alpha=0.5)

        # Salvar visualização
        basic_plot = os.path.join(plot_dir, f"sorocaba_{layer_name}.png")
        plt.savefig(basic_plot, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Visualização básica salva em: {basic_plot}")

        # Criar visualização com mapa base
        fig, ax = plt.subplots(figsize=(12, 10))

        # Converter para WGS84 para compatibilidade com contextily
        gdf_wgs84 = gdf.to_crs(epsg=4326)
        sorocaba_wgs84 = sorocaba_gdf.to_crs(epsg=4326)

        # Plotar camada
        if 'cmap' in style:
            if layer_info['osm_key'] in gdf_wgs84.columns:
                gdf_wgs84.plot(ax=ax, column=layer_info['osm_key'], **style)
            else:
                gdf_wgs84.plot(ax=ax, **style)
        else:
            gdf_wgs84.plot(ax=ax, **style)

        # Adicionar contorno
        sorocaba_wgs84.boundary.plot(ax=ax, color='black', linewidth=1.5)

        # Adicionar mapa base
        try:
            ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)
        except Exception as e:
            print(f"Não foi possível adicionar mapa base: {str(e)}")

        ax.set_title(f"{title} (com mapa base)", fontsize=14)

        # Salvar visualização com mapa base
        basemap_plot = os.path.join(plot_dir, f"sorocaba_{layer_name}_basemap.png")
        plt.savefig(basemap_plot, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Visualização com mapa base salva em: {basemap_plot}")

    except Exception as e:
        print(f"ERRO ao gerar visualização para {layer_name}: {str(e)}")
        import traceback
        traceback.print_exc()

# Inicio do processamento principal
print("\n" + "="*80)
print("INICIANDO PROCESSAMENTO DE DADOS OSM PARA SOROCABA")
print("="*80)

# Extrair área de Sorocaba do OSM
area_file = os.path.join(output_dir, "sorocaba_area.osm.pbf")
if extract_area_from_osm(buffer_bbox, area_file):
    print("Área de Sorocaba extraída com sucesso!")
else:
    print("ERRO ao extrair área. Abortando processamento.")
    exit(1)

# Processar cada camada
processed_layers = []

for layer_name, layer_info in osm_layers.items():
    print(f"\n{'='*50}")
    print(f"Processando camada: {layer_name} - {layer_info['description']}")
    print(f"{'='*50}")

    # Arquivo de saída temporário
    temp_gpkg = os.path.join(output_dir, f"{layer_name}_temp.gpkg")

    # Extrair camada da área
    if extract_layer_from_area(area_file, layer_info, layer_name, temp_gpkg):
        # Arquivo de saída final
        final_gpkg = os.path.join(output_dir, f"sorocaba_{layer_name}.gpkg")

        # Recortar com o polígono de Sorocaba
        clipped_gpkg = clip_with_sorocaba(temp_gpkg, layer_name, final_gpkg)

        if clipped_gpkg:
            # Criar visualização
            create_visualization(clipped_gpkg, layer_name, layer_info)
            processed_layers.append(layer_name)

        # Remover arquivo temporário
        if os.path.exists(temp_gpkg):
            os.remove(temp_gpkg)
            print(f"Arquivo temporário removido: {temp_gpkg}")

# Resumo do processamento
print("\n" + "="*70)
print("RESUMO DO PROCESSAMENTO")
print("="*70)
print(f"Total de camadas processadas: {len(processed_layers)}/{len(osm_layers)}")
print("\nCamadas processadas com sucesso:")
for layer in processed_layers:
    print(f" - {layer}: {osm_layers[layer]['description']}")

print("\nArquivos gerados:")
print(f" - GeoPackages: {len(processed_layers)} arquivos em {output_dir}")
print(f" - CSVs: {len(processed_layers)} arquivos em {csv_dir}")
print(f" - Visualizações: {len(processed_layers)*2} imagens em {plot_dir}")

# Manter o arquivo da área para processamentos futuros
print(f"\nArquivo da área extraída disponível em: {area_file}")

print("\nProcessamento concluído!")

import geopandas as gpd
import os
import fiona

# Definir o caminho para o arquivo GeoPackage
hidrografia_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_hidrografia.gpkg'

# Listar as camadas disponíveis
layers = fiona.listlayers(hidrografia_gpkg)
print(f"Camadas disponíveis no arquivo: {layers}")

# Explorar cada camada
for layer in layers:
    gdf = gpd.read_file(hidrografia_gpkg, layer=layer)
    print(f"\nCamada: {layer}")
    print(f"Número de feições: {len(gdf)}")
    print(f"Tipo de geometria: {gdf.geometry.geom_type.unique()}")
    print(f"Colunas disponíveis: {gdf.columns.tolist()}")
    print(f"Amostra de dados:")
    print(gdf.head(2))

# Importar bibliotecas necessárias
import os
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import contextily as ctx

# Definir diretórios de trabalho
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = f'{base_dir}/data/terreno/processados'
osm_layers_dir = f'{base_dir}/data/terreno/osm_layers'
plots_dir = f'{base_dir}/data/terreno/plots'
os.makedirs(plots_dir, exist_ok=True)

# Definir o sistema de coordenadas padrão
target_crs = 'EPSG:4674'  # SIRGAS 2000

# Carregar dados hidrográficos do OSM
osm_waterways = gpd.read_file(f'{osm_layers_dir}/sorocaba_waterways.gpkg')
print(f"Dados OSM carregados: {len(osm_waterways)} feições")

# Carregar dados hidrográficos da BHO
bho_gpkg = f'{processed_dir}/sorocaba_hidrografia.gpkg'
bho_trecho = gpd.read_file(bho_gpkg, layer='geoft_bho_trecho_drenagem')
bho_curso = gpd.read_file(bho_gpkg, layer='geoft_bho_curso_dagua')
bho_area = gpd.read_file(bho_gpkg, layer='geoft_bho_area_drenagem')
bho_ponto = gpd.read_file(bho_gpkg, layer='geoft_bho_ponto_drenagem')

print(f"Dados BHO carregados: {len(bho_trecho)} trechos, {len(bho_curso)} cursos, {len(bho_area)} áreas, {len(bho_ponto)} pontos")

# Normalizar sistemas de coordenadas
def normalize_crs(gdf, name):
    if gdf.crs is None:
        print(f"{name} não possui CRS definido. Definindo como {target_crs}.")
        gdf.set_crs(target_crs, inplace=True)
    elif gdf.crs != target_crs:
        print(f"Reprojetando {name} de {gdf.crs} para {target_crs}")
        gdf = gdf.to_crs(target_crs)
    return gdf

# Normalizar OSM
osm_waterways = normalize_crs(osm_waterways, "osm_waterways")

# Normalizar camadas BHO
bho_trecho = normalize_crs(bho_trecho, "bho_trecho")
bho_curso = normalize_crs(bho_curso, "bho_curso")
bho_area = normalize_crs(bho_area, "bho_area")
bho_ponto = normalize_crs(bho_ponto, "bho_ponto")

# Criar buffer em torno dos trechos BHO para identificar elementos OSM únicos
buffer_distance = 0.0001  # aproximadamente 10m em graus decimais
bho_buffer = bho_trecho.geometry.buffer(buffer_distance).unary_union

# Identificar elementos OSM complementares (que não têm correspondência na BHO)
osm_unique = osm_waterways[~osm_waterways.geometry.intersects(bho_buffer)].copy()
osm_unique['source'] = 'OSM'

print(f"Elementos OSM complementares identificados: {len(osm_unique)} de {len(osm_waterways)}")

# Adicionar origem aos dados BHO
bho_trecho['source'] = 'BHO'

# Criar conjunto integrado de hidrografia (BHO + OSM complementar)
# Garantir colunas comuns para união
common_columns = ['geometry', 'source']

# Colunas adicionais para BHO
bho_columns = ['cotrecho', 'cocursodag', 'nogenerico', 'nuordemcda', 'nunivotto', 'nustrahler']
bho_subset = bho_trecho[common_columns + [col for col in bho_columns if col in bho_trecho.columns]]

# Colunas adicionais para OSM
osm_columns = ['osm_id', 'name', 'waterway']
osm_subset = osm_unique[common_columns + [col for col in osm_columns if col in osm_unique.columns]]

# Realizar união dos conjuntos de dados
hidrografia_integrada = pd.concat([bho_subset, osm_subset], ignore_index=True)

print(f"Conjunto hidrográfico integrado criado: {len(hidrografia_integrada)} elementos")
print(f"   - Elementos BHO: {len(bho_subset)} ({len(bho_subset)/len(hidrografia_integrada)*100:.1f}%)")
print(f"   - Elementos OSM complementares: {len(osm_subset)} ({len(osm_subset)/len(hidrografia_integrada)*100:.1f}%)")

# Salvar GeoPackage integrado
output_hydro_gpkg = f'{processed_dir}/hidrografia_integrada.gpkg'

# Salvar camada integrada como principal
hidrografia_integrada.to_file(output_hydro_gpkg, layer='hidrografia_integrada')

# Salvar também as camadas originais para referência
bho_trecho.to_file(output_hydro_gpkg, layer='bho_trecho_drenagem')
bho_curso.to_file(output_hydro_gpkg, layer='bho_curso_dagua')
bho_area.to_file(output_hydro_gpkg, layer='bho_area_drenagem')
bho_ponto.to_file(output_hydro_gpkg, layer='bho_ponto_drenagem')
osm_unique.to_file(output_hydro_gpkg, layer='osm_complementar')

print(f"GeoPackage salvo em: {output_hydro_gpkg}")

# Visualizar o resultado
fig, ax = plt.subplots(figsize=(15, 12))

# Plotar áreas de drenagem como fundo
bho_area.plot(ax=ax, color='lightblue', alpha=0.3, label='BHO - Áreas de Drenagem')

# Plotar dados integrados com cores diferentes por fonte
bho_data = hidrografia_integrada[hidrografia_integrada['source'] == 'BHO']
osm_data = hidrografia_integrada[hidrografia_integrada['source'] == 'OSM']

bho_data.plot(ax=ax, color='blue', linewidth=1.5, label='BHO - Trechos')
osm_data.plot(ax=ax, color='lime', linewidth=0.8, label='OSM - Complementar')

# Adicionar pontos BHO
bho_ponto.plot(ax=ax, color='navy', markersize=20, label='BHO - Pontos')

# Adicionar mapa base
try:
    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
except Exception as e:
    print(f"Erro ao adicionar mapa base: {e}")

# Adicionar legenda e título
ax.legend(loc='upper left')
plt.title('Hidrografia Integrada - Sorocaba', fontsize=14)
plt.tight_layout()

# Salvar visualização
plt.savefig(f'{plots_dir}/hidrografia_integrada_visualizacao.png', dpi=300, bbox_inches='tight')
print(f"Visualização salva em: {plots_dir}/hidrografia_integrada_visualizacao.png")

# Exibir estatísticas finais
print("\nEstatísticas da integração hidrográfica:")
print(f"Total de trechos na hidrografia integrada: {len(hidrografia_integrada)}")
print(f"Extensão total BHO: {bho_data.geometry.length.sum():.1f} unidades")
print(f"Extensão OSM complementar: {osm_data.geometry.length.sum():.1f} unidades")
print(f"Extensão total integrada: {hidrografia_integrada.geometry.length.sum():.1f} unidades")

# Calcular percentual de complementação por extensão
osm_percent = osm_data.geometry.length.sum() / hidrografia_integrada.geometry.length.sum() * 100
print(f"Percentual de complementação OSM por extensão: {osm_percent:.1f}%")

plt.show()

import os

caminho = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados'
arquivos = os.listdir(caminho)

print("Arquivos encontrados:")
for arquivo in arquivos:
    print(arquivo)

import os

caminho = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/osm_layers'
arquivos = os.listdir(caminho)

print("Arquivos encontrados:")
for arquivo in arquivos:
    print(arquivo)

pip install contextily

# Importar bibliotecas necessárias
import os
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import contextily as ctx
import warnings
from shapely.ops import unary_union

# Suprimir avisos específicos
warnings.filterwarnings('ignore', category=UserWarning, message='.*CRS mismatch.*')

# Definir diretórios de trabalho
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = f'{base_dir}/data/terreno/processados'
osm_layers_dir = f'{base_dir}/data/terreno/osm_layers'
plots_dir = f'{base_dir}/data/terreno/plots'
csv_dir = f'{base_dir}/data/terreno/csv'

# Garantir que os diretórios existam
os.makedirs(plots_dir, exist_ok=True)
os.makedirs(csv_dir, exist_ok=True)

# Definir o sistema de coordenadas padrão
target_crs = 'EPSG:4674'  # SIRGAS 2000

print("Iniciando integração dos dados de edificações e setores censitários...")

# Normalizar sistemas de coordenadas
def normalize_crs(gdf, name):
    if gdf is None:
        return None

    if gdf.crs is None:
        print(f"{name} não possui CRS definido. Definindo como {target_crs}.")
        gdf.set_crs(target_crs, inplace=True)
    elif gdf.crs != target_crs:
        print(f"Reprojetando {name} de {gdf.crs} para {target_crs}")
        gdf = gdf.to_crs(target_crs)
    return gdf

# Carregar e normalizar fontes de dados
print("\n1. Carregando dados de edificações e setores censitários...")

# Dados de edificações do OSM
try:
    buildings = gpd.read_file(f'{osm_layers_dir}/sorocaba_buildings.gpkg')
    buildings = normalize_crs(buildings, "Edificações OSM")
    buildings['source'] = 'OSM_buildings'
    print(f"  Dados de edificações carregados: {len(buildings)} feições")
    print(f"  Tipos de edificações: {buildings['building'].unique()[:10]}... (primeiros 10)")
    print(f"  Colunas disponíveis: {buildings.columns.tolist()}")
except Exception as e:
    print(f"  Erro ao carregar dados de edificações: {e}")
    buildings = None

# Dados de setores censitários
try:
    setores = gpd.read_file(f'{processed_dir}/sorocaba_SP_setores_CD2022.gpkg')
    setores = normalize_crs(setores, "Setores Censitários")
    setores['source'] = 'IBGE_setores'
    print(f"  Dados de setores censitários carregados: {len(setores)} feições")
    print(f"  Colunas disponíveis: {setores.columns.tolist()}")
except Exception as e:
    print(f"  Erro ao carregar dados de setores censitários: {e}")
    setores = None

# 2. Padronização de esquemas para integração
print("\n2. Padronizando esquemas para integração...")

# Função para padronizar edificações
def standardize_buildings(buildings_gdf):
    if buildings_gdf is None:
        return None

    # Criar uma cópia do GeoDataFrame
    std_gdf = buildings_gdf.copy()

    # Garantir coluna de fonte
    std_gdf['source'] = 'OSM_buildings'

    # Classificar tipos de edificações
    building_mapping = {
        'residential': 'residential',
        'house': 'residential',
        'apartments': 'residential',
        'detached': 'residential',
        'commercial': 'commercial',
        'retail': 'commercial',
        'office': 'commercial',
        'industrial': 'industrial',
        'warehouse': 'industrial',
        'factory': 'industrial',
        'school': 'educational',
        'university': 'educational',
        'college': 'educational',
        'hospital': 'health',
        'clinic': 'health',
        'public': 'public',
        'government': 'public',
        'religious': 'religious',
        'church': 'religious',
        'temple': 'religious',
        'mosque': 'religious',
        'garage': 'service',
        'parking': 'service',
        'shed': 'service',
        'farm': 'agricultural',
        'barn': 'agricultural',
        'greenhouse': 'agricultural',
        'hotel': 'accommodation',
        'dormitory': 'accommodation',
        'roof': 'other',
        'construction': 'other',
        'service': 'service',
        'yes': 'unspecified'
    }

    # Aplicar mapeamento de tipos de edificações
    std_gdf['building_type'] = std_gdf['building'].map(building_mapping)
    std_gdf['building_type'] = std_gdf['building_type'].fillna('other')

    # Determinar nível aproximado de prioridade para emergências
    priority_mapping = {
        'health': 1,          # Máxima prioridade para instalações de saúde
        'educational': 2,     # Escolas e universidades
        'public': 3,          # Prédios públicos e governamentais
        'residential': 4,     # Áreas residenciais
        'accommodation': 5,   # Hotéis e alojamentos
        'commercial': 6,      # Comércio
        'religious': 7,       # Locais religiosos
        'industrial': 8,      # Indústrias
        'service': 9,         # Edificações de serviço
        'agricultural': 10,   # Edificações agrícolas
        'unspecified': 11,    # Tipo não especificado
        'other': 12           # Outros tipos
    }

    std_gdf['priority'] = std_gdf['building_type'].map(priority_mapping)

    # Selecionar e renomear colunas relevantes
    columns = [
        'geometry', 'source', 'building', 'building_type', 'priority',
        'name', 'osm_id', 'osm_way_id', 'amenity'
    ]

    # Manter apenas colunas que existem
    columns_to_keep = [col for col in columns if col in std_gdf.columns]

    return std_gdf[columns_to_keep]

# Função para padronizar setores censitários
def standardize_sectors(sectors_gdf):
    if sectors_gdf is None:
        return None

    # Criar uma cópia do GeoDataFrame
    std_gdf = sectors_gdf.copy()

    # Garantir coluna de fonte
    std_gdf['source'] = 'IBGE_setores'

    # Analisar tipo de situação do setor (urbano/rural)
    if 'SITUACAO' in std_gdf.columns:
        # Criar indicador binário para urbano/rural
        std_gdf['is_urban'] = std_gdf['SITUACAO'].apply(
            lambda x: 1 if 'URBANO' in str(x).upper() else 0
        )

    # Calcular área em km²
    std_gdf['area_km2'] = std_gdf.geometry.area / 10**6

    # Selecionar colunas relevantes que estão presentes no dataframe
    essential_columns = ['geometry', 'source', 'CD_SETOR', 'area_km2']
    if 'SITUACAO' in std_gdf.columns:
        essential_columns.append('SITUACAO')
    if 'is_urban' in std_gdf.columns:
        essential_columns.append('is_urban')
    if 'NM_MUN' in std_gdf.columns:
        essential_columns.append('NM_MUN')

    return std_gdf[essential_columns]

# Aplicar padronização
std_buildings = standardize_buildings(buildings)
std_sectors = standardize_sectors(setores)

# 3. Criar integrações e análises
print("\n3. Criando integrações e análises espaciais...")

# Inicializar dicionário para armazenar resultados das análises
analysis_results = {}

# 3.1 Estatísticas básicas
print("  3.1 Realizando análises estatísticas básicas...")

if std_buildings is not None:
    # Estatísticas de edificações
    building_stats = {
        'total_count': len(std_buildings),
        'by_type': std_buildings['building_type'].value_counts().to_dict(),
        'by_priority': std_buildings.groupby('priority')['building_type'].count().to_dict()
    }
    analysis_results['buildings'] = building_stats
    print(f"    - Total de edificações: {building_stats['total_count']}")
    print(f"    - Tipos mais comuns: {sorted(building_stats['by_type'].items(), key=lambda x: x[1], reverse=True)[:5]}")

if std_sectors is not None:
    # Estatísticas de setores censitários
    sector_stats = {
        'total_count': len(std_sectors),
        'total_area_km2': std_sectors['area_km2'].sum(),
        'avg_area_km2': std_sectors['area_km2'].mean(),
        'min_area_km2': std_sectors['area_km2'].min(),
        'max_area_km2': std_sectors['area_km2'].max()
    }

    if 'is_urban' in std_sectors.columns:
        urban_count = std_sectors['is_urban'].sum()
        rural_count = len(std_sectors) - urban_count
        sector_stats['urban_count'] = urban_count
        sector_stats['rural_count'] = rural_count
        sector_stats['urban_percentage'] = (urban_count / len(std_sectors)) * 100

    analysis_results['sectors'] = sector_stats
    print(f"    - Total de setores censitários: {sector_stats['total_count']}")
    print(f"    - Área total: {sector_stats['total_area_km2']:.2f} km²")
    if 'urban_count' in sector_stats:
        print(f"    - Setores urbanos: {sector_stats['urban_count']} ({sector_stats['urban_percentage']:.1f}%)")
        print(f"    - Setores rurais: {sector_stats['rural_count']} ({100-sector_stats['urban_percentage']:.1f}%)")

# 3.2 Análise de densidade de edificações por setor (se ambos disponíveis)
print("  3.2 Analisando densidade de edificações por setor censitário...")

if std_buildings is not None and std_sectors is not None:
    try:
        print("    Calculando densidade de edificações por setor...")
        # Criar uma cópia dos dados para não afetar os originais
        sector_density = std_sectors.copy()

        # Inicializar coluna de contagem de edificações
        sector_density['building_count'] = 0

        # Realizar operação de intersecção espacial
        # Pode levar algum tempo para conjuntos de dados grandes
        intersection = gpd.sjoin(std_buildings, std_sectors, how='inner', predicate='within')
        building_counts = intersection.groupby('CD_SETOR').size()

        # Atualizar contagens no GeoDataFrame dos setores
        for sector_id, count in building_counts.items():
            sector_density.loc[sector_density['CD_SETOR'] == sector_id, 'building_count'] = count

        # Calcular densidade (edificações por km²)
        sector_density['building_density'] = sector_density['building_count'] / sector_density['area_km2']

        # Armazenar resultados
        analysis_results['building_density'] = {
            'max_density': sector_density['building_density'].max(),
            'min_density': sector_density['building_density'].min(),
            'mean_density': sector_density['building_density'].mean(),
            'median_density': sector_density['building_density'].median()
        }

        print(f"    - Densidade média: {analysis_results['building_density']['mean_density']:.2f} edificações/km²")
        print(f"    - Densidade máxima: {analysis_results['building_density']['max_density']:.2f} edificações/km²")

        # Criar visualização da densidade
        fig, ax = plt.subplots(figsize=(15, 12))
        sector_density.plot(
            column='building_density',
            ax=ax,
            legend=True,
            cmap='YlOrRd',
            scheme='quantiles',
            k=7,
            alpha=0.7,
            legend_kwds={'title': 'Edificações/km²'}
        )
        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
        plt.title('Densidade de Edificações por Setor Censitário', fontsize=14)
        plt.savefig(f'{plots_dir}/densidade_edificacoes.png', dpi=300, bbox_inches='tight')
        print(f"    - Visualização salva em: {plots_dir}/densidade_edificacoes.png")

        # Salvar resultados da análise de densidade
        sector_density.to_file(f'{processed_dir}/setores_densidade_edificacoes.gpkg', driver='GPKG')
        print(f"    - Dados de densidade salvos em: {processed_dir}/setores_densidade_edificacoes.gpkg")

    except Exception as e:
        print(f"    Erro ao calcular densidade de edificações: {e}")
        import traceback
        print(traceback.format_exc())
else:
    print("    Análise de densidade não realizada - dados insuficientes")

# 3.3 Análise de edificações prioritárias (saúde, educação, etc.)
print("  3.3 Identificando edificações prioritárias...")

if std_buildings is not None:
    try:
        # Identificar edificações de alta prioridade (prioridade <= 3)
        priority_buildings = std_buildings[std_buildings['priority'] <= 3].copy()

        if len(priority_buildings) > 0:
            print(f"    - Edificações prioritárias identificadas: {len(priority_buildings)}")

            # Estatísticas por tipo
            priority_counts = priority_buildings.groupby('building_type').size()
            for type_name, count in priority_counts.items():
                print(f"    - {type_name}: {count} edificações")

            # Visualizar edificações prioritárias
            fig, ax = plt.subplots(figsize=(15, 12))

            # Definir cores por tipo de prioridade
            priority_colors = {
                'health': 'red',
                'educational': 'blue',
                'public': 'purple'
            }

            # Plotar por tipo
            for p_type, color in priority_colors.items():
                subset = priority_buildings[priority_buildings['building_type'] == p_type]
                if len(subset) > 0:
                    subset.plot(ax=ax, color=color, alpha=0.8, label=p_type, markersize=50)

            # Adicionar mapa base e estilizar
            ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
            ax.set_title('Edificações Prioritárias - Sorocaba', fontsize=14)
            ax.legend(title='Tipo de Edificação')

            # Salvar visualização
            plt.savefig(f'{plots_dir}/edificacoes_prioritarias.png', dpi=300, bbox_inches='tight')
            print(f"    - Visualização salva em: {plots_dir}/edificacoes_prioritarias.png")

            # Salvar camada de edificações prioritárias
            priority_buildings.to_file(f'{processed_dir}/edificacoes_prioritarias.gpkg', driver='GPKG')
            print(f"    - Dados de edificações prioritárias salvos em: {processed_dir}/edificacoes_prioritarias.gpkg")

        else:
            print("    - Nenhuma edificação prioritária identificada")

    except Exception as e:
        print(f"    Erro ao analisar edificações prioritárias: {e}")
else:
    print("    Análise de edificações prioritárias não realizada - dados insuficientes")

# 4. Salvar arquivo integrado para análises futuras
print("\n4. Salvando arquivos integrados para grafos geoespaciais...")

# Criar GeoPackage para camada socioeconômica
output_socio_gpkg = f'{processed_dir}/socioeconomico_integrado.gpkg'

# Salvar camadas para o arquivo integrado
if std_buildings is not None:
    std_buildings.to_file(output_socio_gpkg, layer='edificacoes')
    print(f"  - Camada de edificações salva em: {output_socio_gpkg}")

if std_sectors is not None:
    std_sectors.to_file(output_socio_gpkg, layer='setores_censitarios')
    print(f"  - Camada de setores censitários salva em: {output_socio_gpkg}")

if 'sector_density' in locals() and sector_density is not None:
    sector_density.to_file(output_socio_gpkg, layer='densidade_edificacoes')
    print(f"  - Camada de densidade de edificações salva em: {output_socio_gpkg}")

if 'priority_buildings' in locals() and len(priority_buildings) > 0:
    priority_buildings.to_file(output_socio_gpkg, layer='edificacoes_prioritarias')
    print(f"  - Camada de edificações prioritárias salva em: {output_socio_gpkg}")

# Salvar resultados das análises como arquivo CSV
analysis_df = pd.DataFrame([analysis_results])
analysis_csv_path = os.path.join(csv_dir, "analise_socioeconomica.csv")
analysis_df.to_csv(analysis_csv_path, index=False)
print(f"  - Resultados das análises salvos em: {analysis_csv_path}")

# 5. Visualização geral dos dados integrados
print("\n5. Criando visualização geral dos dados integrados...")

try:
    # Criar figura para visualização combinada
    fig, ax = plt.subplots(figsize=(15, 12))

    # Plotar setores censitários como fundo
    if std_sectors is not None:
        if 'is_urban' in std_sectors.columns:
            # Colorir por tipo urbano/rural
            std_sectors.plot(
                column='is_urban',
                ax=ax,
                alpha=0.5,
                cmap='Set3',
                legend=True,
                legend_kwds={'labels': ['Rural', 'Urbano'], 'title': 'Tipo de Setor'}
            )
        else:
            # Plotar sem coloração específica
            std_sectors.plot(ax=ax, alpha=0.4, color='lightgray')

    # Plotar edificações (amostra para não sobrecarregar a visualização)
    if std_buildings is not None:
        # Usar uma amostra se houver muitas edificações
        sample_size = min(5000, len(std_buildings))
        buildings_sample = std_buildings.sample(sample_size) if len(std_buildings) > sample_size else std_buildings

        # Plotar por tipo prioritário
        color_map = {
            'health': 'red',
            'educational': 'blue',
            'public': 'purple',
            'residential': 'green',
            'commercial': 'orange',
            'industrial': 'brown'
        }

        for building_type, color in color_map.items():
            subset = buildings_sample[buildings_sample['building_type'] == building_type]
            if len(subset) > 0:
                subset.plot(ax=ax, color=color, markersize=10, alpha=0.7, label=building_type)

    # Adicionar mapa base
    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)

    # Estilizar e adicionar título
    ax.set_title('Integração Socioecônomica - Sorocaba', fontsize=14)

    # Adicionar legenda para tipos de edificações
    if std_buildings is not None:
        handles, labels = ax.get_legend_handles_labels()
        if handles:
            ax.legend(
                handles=handles,
                labels=labels,
                title="Tipos de Edificações",
                loc='upper left',
                bbox_to_anchor=(1.01, 1)
            )

    # Ajustar layout e salvar
    plt.tight_layout()
    plt.savefig(f'{plots_dir}/integracao_socioeconomica.png', dpi=300, bbox_inches='tight')
    print(f"  - Visualização geral salva em: {plots_dir}/integracao_socioeconomica.png")
except Exception as e:
    print(f"  Erro ao criar visualização geral: {e}")
    import traceback
    print(traceback.format_exc())

print("\nProcessamento completo.")

# Importar bibliotecas necessárias
import os
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import contextily as ctx
import warnings
import json
import fiona  # Add this import statement
from shapely.ops import unary_union
from matplotlib.patches import Patch

# Suprimir avisos específicos
warnings.filterwarnings('ignore', category=UserWarning, message='.*CRS mismatch.*')

# Definir diretórios de trabalho
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = f'{base_dir}/data/terreno/processados'
osm_layers_dir = f'{base_dir}/data/terreno/osm_layers'
plots_dir = f'{base_dir}/data/terreno/plots'
csv_dir = f'{base_dir}/data/terreno/csv'

# Garantir que os diretórios existam
os.makedirs(plots_dir, exist_ok=True)
os.makedirs(csv_dir, exist_ok=True)

# Definir o sistema de coordenadas padrão
target_crs = 'EPSG:4674'  # SIRGAS 2000

print("Iniciando integração dos dados de uso da terra e ocupação...")

# Normalizar sistemas de coordenadas
def normalize_crs(gdf, name):
    if gdf is None:
        return None

    if gdf.crs is None:
        print(f"{name} não possui CRS definido. Definindo como {target_crs}.")
        gdf.set_crs(target_crs, inplace=True)
    elif gdf.crs != target_crs:
        print(f"Reprojetando {name} de {gdf.crs} para {target_crs}")
        gdf = gdf.to_crs(target_crs)
    return gdf

# Carregar e normalizar fontes de dados de uso da terra
print("\n1. Carregando fontes de dados de uso da terra e ocupação...")

# Dados OSM de uso da terra
try:
    osm_landuse = gpd.read_file(f'{osm_layers_dir}/sorocaba_landuse.gpkg')
    osm_landuse = normalize_crs(osm_landuse, "OSM Landuse")
    osm_landuse['source'] = 'OSM_landuse'
    print(f"  Dados OSM Landuse carregados: {len(osm_landuse)} feições")
    print(f"  Valores únicos de landuse: {osm_landuse['landuse'].unique()}")
except Exception as e:
    print(f"  Erro ao carregar dados OSM Landuse: {e}")
    osm_landuse = None

# Dados OSM de elementos naturais
try:
    osm_natural = gpd.read_file(f'{osm_layers_dir}/sorocaba_natural.gpkg')
    osm_natural = normalize_crs(osm_natural, "OSM Natural")
    osm_natural['source'] = 'OSM_natural'
    print(f"  Dados OSM Natural carregados: {len(osm_natural)} feições")
    print(f"  Valores únicos de natural: {osm_natural['natural'].unique()}")
except Exception as e:
    print(f"  Erro ao carregar dados OSM Natural: {e}")
    osm_natural = None

# Dados de áreas urbanas
try:
    urbano = gpd.read_file(f'{processed_dir}/sorocaba_Urbano.gpkg')
    urbano = normalize_crs(urbano, "Urbano")
    urbano['source'] = 'Urbano'
    urbano['landcover'] = 'urban'
    print(f"  Dados Urbano carregados: {len(urbano)} feições")
    print(f"  Colunas de dados urbanos: {urbano.columns.tolist()}")
except Exception as e:
    print(f"  Erro ao carregar dados Urbano: {e}")
    urbano = None

# Dados de áreas rurais
try:
    rural = gpd.read_file(f'{processed_dir}/sorocaba_Rural.gpkg')
    rural = normalize_crs(rural, "Rural")
    rural['source'] = 'Rural'
    rural['landcover'] = 'rural'
    print(f"  Dados Rural carregados: {len(rural)} feições")
    print(f"  Colunas de dados rurais: {rural.columns.tolist()}")
except Exception as e:
    print(f"  Erro ao carregar dados Rural: {e}")
    rural = None

# Dados de natureza
try:
    natureza = gpd.read_file(f'{processed_dir}/sorocaba_Natureza.gpkg')
    natureza = normalize_crs(natureza, "Natureza")
    natureza['source'] = 'Natureza'
    natureza['landcover'] = 'nature'
    print(f"  Dados Natureza carregados: {len(natureza)} feições")
    print(f"  Colunas de dados natureza: {natureza.columns.tolist()}")
except Exception as e:
    print(f"  Erro ao carregar dados Natureza: {e}")
    natureza = None

# 2. Padronização de esquemas para integração
print("\n2. Padronizando esquemas para integração...")

# Função para criar um esquema padronizado para cada fonte
def standardize_schema(gdf, source_name, layer_type='landcover'):
    if gdf is None:
        return None

    # Criar uma cópia do GeoDataFrame
    std_gdf = gdf.copy()

    # Garantir coluna de origem
    std_gdf['source'] = source_name

    # Definir o tipo de camada
    std_gdf['layer_type'] = layer_type

    # Criar classificação unificada com base na origem
    if source_name == 'OSM_landuse':
        # Mapear valores de landuse do OSM para categorias unificadas
        landuse_mapping = {
            'residential': 'urban',
            'commercial': 'urban',
            'industrial': 'urban',
            'retail': 'urban',
            'farmland': 'agriculture',
            'farm': 'agriculture',
            'meadow': 'agriculture',
            'orchard': 'agriculture',
            'vineyard': 'agriculture',
            'forest': 'forest',
            'grass': 'vegetation',
            'recreation_ground': 'recreation',
            'park': 'recreation',
            'cemetery': 'urban',
            'brownfield': 'urban',
            'construction': 'urban',
            'greenfield': 'agriculture',
            'military': 'institutional',
            'railway': 'transportation',
            'religious': 'urban',
            'reservoir': 'water',
            # Valores padrão para casos não mapeados
            'quarry': 'industrial',
            'landfill': 'industrial'
        }
        # Aplicar mapeamento
        std_gdf['class'] = std_gdf['landuse'].map(landuse_mapping)
        # Para valores não mapeados, usar 'other'
        std_gdf['class'] = std_gdf['class'].fillna('other')

    elif source_name == 'OSM_natural':
        # Mapear valores naturais do OSM
        natural_mapping = {
            'water': 'water',
            'wetland': 'water',
            'wood': 'forest',
            'tree': 'forest',
            'scrub': 'vegetation',
            'heath': 'vegetation',
            'grassland': 'vegetation',
            'bare_rock': 'bare',
            'beach': 'bare',
            'sand': 'bare',
            'cliff': 'bare'
        }
        std_gdf['class'] = std_gdf['natural'].map(natural_mapping)
        std_gdf['class'] = std_gdf['class'].fillna('other')

    elif source_name == 'Urbano':
        std_gdf['class'] = 'urban'

    elif source_name == 'Rural':
        std_gdf['class'] = 'rural'

    elif source_name == 'Natureza':
        # Mapear com base na coluna USO2018 se disponível
        if 'USO2018' in std_gdf.columns:
            uso_mapping = {
                1: 'forest',
                2: 'agriculture',
                3: 'pasture',
                4: 'water',
                # Adicione outros mapeamentos conforme necessário
            }
            std_gdf['class'] = std_gdf['USO2018'].map(uso_mapping)
            std_gdf['class'] = std_gdf['class'].fillna('other')
        else:
            std_gdf['class'] = 'nature'

    else:
        # Caso padrão
        std_gdf['class'] = 'other'

    # Selecionar colunas relevantes para a integração
    essential_columns = ['geometry', 'source', 'layer_type', 'class']

    # Adicionar colunas específicas de cada fonte que possam ser úteis
    if source_name == 'OSM_landuse':
        additional_columns = ['landuse', 'name', 'osm_id']
    elif source_name == 'OSM_natural':
        additional_columns = ['natural', 'name', 'osm_id']
    elif source_name in ['Urbano', 'Rural', 'Natureza']:
        additional_columns = [col for col in std_gdf.columns if col.startswith('USO')]
    else:
        additional_columns = []

    # Criar lista de colunas a manter
    columns_to_keep = essential_columns + [col for col in additional_columns if col in std_gdf.columns]

    # Retornar GeoDataFrame padronizado
    return std_gdf[columns_to_keep]

# Aplicar padronização a cada fonte
std_osm_landuse = standardize_schema(osm_landuse, 'OSM_landuse', 'landuse')
std_osm_natural = standardize_schema(osm_natural, 'OSM_natural', 'natural')
std_urbano = standardize_schema(urbano, 'Urbano', 'urban')
std_rural = standardize_schema(rural, 'Rural', 'rural')
std_natureza = standardize_schema(natureza, 'Natureza', 'nature')

# 3. Integração dos dados
print("\n3. Integrando dados de uso da terra e ocupação...")

# Lista de todas as fontes padronizadas
std_sources = [
    std_osm_landuse,
    std_osm_natural,
    std_urbano,
    std_rural,
    std_natureza
]

# Filtrar fontes não nulas
valid_sources = [src for src in std_sources if src is not None]

if len(valid_sources) == 0:
    print("  Erro: Nenhuma fonte de dados válida disponível para integração.")
else:
    # Concatenar todos os dados em um único GeoDataFrame
    landcover_integrado = pd.concat(valid_sources, ignore_index=True)

    print(f"  Total de feições integradas: {len(landcover_integrado)}")

    # Verificar classes presentes nos dados integrados
    classes = landcover_integrado['class'].value_counts()
    print("\n  Classes presentes nos dados integrados:")
    for cls, count in classes.items():
        print(f"  - {cls}: {count} feições ({count/len(landcover_integrado)*100:.1f}%)")

    # Verificar fontes presentes nos dados integrados
    sources = landcover_integrado['source'].value_counts()
    print("\n  Fontes presentes nos dados integrados:")
    for src, count in sources.items():
        print(f"  - {src}: {count} feições ({count/len(landcover_integrado)*100:.1f}%)")

    # 4. Salvar resultado
    output_landcover_gpkg = f'{processed_dir}/uso_terra_ocupacao_integrado.gpkg'

    # Salvar camada integrada completa
    landcover_integrado.to_file(output_landcover_gpkg, layer='uso_terra_ocupacao_integrado')

    # Salvar também cada fonte original como camada separada para referência
    if std_osm_landuse is not None:
        std_osm_landuse.to_file(output_landcover_gpkg, layer='osm_landuse')

    if std_osm_natural is not None:
        std_osm_natural.to_file(output_landcover_gpkg, layer='osm_natural')

    if std_urbano is not None:
        std_urbano.to_file(output_landcover_gpkg, layer='urbano')

    if std_rural is not None:
        std_rural.to_file(output_landcover_gpkg, layer='rural')

    if std_natureza is not None:
        std_natureza.to_file(output_landcover_gpkg, layer='natureza')

    print(f"\n  GeoPackage integrado salvo em: {output_landcover_gpkg}")

    # Salvar arquivo CSV derivado (sem a geometria)
    csv_path = os.path.join(csv_dir, "uso_terra_ocupacao_integrado.csv")

    # Adicionar coordenadas dos centróides
    try:
        landcover_integrado['centroid_x'] = landcover_integrado.geometry.centroid.x
        landcover_integrado['centroid_y'] = landcover_integrado.geometry.centroid.y
    except Exception as e:
        print(f"  Aviso: Não foi possível calcular centróides: {e}")

    # Criar DataFrame sem a geometria para CSV
    landcover_csv = landcover_integrado.drop(columns='geometry').copy()

    # Salvar CSV
    landcover_csv.to_csv(csv_path, index=False)
    print(f"  Arquivo CSV salvo em: {csv_path}")

    # 5. Criar visualização
    print("\n4. Criando visualização dos dados integrados...")

    # Criar um mapa de cores por classe
    class_colors = {
        'urban': '#FF0000',          # Vermelho
        'rural': '#F5DEB3',          # Trigo
        'agriculture': '#FFD700',    # Ouro
        'forest': '#008000',         # Verde
        'vegetation': '#90EE90',     # Verde claro
        'water': '#1E90FF',          # Azul
        'recreation': '#32CD32',     # Lima verde
        'industrial': '#A52A2A',     # Marrom
        'transportation': '#808080', # Cinza
        'institutional': '#FF8C00',  # Laranja escuro
        'bare': '#D2B48C',           # Bronzeado
        'nature': '#006400',         # Verde escuro
        'pasture': '#7CFC00',        # Verde gramado
        'other': '#000000'           # Preto
    }

    # Criar visualização
    fig, ax = plt.subplots(figsize=(15, 12))

    # Criar função para plotar cada classe com cor específica
    def plot_by_class(gdf, class_colors, ax):
        for class_name, color in class_colors.items():
            subset = gdf[gdf['class'] == class_name]
            if len(subset) > 0:
                subset.plot(ax=ax, color=color, alpha=0.7, label=class_name)

    # Plotar dados por classe
    plot_by_class(landcover_integrado, class_colors, ax)

    # Plotar também por fonte para uma visualização alternativa
    source_colors = {
        'OSM_landuse': '#FF1493',    # Rosa profundo
        'OSM_natural': '#00BFFF',    # Azul profundo céu
        'Urbano': '#FF4500',         # Laranja vermelho
        'Rural': '#BDB76B',          # Caqui escuro
        'Natureza': '#006400'        # Verde escuro
    }

    # Criar segunda visualização por fonte
    fig2, ax2 = plt.subplots(figsize=(15, 12))

    for source, color in source_colors.items():
        subset = landcover_integrado[landcover_integrado['source'] == source]
        if len(subset) > 0:
            subset.plot(ax=ax2, color=color, alpha=0.7, label=source)

    # Adicionar mapa base
    try:
        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
        ctx.add_basemap(ax2, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
    except Exception as e:
        print(f"  Erro ao adicionar mapa base: {e}")

    # Adicionar legendas e títulos
    ax.legend(loc='upper left', title="Classes de Uso da Terra")
    ax.set_title('Uso da Terra e Ocupação Integrada - Sorocaba (por classe)', fontsize=14)

    ax2.legend(loc='upper left', title="Fontes de Dados")
    ax2.set_title('Uso da Terra e Ocupação Integrada - Sorocaba (por fonte)', fontsize=14)

    # Salvar visualizações
    plt.figure(fig.number)
    plt.tight_layout()
    plt.savefig(f'{plots_dir}/uso_terra_ocupacao_por_classe.png', dpi=300, bbox_inches='tight')

    plt.figure(fig2.number)
    plt.tight_layout()
    plt.savefig(f'{plots_dir}/uso_terra_ocupacao_por_fonte.png', dpi=300, bbox_inches='tight')

    print(f"  Visualizações salvas em:")
    print(f"  - {plots_dir}/uso_terra_ocupacao_por_classe.png")
    print(f"  - {plots_dir}/uso_terra_ocupacao_por_fonte.png")

    # 6. Exibir estatísticas finais
    print("\n5. Estatísticas finais da integração:")
    print(f"  Total de feições: {len(landcover_integrado)}")

    # Calcular áreas por classe
    landcover_integrado['area_km2'] = landcover_integrado.geometry.area / 10**6  # Aproximado para km²
    area_by_class = landcover_integrado.groupby('class')['area_km2'].sum().sort_values(ascending=False)

    print("\n  Área por classe de uso da terra:")
    total_area = area_by_class.sum()
    for cls, area in area_by_class.items():
        print(f"  - {cls}: {area:.2f} km² ({area/total_area*100:.1f}%)")

    print(f"\n  Área total mapeada: {total_area:.2f} km²")

    # Estatísticas por fonte
    area_by_source = landcover_integrado.groupby('source')['area_km2'].sum().sort_values(ascending=False)

    print("\n  Área por fonte de dados:")
    for src, area in area_by_source.items():
        print(f"  - {src}: {area:.2f} km² ({area/total_area*100:.1f}%)")

print("\nProcessamento completo.")

# Importar bibliotecas necessárias
import os
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import contextily as ctx
import warnings
import json
from shapely.ops import unary_union
from matplotlib.patches import Patch
import fiona  # Necessário para uso de fiona.listlayers

# Configurar ambiente para evitar avisos desnecessários
warnings.filterwarnings('ignore', category=UserWarning)
os.environ['USE_PYGEOS'] = '0'  # Evitar avisos de depreciação PyGEOS

# Definir diretórios de trabalho
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = f'{base_dir}/data/terreno/processados'
osm_layers_dir = f'{base_dir}/data/terreno/osm_layers'
plots_dir = f'{base_dir}/data/terreno/plots'
csv_dir = f'{base_dir}/data/terreno/csv'

# Garantir que os diretórios existam
for directory in [processed_dir, plots_dir, csv_dir]:
    os.makedirs(directory, exist_ok=True)

# Definir sistemas de coordenadas
target_crs = 'EPSG:4674'  # SIRGAS 2000 (geográfico)
projected_crs = 'EPSG:31983'  # SIRGAS 2000 / UTM zone 23S (para cálculos de área)

print("Iniciando integração de todos os dados geoespaciais para Sorocaba...")

# ===============================
# 1. INTEGRAÇÃO DE USO DA TERRA E OCUPAÇÃO
# ===============================
def normalize_crs(gdf, name):
    if gdf is None:
        return None
    if gdf.crs is None:
        print(f"  {name} não possui CRS definido. Definindo como {target_crs}.")
        gdf.set_crs(target_crs, inplace=True)
    elif gdf.crs != target_crs:
        print(f"  Reprojetando {name} de {gdf.crs} para {target_crs}")
        gdf = gdf.to_crs(target_crs)
    return gdf

def convert_numpy_types(obj):
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(i) for i in obj]
    else:
        return obj

print("\n1. PARTE I - INTEGRAÇÃO DE USO DA TERRA E OCUPAÇÃO")
print("1.1. Carregando fontes de dados de uso da terra e ocupação...")

# Dados OSM de uso da terra
try:
    osm_landuse = gpd.read_file(f'{osm_layers_dir}/sorocaba_landuse.gpkg')
    osm_landuse = normalize_crs(osm_landuse, "OSM Landuse")
    osm_landuse['source'] = 'OSM_landuse'
    print(f"  Dados OSM Landuse carregados: {len(osm_landuse)} feições")
    print(f"  Valores únicos de landuse: {osm_landuse['landuse'].unique()[:5]}... (primeiros 5)")
except Exception as e:
    print(f"  Erro ao carregar dados OSM Landuse: {e}")
    osm_landuse = None

# Dados OSM de elementos naturais
try:
    osm_natural = gpd.read_file(f'{osm_layers_dir}/sorocaba_natural.gpkg')
    osm_natural = normalize_crs(osm_natural, "OSM Natural")
    osm_natural['source'] = 'OSM_natural'
    print(f"  Dados OSM Natural carregados: {len(osm_natural)} feições")
    print(f"  Valores únicos de natural: {osm_natural['natural'].unique()[:5]}... (primeiros 5)")
except Exception as e:
    print(f"  Erro ao carregar dados OSM Natural: {e}")
    osm_natural = None

# Dados de áreas urbanas
try:
    urbano = gpd.read_file(f'{processed_dir}/sorocaba_Urbano.gpkg')
    urbano = normalize_crs(urbano, "Urbano")
    urbano['source'] = 'Urbano'
    urbano['landcover'] = 'urban'
    print(f"  Dados Urbano carregados: {len(urbano)} feições")
except Exception as e:
    print(f"  Erro ao carregar dados Urbano: {e}")
    urbano = None

# Dados de áreas rurais
try:
    rural = gpd.read_file(f'{processed_dir}/sorocaba_Rural.gpkg')
    rural = normalize_crs(rural, "Rural")
    rural['source'] = 'Rural'
    rural['landcover'] = 'rural'
    print(f"  Dados Rural carregados: {len(rural)} feições")
except Exception as e:
    print(f"  Erro ao carregar dados Rural: {e}")
    rural = None

# Dados de natureza
try:
    natureza = gpd.read_file(f'{processed_dir}/sorocaba_Natureza.gpkg')
    natureza = normalize_crs(natureza, "Natureza")
    natureza['source'] = 'Natureza'
    natureza['landcover'] = 'nature'
    print(f"  Dados Natureza carregados: {len(natureza)} feições")
except Exception as e:
    print(f"  Erro ao carregar dados Natureza: {e}")
    natureza = None

print("\n1.2. Padronizando esquemas para uso da terra...")

def standardize_landcover(gdf, source_name, layer_type='landcover'):
    if gdf is None:
        return None
    std_gdf = gdf.copy()
    std_gdf['source'] = source_name
    std_gdf['layer_type'] = layer_type
    if source_name == 'OSM_landuse':
        landuse_mapping = {
            'residential': 'urban', 'commercial': 'urban', 'industrial': 'urban',
            'retail': 'urban', 'farmland': 'agriculture', 'farm': 'agriculture',
            'meadow': 'agriculture', 'orchard': 'agriculture', 'vineyard': 'agriculture',
            'forest': 'forest', 'grass': 'vegetation', 'recreation_ground': 'recreation',
            'park': 'recreation', 'cemetery': 'urban', 'brownfield': 'urban',
            'construction': 'urban', 'greenfield': 'agriculture', 'military': 'institutional',
            'railway': 'transportation', 'religious': 'urban', 'reservoir': 'water',
            'quarry': 'industrial', 'landfill': 'industrial'
        }
        std_gdf['class'] = std_gdf['landuse'].map(landuse_mapping)
        std_gdf['class'] = std_gdf['class'].fillna('other')
    elif source_name == 'OSM_natural':
        natural_mapping = {
            'water': 'water', 'wetland': 'water', 'wood': 'forest', 'tree': 'forest',
            'scrub': 'vegetation', 'heath': 'vegetation', 'grassland': 'vegetation',
            'bare_rock': 'bare', 'beach': 'bare', 'sand': 'bare', 'cliff': 'bare'
        }
        std_gdf['class'] = std_gdf['natural'].map(natural_mapping)
        std_gdf['class'] = std_gdf['class'].fillna('other')
    elif source_name == 'Urbano':
        std_gdf['class'] = 'urban'
    elif source_name == 'Rural':
        std_gdf['class'] = 'rural'
    elif source_name == 'Natureza':
        if 'USO2018' in std_gdf.columns:
            uso_mapping = {1: 'forest', 2: 'agriculture', 3: 'pasture', 4: 'water'}
            std_gdf['class'] = std_gdf['USO2018'].map(uso_mapping)
            std_gdf['class'] = std_gdf['class'].fillna('other')
        else:
            std_gdf['class'] = 'nature'
    else:
        std_gdf['class'] = 'other'
    essential_columns = ['geometry', 'source', 'layer_type', 'class']
    if source_name == 'OSM_landuse':
        additional_columns = ['landuse', 'name', 'osm_id']
    elif source_name == 'OSM_natural':
        additional_columns = ['natural', 'name', 'osm_id']
    elif source_name in ['Urbano', 'Rural', 'Natureza']:
        additional_columns = [col for col in std_gdf.columns if col.startswith('USO')]
    else:
        additional_columns = []
    columns_to_keep = essential_columns + [col for col in additional_columns if col in std_gdf.columns]
    return std_gdf[columns_to_keep]

std_osm_landuse = standardize_landcover(osm_landuse, 'OSM_landuse', 'landuse')
std_osm_natural = standardize_landcover(osm_natural, 'OSM_natural', 'natural')
std_urbano = standardize_landcover(urbano, 'Urbano', 'urban')
std_rural = standardize_landcover(rural, 'Rural', 'rural')
std_natureza = standardize_landcover(natureza, 'Natureza', 'nature')

print("\n1.3. Integrando dados de uso da terra e ocupação...")
landcover_sources = [std_osm_landuse, std_osm_natural, std_urbano, std_rural, std_natureza]
valid_landcover_sources = [src for src in landcover_sources if src is not None]
if not valid_landcover_sources:
    print("  Erro: Nenhuma fonte de dados de uso da terra válida disponível.")
else:
    landcover_integrado = pd.concat(valid_landcover_sources, ignore_index=True)
    print(f"  Total de feições de uso da terra integradas: {len(landcover_integrado)}")
    classes = landcover_integrado['class'].value_counts()
    print("\n  Classes de uso da terra presentes nos dados integrados:")
    for cls, count in classes.items():
        print(f"  - {cls}: {count} feições ({count/len(landcover_integrado)*100:.1f}%)")
    landcover_proj = landcover_integrado.to_crs(projected_crs)
    landcover_proj['area_km2'] = landcover_proj.geometry.area / 1_000_000
    area_by_class = landcover_proj.groupby('class')['area_km2'].sum().sort_values(ascending=False)
    print("\n  Área por classe de uso da terra:")
    total_area = area_by_class.sum()
    for cls, area in area_by_class.items():
        print(f"  - {cls}: {area:.2f} km² ({area/total_area*100:.1f}%)")
    print(f"\n  Área total de uso da terra mapeada: {total_area:.2f} km²")
    output_landcover_gpkg = f'{processed_dir}/uso_terra_ocupacao_integrado.gpkg'
    landcover_integrado.to_file(output_landcover_gpkg, layer='uso_terra_ocupacao_integrado')
    print(f"\n  GeoPackage de uso da terra integrado salvo em: {output_landcover_gpkg}")

# ===============================
# 2. INTEGRAÇÃO DE EDIFICAÇÕES E SETORES CENSITÁRIOS
# ===============================
print("\n2. PARTE II - INTEGRAÇÃO DE EDIFICAÇÕES E SETORES CENSITÁRIOS")
print("2.1. Carregando dados de edificações e setores censitários...")

try:
    buildings = gpd.read_file(f'{osm_layers_dir}/sorocaba_buildings.gpkg')
    buildings = normalize_crs(buildings, "Edificações OSM")
    buildings['source'] = 'OSM_buildings'
    print(f"  Dados de edificações carregados: {len(buildings)} feições")
    if 'building' in buildings.columns:
        print(f"  Tipos de edificações mais comuns: {buildings['building'].value_counts().head(5).to_dict()}")
except Exception as e:
    print(f"  Erro ao carregar dados de edificações: {e}")
    buildings = None

try:
    setores = gpd.read_file(f'{processed_dir}/sorocaba_SP_setores_CD2022.gpkg')
    setores = normalize_crs(setores, "Setores Censitários")
    setores['source'] = 'IBGE_setores'
    print(f"  Dados de setores censitários carregados: {len(setores)} feições")
except Exception as e:
    print(f"  Erro ao carregar dados de setores censitários: {e}")
    setores = None

print("\n2.2. Padronizando esquemas para edificações e setores...")

def standardize_buildings(buildings_gdf):
    if buildings_gdf is None:
        return None
    std_gdf = buildings_gdf.copy()
    std_gdf['source'] = 'OSM_buildings'
    std_gdf['layer_type'] = 'buildings'
    building_mapping = {
        'residential': 'residential', 'house': 'residential', 'apartments': 'residential',
        'detached': 'residential', 'commercial': 'commercial', 'retail': 'commercial',
        'office': 'commercial', 'industrial': 'industrial', 'warehouse': 'industrial',
        'factory': 'industrial', 'school': 'educational', 'university': 'educational',
        'college': 'educational', 'hospital': 'health', 'clinic': 'health',
        'public': 'public', 'government': 'public', 'religious': 'religious',
        'church': 'religious', 'temple': 'religious', 'mosque': 'religious',
        'garage': 'service', 'parking': 'service', 'shed': 'service', 'farm': 'agricultural',
        'barn': 'agricultural', 'greenhouse': 'agricultural', 'hotel': 'accommodation',
        'dormitory': 'accommodation', 'roof': 'other', 'construction': 'other',
        'service': 'service', 'yes': 'unspecified'
    }
    std_gdf['building_type'] = std_gdf['building'].map(building_mapping)
    std_gdf['building_type'] = std_gdf['building_type'].fillna('other')
    priority_mapping = {
        'health': 1, 'educational': 2, 'public': 3, 'residential': 4,
        'accommodation': 5, 'commercial': 6, 'religious': 7, 'industrial': 8,
        'service': 9, 'agricultural': 10, 'unspecified': 11, 'other': 12
    }
    std_gdf['priority'] = std_gdf['building_type'].map(priority_mapping)
    columns = ['geometry', 'source', 'layer_type', 'building', 'building_type', 'priority',
               'name', 'osm_id', 'osm_way_id', 'amenity']
    columns_to_keep = [col for col in columns if col in std_gdf.columns]
    return std_gdf[columns_to_keep]

def standardize_sectors(sectors_gdf):
    if sectors_gdf is None:
        return None
    std_gdf = sectors_gdf.copy()
    std_gdf['source'] = 'IBGE_setores'
    std_gdf['layer_type'] = 'census_sectors'
    if 'SITUACAO' in std_gdf.columns:
        situacao_mapping = {
            'Área urbanizada': 'urban', 'Área urbana': 'urban',
            'Urbana': 'urban', 'Área rural': 'rural', 'Rural': 'rural'
        }
        std_gdf['area_type'] = std_gdf['SITUACAO'].astype(str).apply(
            lambda x: next((v for k, v in situacao_mapping.items() if k.lower() in x.lower()), 'other')
        )
        std_gdf['is_urban'] = (std_gdf['area_type'] == 'urban').astype(int)
    essential_columns = ['geometry', 'source', 'layer_type', 'CD_SETOR']
    if 'SITUACAO' in std_gdf.columns:
        essential_columns.append('SITUACAO')
    if 'area_type' in std_gdf.columns:
        essential_columns.append('area_type')
    if 'is_urban' in std_gdf.columns:
        essential_columns.append('is_urban')
    if 'NM_MUN' in std_gdf.columns:
        essential_columns.append('NM_MUN')
    return std_gdf[essential_columns]

std_buildings = standardize_buildings(buildings)
std_sectors = standardize_sectors(setores)

print("\n2.3. Analisando dados socioeconômicos...")
analysis_results = {}
if std_buildings is not None:
    building_stats = {
        'total_count': len(std_buildings),
        'by_type': std_buildings['building_type'].value_counts().to_dict()
    }
    analysis_results['buildings'] = building_stats
    print(f"  Total de edificações: {building_stats['total_count']}")
    print(f"  Tipos mais comuns: {sorted([(k, v) for k, v in building_stats['by_type'].items() if v > 100], key=lambda x: x[1], reverse=True)}")
if std_sectors is not None:
    sectors_proj = std_sectors.to_crs(projected_crs)
    sectors_proj['area_km2'] = sectors_proj.geometry.area / 1_000_000
    sector_stats = {
        'total_count': len(sectors_proj),
        'total_area_km2': sectors_proj['area_km2'].sum(),
        'avg_area_km2': sectors_proj['area_km2'].mean()
    }
    if 'is_urban' in std_sectors.columns:
        urban_count = std_sectors['is_urban'].sum()
        rural_count = len(std_sectors) - urban_count
        sector_stats['urban_count'] = urban_count
        sector_stats['rural_count'] = rural_count
        sector_stats['urban_percentage'] = (urban_count / len(std_sectors)) * 100
    analysis_results['sectors'] = sector_stats
    print(f"  Total de setores censitários: {sector_stats['total_count']}")
    print(f"  Área total: {sector_stats['total_area_km2']:.2f} km²")
    if 'urban_count' in sector_stats:
        print(f"  Setores urbanos: {sector_stats['urban_count']} ({sector_stats['urban_percentage']:.1f}%)")
        print(f"  Setores rurais: {sector_stats['rural_count']} ({100 - sector_stats['urban_percentage']:.1f}%)")

print("\n2.4. Analisando distribuição de edificações por setor...")
if std_buildings is not None and std_sectors is not None:
    try:
        sectors_proj = std_sectors.to_crs(projected_crs)
        buildings_proj = std_buildings.to_crs(projected_crs)
        if 'building_count' not in sectors_proj.columns:
            sectors_proj['building_count'] = 0
        intersection = gpd.sjoin(buildings_proj, sectors_proj, how='inner', predicate='within')
        building_counts = intersection.groupby('CD_SETOR').size()
        for sector_id, count in building_counts.items():
            sectors_proj.loc[sectors_proj['CD_SETOR'] == sector_id, 'building_count'] = count
        if 'area_km2' not in sectors_proj.columns:
            sectors_proj['area_km2'] = sectors_proj.geometry.area / 1_000_000
        sectors_proj['building_density'] = sectors_proj['building_count'] / sectors_proj['area_km2']
        sectors_proj['building_density'] = sectors_proj['building_density'].replace([np.inf, -np.inf], np.nan).fillna(0)
        analysis_results['building_density'] = {
            'max_density': float(sectors_proj['building_density'].max()),
            'min_density': float(sectors_proj['building_density'].min()),
            'mean_density': float(sectors_proj['building_density'].mean()),
            'median_density': float(sectors_proj['building_density'].median())
        }
        print(f"  Densidade média: {analysis_results['building_density']['mean_density']:.2f} edificações/km²")
        print(f"  Densidade máxima: {analysis_results['building_density']['max_density']:.2f} edificações/km²")
    except Exception as e:
        print(f"  Erro ao calcular distribuição de edificações: {e}")
        import traceback
        print(traceback.format_exc())
else:
    print("  Análise de distribuição não realizada - dados insuficientes")

print("\n2.5. Identificando edificações prioritárias...")
if std_buildings is not None:
    try:
        priority_buildings = std_buildings[std_buildings['priority'] <= 3].copy()
        if len(priority_buildings) > 0:
            print(f"  Edificações prioritárias identificadas: {len(priority_buildings)}")
            priority_counts = priority_buildings.groupby('building_type').size()
            for type_name, count in priority_counts.items():
                print(f"  - {type_name}: {count} edificações")
        else:
            print("  Nenhuma edificação prioritária identificada")
    except Exception as e:
        print(f"  Erro ao analisar edificações prioritárias: {e}")
else:
    print("  Análise de edificações prioritárias não realizada - dados insuficientes")

# Salvar dados socioeconômicos integrados
print("\n3. INTEGRANDO TODOS OS DADOS GEOESPACIAIS")
print("3.1. Salvando dados integrados para uso em grafos geoespaciais...")
output_socio_gpkg = f'{processed_dir}/socioeconomico_integrado.gpkg'
if std_buildings is not None:
    std_buildings.to_file(output_socio_gpkg, layer='edificacoes')
    print(f"  - Camada de edificações salva em: {output_socio_gpkg}")
if std_sectors is not None:
    std_sectors.to_file(output_socio_gpkg, layer='setores_censitarios')
    print(f"  - Camada de setores censitários salva em: {output_socio_gpkg}")
if 'sectors_proj' in locals() and 'building_count' in sectors_proj.columns:
    sectors_proj.to_file(output_socio_gpkg, layer='setores_com_edificacoes')
    print(f"  - Camada de setores com contagem de edificações salva em: {output_socio_gpkg}")
if 'priority_buildings' in locals() and len(priority_buildings) > 0:
    priority_buildings.to_file(output_socio_gpkg, layer='edificacoes_prioritarias')
    print(f"  - Camada de edificações prioritárias salva em: {output_socio_gpkg}")

# Criar arquivo GeoPackage final com todas as camadas integradas
final_gpkg = f'{processed_dir}/sorocaba_dados_integrados.gpkg'
print("\n3.2. Criando GeoPackage final com todas as camadas integradas...")
camadas_integracao = [
    {'gpkg': output_landcover_gpkg, 'layer': 'uso_terra_ocupacao_integrado', 'new_layer': 'uso_terra_ocupacao'},
    {'gpkg': output_socio_gpkg, 'layer': 'edificacoes', 'new_layer': 'edificacoes'},
    {'gpkg': output_socio_gpkg, 'layer': 'setores_censitarios', 'new_layer': 'setores_censitarios'},
    {'gpkg': output_socio_gpkg, 'layer': 'setores_com_edificacoes', 'new_layer': 'setores_com_edificacoes'},
    {'gpkg': output_socio_gpkg, 'layer': 'edificacoes_prioritarias', 'new_layer': 'edificacoes_prioritarias'},
    {'gpkg': f'{processed_dir}/hidrografia_integrada.gpkg', 'layer': 'hidrografia_integrada', 'new_layer': 'hidrografia'}
]
for camada in camadas_integracao:
    try:
        if not os.path.exists(camada['gpkg']):
            print(f"  Aviso: Arquivo {camada['gpkg']} não encontrado. Pulando camada {camada['layer']}.")
            continue
        layers = fiona.listlayers(camada['gpkg'])
        if camada['layer'] not in layers:
            print(f"  Aviso: Camada {camada['layer']} não encontrada em {camada['gpkg']}. Pulando.")
            continue
        gdf = gpd.read_file(camada['gpkg'], layer=camada['layer'])
        gdf.to_file(final_gpkg, layer=camada['new_layer'])
        print(f"  - Camada {camada['new_layer']} adicionada ao arquivo integrado final")
    except Exception as e:
        print(f"  Erro ao processar camada {camada['layer']}: {e}")

print("\n3.3. Salvando resultados das análises...")
analysis_json_path = os.path.join(csv_dir, "analise_integrada.json")
with open(analysis_json_path, 'w') as f:
    json.dump(convert_numpy_types(analysis_results), f, indent=2)
print(f"  - Resultados das análises salvos em: {analysis_json_path}")

# ===============================
# 4. CRIAÇÃO DE VISUALIZAÇÕES INTEGRADAS FINAIS
# ===============================
print("\n4. Criando visualizações integradas finais...")

# 4.1 Visualização integrada do uso da terra com edificações prioritárias
try:
    print("4.1. Gerando visualização integrada de uso da terra e edificações prioritárias...")
    fig, ax = plt.subplots(figsize=(15, 12))
    if 'landcover_integrado' in locals():
        class_colors = {
            'urban': '#FF0000', 'rural': '#F5DEB3', 'agriculture': '#FFD700',
            'forest': '#008000', 'vegetation': '#90EE90', 'water': '#1E90FF',
            'recreation': '#32CD32', 'industrial': '#A52A2A', 'transportation': '#808080',
            'institutional': '#FF8C00', 'bare': '#D2B48C', 'nature': '#006400',
            'pasture': '#7CFC00', 'other': '#000000'
        }
        for class_name, color in class_colors.items():
            subset = landcover_integrado[landcover_integrado['class'] == class_name]
            if not subset.empty:
                subset.plot(ax=ax, color=color, alpha=0.6)
    if 'priority_buildings' in locals() and not priority_buildings.empty:
        priority_colors = {'health': 'red', 'educational': 'blue', 'public': 'purple'}
        legend_patches = []
        for p_type, color in priority_colors.items():
            subset = priority_buildings[priority_buildings['building_type'] == p_type]
            if not subset.empty:
                subset.plot(ax=ax, color=color, markersize=30, alpha=0.8)
                legend_patches.append(Patch(facecolor=color, edgecolor='black', alpha=0.8, label=p_type))
        if legend_patches:
            ax.legend(handles=legend_patches, title='Edificações Prioritárias', loc='upper left')
    try:
        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
    except Exception as e:
        print(f"  Aviso: Não foi possível adicionar mapa base: {e}")
    ax.set_title('Uso da Terra e Edificações Prioritárias - Sorocaba', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{plots_dir}/uso_terra_edificacoes_prioritarias.png', dpi=300, bbox_inches='tight')
    print(f"  - Visualização salva em: {plots_dir}/uso_terra_edificacoes_prioritarias.png")
    plt.close(fig)
except Exception as e:
    print(f"  Erro ao criar visualização integrada de uso da terra e edificações: {e}")
    import traceback
    print(traceback.format_exc())

# 4.2 Visualização integrada de densidade de edificações por setor
try:
    print("4.2. Gerando visualização de densidade de edificações por setor...")
    if 'sectors_proj' in locals() and 'building_density' in sectors_proj.columns:
        fig, ax = plt.subplots(figsize=(15, 12))
        vmax = sectors_proj['building_density'].quantile(0.95)
        sectors_proj.plot(column='building_density', ax=ax, legend=True,
                          cmap='YlOrRd', vmin=0, vmax=vmax, alpha=0.7)
        try:
            ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=sectors_proj.crs)
        except Exception as e:
            print(f"  Aviso: Não foi possível adicionar mapa base: {e}")
        ax.set_title('Densidade de Edificações por Setor Censitário - Sorocaba', fontsize=14)
        plt.tight_layout()
        plt.savefig(f'{plots_dir}/densidade_edificacoes_por_setor.png', dpi=300, bbox_inches='tight')
        print(f"  - Visualização salva em: {plots_dir}/densidade_edificacoes_por_setor.png")
        plt.close(fig)
    else:
        print("  - Dados de densidade de edificações não disponíveis para visualização")
except Exception as e:
    print(f"  Erro ao criar visualização de densidade de edificações: {e}")
    import traceback
    print(traceback.format_exc())

# 4.3 Mapa de síntese final (topografia + hidrografia + uso da terra + edificações)
try:
    print("4.3. Gerando mapa de síntese final com múltiplas camadas...")
    fig, ax = plt.subplots(figsize=(15, 12))
    try:
        curvas_nivel = gpd.read_file(f'{processed_dir}/sorocaba_curvas_nivel_50m.gpkg')
        if not curvas_nivel.empty:
            curvas_nivel.plot(ax=ax, column='elevation', cmap='terrain', alpha=0.3, linewidth=0.5)
            print("  - Camada de topografia adicionada")
    except Exception as e:
        print(f"  Aviso: Curvas de nível não disponíveis: {e}")
    try:
        hidrografia = gpd.read_file(f'{processed_dir}/hidrografia_integrada.gpkg', layer='hidrografia_integrada')
        if not hidrografia.empty:
            hidrografia.plot(ax=ax, color='blue', linewidth=0.8, alpha=0.7)
            print("  - Camada de hidrografia adicionada")
    except Exception as e:
        print(f"  Aviso: Hidrografia não disponível: {e}")
    if 'landcover_integrado' in locals():
        landcover_groups = {
            'urban_group': ['urban', 'industrial', 'institutional', 'transportation'],
            'vegetation_group': ['forest', 'vegetation', 'nature', 'recreation', 'pasture'],
            'agriculture_group': ['agriculture', 'rural'],
            'water_group': ['water'],
            'other_group': ['bare', 'other']
        }
        group_colors = {
            'urban_group': '#FF6347', 'vegetation_group': '#228B22',
            'agriculture_group': '#DAA520', 'water_group': '#1E90FF',
            'other_group': '#A9A9A9'
        }
        landcover_integrado['group'] = 'other_group'
        for group_name, classes in landcover_groups.items():
            landcover_integrado.loc[landcover_integrado['class'].isin(classes), 'group'] = group_name
        for group_name, color in group_colors.items():
            subset = landcover_integrado[landcover_integrado['group'] == group_name]
            if not subset.empty:
                subset.plot(ax=ax, color=color, alpha=0.5)
        print("  - Camada de uso da terra adicionada")
    if 'priority_buildings' in locals() and not priority_buildings.empty:
        priority_buildings.plot(ax=ax, color='red', markersize=20, alpha=0.8)
        print("  - Camada de edificações prioritárias adicionada")
    try:
        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
        print("  - Mapa base adicionado")
    except Exception as e:
        print(f"  Aviso: Não foi possível adicionar mapa base: {e}")
    ax.set_title('Mapa Síntese - Sorocaba', fontsize=16)
    plt.tight_layout()
    plt.savefig(f'{plots_dir}/mapa_sintese_final.png', dpi=300, bbox_inches='tight')
    print(f"  - Mapa de síntese salvo em: {plots_dir}/mapa_sintese_final.png")
    plt.close(fig)
except Exception as e:
    print(f"  Erro ao criar mapa de síntese final: {e}")
    import traceback
    print(traceback.format_exc())

# ===============================
# 5. INTEGRAÇÃO DA REDE DE TRANSPORTES
# ===============================
print("\n5. PARTE III - INTEGRAÇÃO DA REDE DE TRANSPORTES")
print("5.1. Carregando dados de rodovias e ferrovias...")

# Dados de rodovias do OSM
try:
    roads = gpd.read_file(f'{osm_layers_dir}/sorocaba_roads.gpkg')
    roads = normalize_crs(roads, "Rodovias OSM")
    roads['source'] = 'OSM_roads'
    roads['layer_type'] = 'transportation'
    print(f"  Dados de rodovias carregados: {len(roads)} feições")
    if 'highway' in roads.columns:
        print(f"  Tipos de rodovias mais comuns: {roads['highway'].value_counts().head(5).to_dict()}")
except Exception as e:
    print(f"  Erro ao carregar dados de rodovias: {e}")
    roads = None

# Dados de ferrovias do OSM
try:
    railways = gpd.read_file(f'{osm_layers_dir}/sorocaba_railway.gpkg')
    railways = normalize_crs(railways, "Ferrovias OSM")
    railways['source'] = 'OSM_railways'
    railways['layer_type'] = 'transportation'
    print(f"  Dados de ferrovias carregados: {len(railways)} feições")
    if 'railway' in railways.columns:
        print(f"  Tipos de ferrovias mais comuns: {railways['railway'].value_counts().head(5).to_dict()}")
except Exception as e:
    print(f"  Erro ao carregar dados de ferrovias: {e}")
    railways = None

print("\n5.2. Padronizando esquemas para rede de transportes...")

def standardize_roads(roads_gdf):
    if roads_gdf is None:
        return None
    std_gdf = roads_gdf.copy()
    std_gdf['source'] = 'OSM_roads'
    std_gdf['layer_type'] = 'transportation'
    if 'highway' in std_gdf.columns:
        highway_mapping = {
            'motorway': 'primary', 'trunk': 'primary', 'primary': 'primary',
            'secondary': 'secondary', 'tertiary': 'secondary', 'residential': 'tertiary',
            'service': 'tertiary', 'unclassified': 'tertiary', 'track': 'tertiary',
            'path': 'tertiary', 'footway': 'pedestrian', 'pedestrian': 'pedestrian',
            'cycleway': 'pedestrian', 'steps': 'pedestrian'
        }
        std_gdf['road_class'] = std_gdf['highway'].map(highway_mapping)
        std_gdf['road_class'] = std_gdf['road_class'].fillna('other')
        priority_mapping = {
            'primary': 1, 'secondary': 2, 'tertiary': 3, 'pedestrian': 4, 'other': 5
        }
        std_gdf['priority'] = std_gdf['road_class'].map(priority_mapping)
    columns = ['geometry', 'source', 'layer_type', 'highway', 'road_class', 'priority', 'name', 'osm_id']
    columns_to_keep = [col for col in columns if col in std_gdf.columns]
    return std_gdf[columns_to_keep]

def standardize_railways(railways_gdf):
    if railways_gdf is None:
        return None
    std_gdf = railways_gdf.copy()
    std_gdf['source'] = 'OSM_railways'
    std_gdf['layer_type'] = 'transportation'
    if 'railway' in std_gdf.columns:
        railway_mapping = {
            'rail': 'main', 'subway': 'metro', 'tram': 'tram', 'light_rail': 'light',
            'monorail': 'special', 'narrow_gauge': 'special', 'disused': 'inactive', 'abandoned': 'inactive'
        }
        std_gdf['rail_class'] = std_gdf['railway'].map(railway_mapping)
        std_gdf['rail_class'] = std_gdf['rail_class'].fillna('other')
    columns = ['geometry', 'source', 'layer_type', 'railway', 'rail_class', 'name', 'osm_id']
    columns_to_keep = [col for col in columns if col in std_gdf.columns]
    return std_gdf[columns_to_keep]

std_roads = standardize_roads(roads)
std_railways = standardize_railways(railways)

print("\n5.3. Analisando rede de transportes...")
transportation_results = {}
if std_roads is not None:
    roads_proj = std_roads.to_crs(projected_crs)
    roads_proj['length_m'] = roads_proj.geometry.length
    road_stats = {
        'total_count': len(roads_proj),
        'total_length_km': roads_proj['length_m'].sum() / 1000,
        'by_class': {}
    }
    if 'road_class' in roads_proj.columns:
        for class_name, group in roads_proj.groupby('road_class'):
            road_stats['by_class'][class_name] = {
                'count': len(group),
                'length_km': group['length_m'].sum() / 1000
            }
    transportation_results['roads'] = road_stats
    print(f"  Total de rodovias: {road_stats['total_count']} feições")
    print(f"  Comprimento total da rede viária: {road_stats['total_length_km']:.2f} km")
    if road_stats['by_class']:
        print("  Distribuição por classe:")
        for class_name, stats in sorted(road_stats['by_class'].items(), key=lambda x: x[1]['length_km'], reverse=True):
            print(f"    - {class_name}: {stats['count']} feições, {stats['length_km']:.2f} km")
if std_railways is not None:
    railways_proj = std_railways.to_crs(projected_crs)
    railways_proj['length_m'] = railways_proj.geometry.length
    railway_stats = {
        'total_count': len(railways_proj),
        'total_length_km': railways_proj['length_m'].sum() / 1000,
        'by_class': {}
    }
    if 'rail_class' in railways_proj.columns:
        for class_name, group in railways_proj.groupby('rail_class'):
            railway_stats['by_class'][class_name] = {
                'count': len(group),
                'length_km': group['length_m'].sum() / 1000
            }
    transportation_results['railways'] = railway_stats
    print(f"  Total de ferrovias: {railway_stats['total_count']} feições")
    print(f"  Comprimento total da rede ferroviária: {railway_stats['total_length_km']:.2f} km")
    if railway_stats['by_class']:
        print("  Distribuição por classe:")
        for class_name, stats in sorted(railway_stats['by_class'].items(), key=lambda x: x[1]['length_km'], reverse=True):
            print(f"    - {class_name}: {stats['count']} feições, {stats['length_km']:.2f} km")

print("\n5.4. Salvando dados de transportes para uso em grafos geoespaciais...")
output_transport_gpkg = f'{processed_dir}/transportes_integrado.gpkg'
if std_roads is not None:
    std_roads.to_file(output_transport_gpkg, layer='rodovias')
    print(f"  - Camada de rodovias salva em: {output_transport_gpkg}")
if std_railways is not None:
    std_railways.to_file(output_transport_gpkg, layer='ferrovias')
    print(f"  - Camada de ferrovias salva em: {output_transport_gpkg}")
try:
    if std_roads is not None:
        std_roads.to_file(final_gpkg, layer='rodovias')
        print(f"  - Camada de rodovias adicionada ao arquivo integrado final")
    if std_railways is not None:
        std_railways.to_file(final_gpkg, layer='ferrovias')
        print(f"  - Camada de ferrovias adicionada ao arquivo integrado final")
except Exception as e:
    print(f"  Erro ao adicionar camadas de transporte ao arquivo final: {e}")

analysis_results['transportation'] = transportation_results
analysis_json_path = os.path.join(csv_dir, "analise_integrada.json")
with open(analysis_json_path, 'w') as f:
    json.dump(convert_numpy_types(analysis_results), f, indent=2)
print(f"  - Resultados das análises atualizados em: {analysis_json_path}")

print("\n5.5. Gerando visualização da rede de transportes...")
try:
    fig, ax = plt.subplots(figsize=(15, 12))
    if 'landcover_integrado' in locals():
        landcover_groups = {
            'urban_group': ['urban', 'industrial', 'institutional', 'transportation'],
            'vegetation_group': ['forest', 'vegetation', 'nature', 'recreation', 'pasture'],
            'agriculture_group': ['agriculture', 'rural'],
            'water_group': ['water'],
            'other_group': ['bare', 'other']
        }
        group_colors = {
            'urban_group': '#E0E0E0', 'vegetation_group': '#F0F8F0',
            'agriculture_group': '#F8F8E0', 'water_group': '#E6F0FF',
            'other_group': '#F8F8F8'
        }
        if 'group' not in landcover_integrado.columns:
            landcover_integrado['group'] = 'other_group'
            for group_name, classes in landcover_groups.items():
                landcover_integrado.loc[landcover_integrado['class'].isin(classes), 'group'] = group_name
        for group_name, color in group_colors.items():
            subset = landcover_integrado[landcover_integrado['group'] == group_name]
            if not subset.empty:
                subset.plot(ax=ax, color=color, alpha=0.3)
    if std_railways is not None:
        std_railways.plot(ax=ax, color='black', linewidth=1.5, label='Ferrovias')
    if std_roads is not None and 'road_class' in std_roads.columns:
        road_colors = {
            'primary': ('#FF0000', 1.5),
            'secondary': ('#FFA500', 1.0),
            'tertiary': ('#FFFF00', 0.5),
            'pedestrian': ('#00FF00', 0.3),
            'other': ('#808080', 0.3)
        }
        legend_patches = []
        for class_name, (color, width) in road_colors.items():
            subset = std_roads[std_roads['road_class'] == class_name]
            if not subset.empty:
                subset.plot(ax=ax, color=color, linewidth=width)
                legend_patches.append(Patch(facecolor=color, edgecolor='black', label=f'Rodovias: {class_name}'))
        if std_railways is not None:
            legend_patches.append(Patch(facecolor='black', edgecolor='black', label='Ferrovias'))
        if legend_patches:
            ax.legend(handles=legend_patches, title='Rede de Transportes', loc='upper left')
    try:
        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=target_crs)
    except Exception as e:
        print(f"  Aviso: Não foi possível adicionar mapa base: {e}")
    ax.set_title('Rede de Transportes - Sorocaba', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'{plots_dir}/rede_transportes.png', dpi=300, bbox_inches='tight')
    print(f"  - Visualização salva em: {plots_dir}/rede_transportes.png")
    plt.close(fig)
except Exception as e:
    print(f"  Erro ao criar visualização da rede de transportes: {e}")
    import traceback
    print(traceback.format_exc())

print("\nProcessamento completo. Todos os dados geoespaciais foram integrados com sucesso.")
print(f"Arquivo final com todas as camadas: {final_gpkg}")
print(f"Visualizações disponíveis no diretório: {plots_dir}")
print(f"Dados para análise em CSV/JSON disponíveis em: {csv_dir}")

# Integração de Dados de Altitude nas Camadas
import os
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
import rasterio
from rasterio.transform import from_origin
from rasterio.features import rasterize
from shapely.geometry import Point, mapping
import fiona

print("\n6. INTEGRAÇÃO DE DADOS DE ALTITUDE NAS CAMADAS")
print("6.1. Carregando curvas de nível...")

# Definir diretórios e arquivos
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = f'{base_dir}/data/terreno/processados'
plots_dir = f'{base_dir}/data/terreno/plots'
curvas_nivel_path = f'{base_dir}/data/terreno/curvas_nivel_50m.gpkg'
final_gpkg = f'{processed_dir}/sorocaba_dados_integrados.gpkg'

# Sistemas de coordenadas
target_crs = 'EPSG:4674'     # CRS geográfico
projected_crs = 'EPSG:31983'  # CRS projetado para cálculos

# Carregar curvas de nível
try:
    curvas_nivel = gpd.read_file(curvas_nivel_path)
    # Se não tiver CRS, definir como target_crs
    if curvas_nivel.crs is None:
        print("  Curvas de nível não possuem CRS definido. Definindo como EPSG:4674.")
        curvas_nivel.set_crs(target_crs, inplace=True)
    # Transformar para CRS projetado
    curvas_nivel = curvas_nivel.to_crs(projected_crs)
    print(f"  Curvas de nível carregadas: {len(curvas_nivel)} feições")
    print(f"  Intervalo de elevação: {curvas_nivel['elevation'].min()} - {curvas_nivel['elevation'].max()} metros")
except Exception as e:
    print(f"  Erro ao carregar curvas de nível: {e}")
    import traceback
    print(traceback.format_exc())
    print("  Não é possível prosseguir sem os dados de altitude.")
    exit()

print("\n6.2. Criando Modelo Digital de Elevação (MDE) a partir das curvas de nível...")

def create_dem_from_contours(contours_gdf, resolution=30, method='linear'):
    """
    Cria um Modelo Digital de Elevação a partir de curvas de nível.
    Parâmetros:
      contours_gdf (GeoDataFrame): Curvas de nível com coluna 'elevation'
      resolution (int): Resolução do MDE em metros
      method (str): Método de interpolação ('linear', 'cubic', 'nearest')
    Retorna:
      tuple: (array 2D com elevação, transform rasterio, extent)
    """
    # Obter a extensão
    minx, miny, maxx, maxy = contours_gdf.total_bounds
    print(f"  Extensão do MDE: minx={minx}, miny={miny}, maxx={maxx}, maxy={maxy}")

    # Criar grades de pontos
    x_range = np.arange(minx, maxx + resolution, resolution)
    y_range = np.arange(maxy, miny - resolution, -resolution)  # Ordem invertida para rasterio
    grid_x, grid_y = np.meshgrid(x_range, y_range)

    # Extrair pontos das curvas
    points = []
    values = []
    for idx, row in contours_gdf.iterrows():
        geom = row.geometry
        elevation = row['elevation']
        if geom.geom_type == 'LineString':
            for coord in geom.coords:
                points.append(coord)
                values.append(elevation)
        elif geom.geom_type == 'MultiLineString':
            for line in geom.geoms:
                for coord in line.coords:
                    points.append(coord)
                    values.append(elevation)

    points = np.array(points)
    values = np.array(values)

    print(f"  Interpolando {len(points)} pontos usando método '{method}'...")
    grid_z = griddata(points, values, (grid_x, grid_y), method=method, fill_value=np.nan)
    # Preencher NaN com o valor mínimo encontrado (ajuste conforme necessário)
    min_valid = np.nanmin(grid_z)
    grid_z = np.nan_to_num(grid_z, nan=min_valid)

    transform = from_origin(minx, maxy, resolution, resolution)
    extent = (minx, maxx, miny, maxy)
    return grid_z, transform, extent

# Criar MDE com resolução ajustada (por exemplo, 50 metros)
try:
    dem_data, dem_transform, dem_extent = create_dem_from_contours(curvas_nivel, resolution=50)
    print(f"  MDE criado com sucesso. Dimensões: {dem_data.shape}")

    # Salvar MDE como GeoTIFF (opcional)
    dem_path = f'{processed_dir}/mde_sorocaba.tif'
    with rasterio.open(
        dem_path,
        'w',
        driver='GTiff',
        height=dem_data.shape[0],
        width=dem_data.shape[1],
        count=1,
        dtype=dem_data.dtype,
        crs=projected_crs,
        transform=dem_transform,
    ) as dst:
        dst.write(dem_data, 1)
    print(f"  MDE salvo em: {dem_path}")

    # Visualizar o MDE
    fig, ax = plt.subplots(figsize=(12, 10))
    img = ax.imshow(dem_data, extent=dem_extent, cmap='terrain', origin='upper')
    plt.colorbar(img, ax=ax, label='Elevação (m)')
    ax.set_title('Modelo Digital de Elevação - Sorocaba')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    plt.savefig(f'{plots_dir}/mde_sorocaba.png', dpi=300, bbox_inches='tight')
    plt.close(fig)
except Exception as e:
    print(f"  Erro ao criar MDE: {e}")
    import traceback
    print(traceback.format_exc())
    print("  Utilizando abordagem alternativa para obtenção de altitude.")
    dem_data = None

print("\n6.3. Adicionando informações de altitude às camadas existentes...")

def get_elevation_from_dem(x, y, dem_data, dem_transform, dem_extent):
    # Verificar se o ponto está dentro da extensão do MDE
    minx, maxx, miny, maxy = dem_extent
    if x < minx or x > maxx or y < miny or y > maxy:
        return None
    # Converter coordenadas para índices
    row, col = ~dem_transform * (x, y)
    row, col = int(row), int(col)
    if 0 <= row < dem_data.shape[0] and 0 <= col < dem_data.shape[1]:
        return dem_data[row, col]
    else:
        return None

def get_elevation_from_nearest_contour(point, contours_gdf, max_distance=500):
    distances = contours_gdf.geometry.distance(point)
    idx = distances.idxmin()
    min_distance = distances[idx]
    if min_distance <= max_distance:
        return contours_gdf.iloc[idx]['elevation'], min_distance
    else:
        return None, min_distance

def add_elevation_to_layer(layer_gdf, dem_data=None, dem_transform=None, dem_extent=None, contours_gdf=None):
    gdf = layer_gdf.copy()
    if gdf.crs != projected_crs:
        gdf = gdf.to_crs(projected_crs)
    gdf['elevation'] = np.nan
    gdf['sample_point'] = gdf.geometry.centroid
    if dem_data is not None:
        print("  Usando MDE para extração de altitude...")
        for idx, row in gdf.iterrows():
            x, y = row['sample_point'].x, row['sample_point'].y
            elevation = get_elevation_from_dem(x, y, dem_data, dem_transform, dem_extent)
            if elevation is not None:
                gdf.at[idx, 'elevation'] = elevation
    if contours_gdf is not None:
        print("  Verificando pontos sem elevação usando curvas de nível mais próximas...")
        missing_count = gdf['elevation'].isna().sum()
        if missing_count > 0:
            print(f"  Processando {missing_count} pontos sem elevação...")
            for idx, row in gdf[gdf['elevation'].isna()].iterrows():
                elevation, distance = get_elevation_from_nearest_contour(row['sample_point'], contours_gdf)
                if elevation is not None:
                    gdf.at[idx, 'elevation'] = elevation
    gdf = gdf.drop(columns=['sample_point'])
    missing = gdf['elevation'].isna().sum()
    total = len(gdf)
    if total > 0:
        coverage = (total - missing) / total * 100
        print(f"  Atribuição de elevação: {total - missing}/{total} feições ({coverage:.1f}%)")
        if not gdf['elevation'].isna().all():
            print(f"  Intervalo de elevação: {gdf['elevation'].min():.1f} - {gdf['elevation'].max():.1f} metros")
    return gdf

# Listar camadas disponíveis no arquivo integrado
try:
    available_layers = fiona.listlayers(final_gpkg)
    print(f"  Camadas disponíveis no arquivo integrado: {available_layers}")
except Exception as e:
    print(f"  Erro ao listar camadas do arquivo integrado: {e}")
    available_layers = []

layers_to_process = [
    'uso_terra_ocupacao',
    'edificacoes',
    'edificacoes_prioritarias',
    'rodovias',
    'ferrovias',
    'hidrografia'
]
layers_to_process = [layer for layer in layers_to_process if layer in available_layers]

for layer_name in layers_to_process:
    try:
        print(f"\n  Processando camada: {layer_name}")
        layer_gdf = gpd.read_file(final_gpkg, layer=layer_name)
        print(f"  Carregados {len(layer_gdf)} feições")
        layer_with_elevation = add_elevation_to_layer(
            layer_gdf,
            dem_data=dem_data,
            dem_transform=dem_transform,
            dem_extent=dem_extent,
            contours_gdf=curvas_nivel
        )
        layer_with_elevation.to_file(final_gpkg, layer=f"{layer_name}_com_elevacao")
        print(f"  Camada com elevação salva como: {layer_name}_com_elevacao")
    except Exception as e:
        print(f"  Erro ao processar camada {layer_name}: {e}")
        import traceback
        print(traceback.format_exc())

print("\n6.4. Criando visualização 3D da topografia com camadas principais...")

try:
    # Importar ferramenta para gráficos 3D
    from mpl_toolkits.mplot3d import Axes3D

    visual_layers = []
    if 'edificacoes_prioritarias_com_elevacao' in available_layers:
        edificacoes_prior = gpd.read_file(final_gpkg, layer='edificacoes_prioritarias_com_elevacao')
        if not edificacoes_prior['elevation'].isna().all():
            visual_layers.append({
                'data': edificacoes_prior,
                'name': 'Edificações Prioritárias',
                'color': 'red',
                'marker': 'o',
                'size': 50
            })
    if 'rodovias_com_elevacao' in available_layers:
        rodovias = gpd.read_file(final_gpkg, layer='rodovias_com_elevacao')
        if 'road_class' in rodovias.columns and not rodovias['elevation'].isna().all():
            rodovias_principais = rodovias[rodovias['road_class'] == 'primary']
            if len(rodovias_principais) > 0:
                visual_layers.append({
                    'data': rodovias_principais,
                    'name': 'Rodovias Principais',
                    'color': 'black',
                    'marker': '.',
                    'size': 10
                })
    if 'hidrografia_com_elevacao' in available_layers:
        hidrografia = gpd.read_file(final_gpkg, layer='hidrografia_com_elevacao')
        if not hidrografia['elevation'].isna().all():
            visual_layers.append({
                'data': hidrografia,
                'name': 'Hidrografia',
                'color': 'blue',
                'marker': '.',
                'size': 10
            })

    if visual_layers:
        print("  Criando visualização 3D...")
        fig = plt.figure(figsize=(14, 12))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_title('Visualização 3D - Sorocaba', fontsize=16)
        ax.set_xlabel('X (Este)')
        ax.set_ylabel('Y (Norte)')
        ax.set_zlabel('Elevação (m)')
        for layer in visual_layers:
            gdf = layer['data']
            x = gdf.geometry.centroid.x
            y = gdf.geometry.centroid.y
            z = gdf['elevation']
            ax.scatter(x, y, z,
                       color=layer['color'],
                       label=layer['name'],
                       marker=layer['marker'],
                       s=layer['size'],
                       alpha=0.7)
        ax.legend()
        ax.view_init(elev=30, azim=225)
        plt.savefig(f'{plots_dir}/visualizacao_3d_sorocaba.png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"  Visualização 3D salva em: {plots_dir}/visualizacao_3d_sorocaba.png")
    else:
        print("  Não há camadas suficientes com dados de elevação para criar visualização 3D.")
except Exception as e:
    print(f"  Erro ao criar visualização 3D: {e}")
    import traceback
    print(traceback.format_exc())

print("\nIntegração de dados de altitude concluída.")

import os
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
import rasterio
from rasterio.transform import from_origin
from shapely.geometry import Point
import fiona

print("\n6. INTEGRAÇÃO DE DADOS DE ALTITUDE NAS CAMADAS")
print("6.1. Carregando curvas de nível...")

# Definir diretórios e arquivos
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = os.path.join(base_dir, 'data/terreno/processados')
plots_dir = os.path.join(base_dir, 'data/terreno/plots')
curvas_nivel_path = os.path.join(base_dir, 'data/terreno/curvas_nivel_50m.gpkg')
# Arquivo integrado original (já existente)
final_gpkg = os.path.join(processed_dir, 'sorocaba_dados_integrados.gpkg')
# Novo arquivo integrado que conterá TODAS as camadas (atualizadas ou copiadas) + a camada de curvas de nível
final_gpkg_alt = os.path.join(processed_dir, 'sorocaba_dados_integrados_alt.gpkg')

# Sistemas de coordenadas
target_crs = 'EPSG:4674'     # CRS geográfico
projected_crs = 'EPSG:31983'  # CRS projetado para cálculos

# Carregar curvas de nível
try:
    curvas_nivel = gpd.read_file(curvas_nivel_path)
    if curvas_nivel.crs is None:
        print("  Curvas de nível não possuem CRS definido. Definindo como EPSG:4674.")
        curvas_nivel.set_crs(target_crs, inplace=True)
    curvas_nivel = curvas_nivel.to_crs(projected_crs)
    print(f"  Curvas de nível carregadas: {len(curvas_nivel)} feições")
    print(f"  Intervalo de elevação: {curvas_nivel['elevation'].min()} - {curvas_nivel['elevation'].max()} metros")
except Exception as e:
    print(f"  Erro ao carregar curvas de nível: {e}")
    exit()

print("\n6.2. Criando Modelo Digital de Elevação (MDE) a partir das curvas de nível...")

def create_dem_from_contours(contours_gdf, resolution=30, method='linear'):
    """
    Cria um Modelo Digital de Elevação a partir de curvas de nível.
    """
    minx, miny, maxx, maxy = contours_gdf.total_bounds
    print(f"  Extensão do MDE: minx={minx}, miny={miny}, maxx={maxx}, maxy={maxy}")

    x_range = np.arange(minx, maxx + resolution, resolution)
    y_range = np.arange(maxy, miny - resolution, -resolution)
    grid_x, grid_y = np.meshgrid(x_range, y_range)

    points = []
    values = []
    for idx, row in contours_gdf.iterrows():
        geom = row.geometry
        elevation = row['elevation']
        if geom.geom_type == 'LineString':
            for coord in geom.coords:
                points.append(coord)
                values.append(elevation)
        elif geom.geom_type == 'MultiLineString':
            for line in geom.geoms:
                for coord in line.coords:
                    points.append(coord)
                    values.append(elevation)
    points = np.array(points)
    values = np.array(values)

    print(f"  Interpolando {len(points)} pontos usando método '{method}'...")
    grid_z = griddata(points, values, (grid_x, grid_y), method=method, fill_value=np.nan)
    min_valid = np.nanmin(grid_z)
    grid_z = np.nan_to_num(grid_z, nan=min_valid)

    transform = from_origin(minx, maxy, resolution, resolution)
    extent = (minx, maxx, miny, maxy)
    return grid_z, transform, extent

try:
    dem_data, dem_transform, dem_extent = create_dem_from_contours(curvas_nivel, resolution=50)
    print(f"  MDE criado com sucesso. Dimensões: {dem_data.shape}")

    dem_path = os.path.join(processed_dir, 'mde_sorocaba.tif')
    with rasterio.open(
        dem_path,
        'w',
        driver='GTiff',
        height=dem_data.shape[0],
        width=dem_data.shape[1],
        count=1,
        dtype=dem_data.dtype,
        crs=projected_crs,
        transform=dem_transform,
    ) as dst:
        dst.write(dem_data, 1)
    print(f"  MDE salvo em: {dem_path}")

    fig, ax = plt.subplots(figsize=(12, 10))
    img = ax.imshow(dem_data, extent=dem_extent, cmap='terrain', origin='upper')
    plt.colorbar(img, ax=ax, label='Elevação (m)')
    ax.set_title('Modelo Digital de Elevação - Sorocaba')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    plt.savefig(os.path.join(plots_dir, 'mde_sorocaba.png'), dpi=300, bbox_inches='tight')
    plt.close(fig)
except Exception as e:
    print(f"  Erro ao criar MDE: {e}")
    dem_data = None

print("\n6.3. Adicionando informações de altitude às camadas existentes...")

def get_elevation_from_dem(x, y, dem_data, dem_transform, dem_extent):
    minx, maxx, miny, maxy = dem_extent
    if x < minx or x > maxx or y < miny or y > maxy:
        return None
    row, col = ~dem_transform * (x, y)
    row, col = int(row), int(col)
    if 0 <= row < dem_data.shape[0] and 0 <= col < dem_data.shape[1]:
        return dem_data[row, col]
    else:
        return None

def get_elevation_from_nearest_contour(point, contours_gdf, max_distance=500):
    distances = contours_gdf.geometry.distance(point)
    idx = distances.idxmin()
    min_distance = distances[idx]
    if min_distance <= max_distance:
        return contours_gdf.iloc[idx]['elevation'], min_distance
    else:
        return None, min_distance

def add_elevation_to_layer(layer_gdf, dem_data=None, dem_transform=None, dem_extent=None, contours_gdf=None):
    gdf = layer_gdf.copy()
    if gdf.crs != projected_crs:
        gdf = gdf.to_crs(projected_crs)
    # Tentar adicionar elevação apenas se a camada possuir geometria pontual ou poligonal
    gdf['elevation'] = np.nan
    gdf['sample_point'] = gdf.geometry.centroid
    if dem_data is not None:
        print("  Usando MDE para extração de altitude...")
        for idx, row in gdf.iterrows():
            x, y = row['sample_point'].x, row['sample_point'].y
            elevation = get_elevation_from_dem(x, y, dem_data, dem_transform, dem_extent)
            if elevation is not None:
                gdf.at[idx, 'elevation'] = elevation
    if contours_gdf is not None:
        print("  Verificando pontos sem elevação usando curvas de nível mais próximas...")
        missing_count = gdf['elevation'].isna().sum()
        if missing_count > 0:
            print(f"  Processando {missing_count} pontos sem elevação...")
            for idx, row in gdf[gdf['elevation'].isna()].iterrows():
                elevation, _ = get_elevation_from_nearest_contour(row['sample_point'], contours_gdf)
                if elevation is not None:
                    gdf.at[idx, 'elevation'] = elevation
    gdf = gdf.drop(columns=['sample_point'])
    missing = gdf['elevation'].isna().sum()
    total = len(gdf)
    if total > 0:
        coverage = (total - missing) / total * 100
        print(f"  Atribuição de elevação: {total - missing}/{total} feições ({coverage:.1f}%)")
        if total - missing > 0:
            print(f"  Intervalo de elevação: {gdf['elevation'].min():.1f} - {gdf['elevation'].max():.1f} metros")
    return gdf

# Listar todas as camadas do arquivo integrado original
try:
    available_layers = fiona.listlayers(final_gpkg)
    print(f"  Camadas disponíveis no arquivo integrado original: {available_layers}")
except Exception as e:
    print(f"  Erro ao listar camadas do arquivo integrado: {e}")
    available_layers = []

# Defina as camadas que podem ser atualizadas com altitude
update_layers = ['uso_terra_ocupacao', 'edificacoes', 'edificacoes_prioritarias', 'rodovias', 'ferrovias', 'hidrografia']

# Agora, para cada camada disponível no arquivo original, se ela estiver na lista update_layers, atualizamos com altimetria;
# caso contrário, copiamos a camada sem alteração.
for layer_name in available_layers:
    try:
        print(f"\n  Processando camada: {layer_name}")
        layer_gdf = gpd.read_file(final_gpkg, layer=layer_name)
        print(f"  Carregados {len(layer_gdf)} feições")
        if layer_name in update_layers and dem_data is not None:
            layer_updated = add_elevation_to_layer(layer_gdf, dem_data=dem_data, dem_transform=dem_transform, dem_extent=dem_extent, contours_gdf=curvas_nivel)
            gdf_to_save = layer_updated
            print(f"  Altimetria adicionada à camada '{layer_name}'.")
        else:
            gdf_to_save = layer_gdf
            print(f"  Camada '{layer_name}' não foi atualizada (sem processamento de altitude).")
        # Salvar a camada (sobrescrevendo ou copiando) no novo arquivo integrado
        gdf_to_save.to_file(final_gpkg_alt, layer=layer_name, driver="GPKG")
        print(f"  Camada '{layer_name}' salva no arquivo integrado com altitude.")
    except Exception as e:
        print(f"  Erro ao processar camada {layer_name}: {e}")

# Adicionar a camada das curvas de nível (com altitude) ao novo arquivo integrado
try:
    curvas_nivel.to_file(final_gpkg_alt, layer='curvas_nivel_com_elevacao', driver="GPKG")
    print("  Camada 'curvas_nivel_com_elevacao' adicionada ao arquivo integrado.")
except Exception as e:
    print(f"  Erro ao salvar camada de curvas de nível: {e}")

# Atualizar a lista de camadas disponíveis no novo arquivo integrado
try:
    available_layers_alt = fiona.listlayers(final_gpkg_alt)
    print(f"\n  Camadas disponíveis no arquivo integrado com altitude: {available_layers_alt}")
except Exception as e:
    print(f"  Erro ao listar camadas do arquivo integrado com altitude: {e}")
    available_layers_alt = []

print("\n6.4. Criando visualização 3D da topografia com camadas principais...")

try:
    from mpl_toolkits.mplot3d import Axes3D  # Para gráficos 3D

    visual_layers = []
    # Vamos utilizar camadas que contenham a coluna 'elevation'
    if 'edificacoes' in available_layers_alt:
        edificacoes = gpd.read_file(final_gpkg_alt, layer='edificacoes')
        if 'elevation' in edificacoes.columns and not edificacoes['elevation'].isna().all():
            visual_layers.append({
                'data': edificacoes,
                'name': 'Edificações',
                'color': 'red',
                'marker': 'o',
                'size': 20
            })
    if 'rodovias' in available_layers_alt:
        rodovias = gpd.read_file(final_gpkg_alt, layer='rodovias')
        if 'road_class' in rodovias.columns and not rodovias['elevation'].isna().all():
            rodovias_principais = rodovias[rodovias['road_class'] == 'primary']
            if len(rodovias_principais) > 0:
                visual_layers.append({
                    'data': rodovias_principais,
                    'name': 'Rodovias',
                    'color': 'black',
                    'marker': '.',
                    'size': 10
                })
    if 'hidrografia' in available_layers_alt:
        hidrografia = gpd.read_file(final_gpkg_alt, layer='hidrografia')
        if 'elevation' in hidrografia.columns and not hidrografia['elevation'].isna().all():
            visual_layers.append({
                'data': hidrografia,
                'name': 'Hidrografia',
                'color': 'blue',
                'marker': '.',
                'size': 10
            })
    # Opcional: incluir as curvas de nível na visualização 3D
    if 'curvas_nivel_com_elevacao' in available_layers_alt:
        curvas_vis = gpd.read_file(final_gpkg_alt, layer='curvas_nivel_com_elevacao')
        if 'elevation' in curvas_vis.columns and not curvas_vis['elevation'].isna().all():
            visual_layers.append({
                'data': curvas_vis,
                'name': 'Curvas de Nível',
                'color': 'green',
                'marker': '.',
                'size': 5
            })

    if visual_layers:
        print("  Criando visualização 3D...")
        fig = plt.figure(figsize=(14, 12))
        ax = fig.add_subplot(111, projection='3d')
        ax.set_title('Visualização 3D - Sorocaba', fontsize=16)
        ax.set_xlabel('X (Este)')
        ax.set_ylabel('Y (Norte)')
        ax.set_zlabel('Elevação (m)')
        for layer in visual_layers:
            gdf = layer['data']
            x = gdf.geometry.centroid.x
            y = gdf.geometry.centroid.y
            z = gdf['elevation']
            ax.scatter(
                x, y, z,
                color=layer['color'],
                label=layer['name'],
                marker=layer['marker'],
                s=layer['size'],
                alpha=0.7
            )
        ax.legend()
        ax.view_init(elev=30, azim=225)
        plt.savefig(os.path.join(plots_dir, 'visualizacao_3d_sorocaba.png'), dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"  Visualização 3D salva em: {os.path.join(plots_dir, 'visualizacao_3d_sorocaba.png')}")
    else:
        print("  Não há camadas suficientes com dados de elevação para criar visualização 3D.")
except Exception as e:
    print(f"  Erro ao criar visualização 3D: {e}")
    import traceback
    print(traceback.format_exc())

print("\nIntegração de dados de altitude concluída.")

import os
import geopandas as gpd
import fiona

# Definir diretórios
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
processed_dir = os.path.join(base_dir, 'data/terreno/processados')
csv_dir = os.path.join(base_dir, 'data/terreno/csv')

# Nome do arquivo integrado – ajuste a extensão se necessário
# Se o arquivo tiver extensão .gpkg, use:
gpkg_file = os.path.join(processed_dir, 'sorocaba_dados_integrados_alt.gpkg')
# Caso o arquivo seja de fato .gpk, utilize:
# gpkg_file = os.path.join(processed_dir, 'sorocaba_dados_integrados_alt.gpk')

# Verificar se o arquivo existe
if not os.path.exists(gpkg_file):
    print(f"Arquivo não encontrado: {gpkg_file}")
else:
    # Listar camadas disponíveis no GeoPackage
    layers = fiona.listlayers(gpkg_file)
    print("Camadas encontradas:", layers)

    # Converter cada camada para CSV
    for layer in layers:
        gdf = gpd.read_file(gpkg_file, layer=layer)
        csv_path = os.path.join(csv_dir, f"{layer}.csv")
        gdf.to_csv(csv_path, index=False)
        print(f"Camada '{layer}' salva como CSV em: {csv_path}")

import os
import logging
from datetime import datetime, time
import traceback

import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import fiona

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Definir diretórios de trabalho
base_dir = '/content/drive/MyDrive/GrafosGeoespaciais'
data_dir = os.path.join(base_dir, 'data', 'terreno', 'processados')
output_dir = os.path.join(base_dir, 'data', 'terreno', 'visualizacoes')

# Caminho para o GeoPackage unificado
gpkg_path = os.path.join(data_dir, 'sorocaba_dados_integrados_alt.gpkg')

# Verifica se o arquivo GeoPackage existe
if not os.path.exists(gpkg_path):
    logging.error(f"GeoPackage não encontrado em: {gpkg_path}")
    raise FileNotFoundError(f"GeoPackage não encontrado em: {gpkg_path}")

# Criar diretório de saída se não existir
os.makedirs(output_dir, exist_ok=True)

def distribuir_populacao_por_horario(hora):
    """
    Distribui a população entre diferentes atividades (residência, trabalho, escola, etc.)
    de acordo com o horário.
    Retorna um dicionário com os fatores de distribuição.
    """
    # Por padrão, considerar dia útil
    dia_util = True

    # Definir períodos do dia
    horario_comercial = time(8, 0) <= hora.time() <= time(18, 0)
    horario_escolar_manha = time(7, 30) <= hora.time() <= time(12, 30)
    horario_escolar_tarde = time(13, 0) <= hora.time() <= time(18, 0)
    horario_universitario_manha = time(8, 0) <= hora.time() <= time(12, 0)
    horario_universitario_noite = time(19, 0) <= hora.time() <= time(22, 30)

    # Distribuição padrão para horários com baixa atividade
    distribuicao = {
        'residencia': 0.95,
        'trabalho_industria': 0.01,
        'trabalho_comercio': 0.0,
        'trabalho_servicos': 0.01,
        'escola_fundamental': 0.0,
        'escola_medio': 0.0,
        'universidade': 0.0,
        'lazer': 0.03,
    }

    if dia_util and horario_comercial:
        distribuicao.update({
            'residencia': 0.30,
            'trabalho_industria': 0.15,
            'trabalho_comercio': 0.15,
            'trabalho_servicos': 0.25,
            'lazer': 0.05,
        })
        if horario_escolar_manha:
            distribuicao.update({
                'escola_fundamental': 0.05,
                'escola_medio': 0.03,
            })
        elif horario_escolar_tarde:
            distribuicao.update({
                'escola_fundamental': 0.04,
                'escola_medio': 0.03,
            })
        if horario_universitario_manha:
            distribuicao['universidade'] = 0.02
        elif horario_universitario_noite:
            distribuicao['universidade'] = 0.03

    return distribuicao

def listar_camadas_geopackage():
    """
    Lista as camadas disponíveis no GeoPackage.
    """
    logging.info("Verificando camadas disponíveis no GeoPackage...")
    try:
        layers = fiona.listlayers(gpkg_path)
        logging.info(f"Camadas encontradas: {layers}")
        return layers
    except Exception as e:
        logging.error(f"Erro ao acessar o GeoPackage: {e}")
        logging.debug(traceback.format_exc())
        return []

def enriquecer_setores_censitarios():
    """
    Enriquecer a camada de setores censitários com dados populacionais:
      - Cálculo de área (em km²);
      - Distribuição de população total de Sorocaba proporcional à área e ao grau de urbanização;
      - Distribuição por faixa etária e população economicamente ativa (PEA).
    """
    logging.info("Enriquecendo setores censitários com dados populacionais...")
    try:
        setores_gdf = gpd.read_file(gpkg_path, layer='setores_censitarios')
        setores_gdf = setores_gdf.to_crs("EPSG:31983")

        # Calcular área (km²) se não existir
        if 'area_km2' not in setores_gdf.columns:
            setores_gdf['area_km2'] = setores_gdf.geometry.area / 1e6

        # Dados populacionais de Sorocaba (Fonte: Guia Estatístico)
        populacao_total = 668973

        # Distribuição entre áreas urbanas e rurais
        urban_mask = setores_gdf['is_urban'] == 1
        total_area_urbana = setores_gdf.loc[urban_mask, 'area_km2'].sum()
        total_area_rural = setores_gdf.loc[~urban_mask, 'area_km2'].sum()

        populacao_urbana = populacao_total * 0.99
        populacao_rural = populacao_total * 0.01

        # Distribuir população proporcionalmente (vetorizado)
        setores_gdf.loc[urban_mask, 'est_populacao'] = (
            setores_gdf.loc[urban_mask, 'area_km2'] / total_area_urbana * populacao_urbana
            if total_area_urbana > 0 else 0
        )
        setores_gdf.loc[~urban_mask, 'est_populacao'] = (
            setores_gdf.loc[~urban_mask, 'area_km2'] / total_area_rural * populacao_rural
            if total_area_rural > 0 else 0
        )

        # Distribuição por faixa etária
        setores_gdf['est_pop_0_14'] = setores_gdf['est_populacao'] * 0.18
        setores_gdf['est_pop_15_29'] = setores_gdf['est_populacao'] * 0.20
        setores_gdf['est_pop_30_59'] = setores_gdf['est_populacao'] * 0.45
        setores_gdf['est_pop_60_mais'] = setores_gdf['est_populacao'] * 0.17

        # Estimativa de população economicamente ativa (PEA): 15 a 59 anos com 65% de atividade
        setores_gdf['est_pop_pea'] = (setores_gdf['est_pop_15_29'] + setores_gdf['est_pop_30_59']) * 0.65

        # Salvar atualizações no GeoPackage
        setores_gdf.to_file(gpkg_path, layer='setores_censitarios', driver="GPKG")

        total_est = setores_gdf['est_populacao'].sum()
        logging.info(f"Setores censitários enriquecidos. População total estimada: {total_est:.0f}")
        return setores_gdf
    except Exception as e:
        logging.error(f"Erro ao enriquecer setores censitários: {e}")
        logging.debug(traceback.format_exc())
        return None

def enriquecer_areas_economicas(setores_gdf=None):
    """
    Enriquecer áreas de uso da terra com estimativas de trabalhadores,
    distribuindo-os conforme os dados de população economicamente ativa.
    """
    logging.info("Enriquecendo áreas industriais e comerciais...")
    try:
        if setores_gdf is None:
            setores_gdf = gpd.read_file(gpkg_path, layer='setores_censitarios')
            if 'est_pop_pea' not in setores_gdf.columns:
                logging.error("Dados populacionais não encontrados. Execute 'enriquecer_setores_censitarios' primeiro.")
                return None

        uso_terra_gdf = gpd.read_file(gpkg_path, layer='uso_terra_ocupacao')
        uso_terra_gdf = uso_terra_gdf.to_crs("EPSG:31983")

        if 'area_km2' not in uso_terra_gdf.columns:
            uso_terra_gdf['area_km2'] = uso_terra_gdf.geometry.area / 1e6

        # Selecionar áreas econômicas
        areas_industriais = uso_terra_gdf[uso_terra_gdf['class'] == 'industrial'].copy()
        areas_comerciais = uso_terra_gdf[uso_terra_gdf['class'] == 'urban'].copy()

        pea_total = setores_gdf['est_pop_pea'].sum()
        # Proporções de trabalhadores
        prop_industria = 0.25
        prop_comercio = 0.30
        prop_servicos = 0.45

        total_trab_industria = pea_total * prop_industria
        total_trab_comercio = pea_total * prop_comercio
        total_trab_servicos = pea_total * prop_servicos

        # Inicializar colunas se não existirem
        for col in ['est_trabalhadores', 'est_trabalhadores_comercio', 'est_trabalhadores_servicos']:
            if col not in uso_terra_gdf.columns:
                uso_terra_gdf[col] = 0

        # Distribuição de trabalhadores (vetorizado)
        if not areas_industriais.empty:
            area_total_industrial = areas_industriais['area_km2'].sum()
            uso_terra_gdf.loc[uso_terra_gdf['class'] == 'industrial', 'est_trabalhadores'] = (
                uso_terra_gdf.loc[uso_terra_gdf['class'] == 'industrial', 'area_km2'] / area_total_industrial * total_trab_industria
                if area_total_industrial > 0 else 0
            )
        if not areas_comerciais.empty:
            area_total_comercial = areas_comerciais['area_km2'].sum()
            mask = uso_terra_gdf['class'] == 'urban'
            uso_terra_gdf.loc[mask, 'est_trabalhadores_comercio'] = (
                uso_terra_gdf.loc[mask, 'area_km2'] / area_total_comercial * total_trab_comercio
                if area_total_comercial > 0 else 0
            )
            uso_terra_gdf.loc[mask, 'est_trabalhadores_servicos'] = (
                uso_terra_gdf.loc[mask, 'area_km2'] / area_total_comercial * total_trab_servicos
                if area_total_comercial > 0 else 0
            )

        # Preencher NaN com 0
        uso_terra_gdf.fillna(0, inplace=True)
        uso_terra_gdf.to_file(gpkg_path, layer='uso_terra_ocupacao', driver="GPKG")

        logging.info("Áreas econômicas enriquecidas com sucesso. " +
                     f"Indústria={uso_terra_gdf['est_trabalhadores'].sum():.0f}, " +
                     f"Comércio={uso_terra_gdf['est_trabalhadores_comercio'].sum():.0f}, " +
                     f"Serviços={uso_terra_gdf['est_trabalhadores_servicos'].sum():.0f}")
        return uso_terra_gdf
    except Exception as e:
        logging.error(f"Erro ao enriquecer áreas econômicas: {e}")
        logging.debug(traceback.format_exc())
        return None

def enriquecer_edificacoes_educacionais(setores_gdf=None):
    """
    Enriquecer a camada de edificações educacionais:
      - Identificar edifícios de interesse (através de 'building_type' ou 'amenity');
      - Estimar número de alunos baseado na população em idade escolar;
      - Distribuir alunos conforme nível de ensino.
    """
    logging.info("Enriquecendo edificações educacionais...")
    try:
        if setores_gdf is None:
            setores_gdf = gpd.read_file(gpkg_path, layer='setores_censitarios')
            if 'est_pop_0_14' not in setores_gdf.columns:
                logging.error("Dados populacionais não encontrados. Execute 'enriquecer_setores_censitarios' primeiro.")
                return None

        try:
            edificacoes_gdf = gpd.read_file(gpkg_path, layer='edificacoes')
        except Exception as e:
            logging.warning(f"Erro ao carregar camada 'edificacoes': {e}. Tentando 'edificacoes_prioritarias'...")
            edificacoes_gdf = gpd.read_file(gpkg_path, layer='edificacoes_prioritarias')

        # Filtrar edificações educacionais
        if 'building_type' in edificacoes_gdf.columns:
            educ_edificacoes = edificacoes_gdf[edificacoes_gdf['building_type'] == 'educational'].copy()
            if educ_edificacoes.empty and 'amenity' in edificacoes_gdf.columns:
                educ_edificacoes = edificacoes_gdf[edificacoes_gdf['amenity'].isin(
                    ['school', 'university', 'college', 'kindergarten'])].copy()
        else:
            if 'amenity' in edificacoes_gdf.columns:
                educ_edificacoes = edificacoes_gdf[edificacoes_gdf['amenity'].isin(
                    ['school', 'university', 'college', 'kindergarten'])].copy()
            else:
                logging.error("Não foi possível identificar edificações educacionais.")
                return edificacoes_gdf

        if educ_edificacoes.empty:
            logging.warning("Nenhuma edificação educacional encontrada.")
            return edificacoes_gdf

        logging.info(f"Encontradas {len(educ_edificacoes)} edificações educacionais.")

        # Estimar população em idade escolar
        pop_fundamental = setores_gdf['est_pop_0_14'].sum() * 0.75  # 75% dos 0-14
        pop_medio = setores_gdf['est_pop_15_29'].sum() * 0.30         # 30% dos 15-29
        pop_universitario = setores_gdf['est_pop_15_29'].sum() * 0.20   # 20% dos 15-29

        # Preparar colunas para nível de ensino e estimativa de alunos
        if 'nivel_ensino' not in edificacoes_gdf.columns:
            edificacoes_gdf['nivel_ensino'] = ''
        if 'est_alunos' not in edificacoes_gdf.columns:
            edificacoes_gdf['est_alunos'] = 0

        num_escolas = len(educ_edificacoes)
        # Distribuir níveis: 60% fundamental, 30% médio, 10% superior (ou o restante)
        n_fundamental = int(num_escolas * 0.6)
        n_medio = int(num_escolas * 0.3)
        n_superior = num_escolas - n_fundamental - n_medio

        educ_edificacoes = educ_edificacoes.reset_index(drop=True)
        educ_edificacoes.loc[:n_fundamental-1, 'nivel_ensino'] = 'fundamental'
        educ_edificacoes.loc[n_fundamental:n_fundamental+n_medio-1, 'nivel_ensino'] = 'medio'
        educ_edificacoes.loc[n_fundamental+n_medio:, 'nivel_ensino'] = 'superior'

        alunos_por_escola_fund = (pop_fundamental * 0.986) / n_fundamental if n_fundamental > 0 else 0
        alunos_por_escola_medio = (pop_medio * 0.986) / n_medio if n_medio > 0 else 0
        alunos_por_escola_sup = (pop_universitario * 0.986) / n_superior if n_superior > 0 else 0

        educ_edificacoes.loc[educ_edificacoes['nivel_ensino'] == 'fundamental', 'est_alunos'] = alunos_por_escola_fund
        educ_edificacoes.loc[educ_edificacoes['nivel_ensino'] == 'medio', 'est_alunos'] = alunos_por_escola_medio
        educ_edificacoes.loc[educ_edificacoes['nivel_ensino'] == 'superior', 'est_alunos'] = alunos_por_escola_sup

        # Atualizar os registros na camada original, utilizando 'osm_id' se disponível ou comparando geometria
        for idx, row in educ_edificacoes.iterrows():
            if 'osm_id' in row and 'osm_id' in edificacoes_gdf.columns:
                mask = edificacoes_gdf['osm_id'] == row['osm_id']
            else:
                mask = edificacoes_gdf.geometry == row.geometry
            if mask.any():
                edificacoes_gdf.loc[mask, 'nivel_ensino'] = row['nivel_ensino']
                edificacoes_gdf.loc[mask, 'est_alunos'] = row['est_alunos']

        edificacoes_gdf.to_file(gpkg_path, layer='edificacoes', driver="GPKG")
        total_alunos = edificacoes_gdf['est_alunos'].sum()
        logging.info(f"Edificações educacionais enriquecidas. Total de alunos estimados: {total_alunos:.0f}")
        return edificacoes_gdf
    except Exception as e:
        logging.error(f"Erro ao enriquecer edificações educacionais: {e}")
        logging.debug(traceback.format_exc())
        return None

def calcular_distribuicao_temporal(hora):
    """
    Calcula a distribuição temporal para um dado horário:
      - Atualiza os setores com a população residente;
      - Atualiza áreas econômicas com trabalhadores (industrial, comércio e serviços);
      - Atualiza edificações educacionais com alunos.
    """
    logging.info(f"Calculando distribuição temporal para {hora.strftime('%H:%M')}...")
    try:
        fatores = distribuir_populacao_por_horario(hora)
        setores_gdf = gpd.read_file(gpkg_path, layer='setores_censitarios')
        uso_terra_gdf = gpd.read_file(gpkg_path, layer='uso_terra_ocupacao')
        edificacoes_gdf = gpd.read_file(gpkg_path, layer='edificacoes')

        if 'est_populacao' not in setores_gdf.columns:
            logging.error("Setores censitários não estão enriquecidos. Execute 'enriquecer_setores_censitarios' primeiro.")
            return None

        hora_str = hora.strftime("%H%M")
        # Distribuição residencial
        setores_gdf[f'pop_atual_{hora_str}'] = setores_gdf['est_populacao'] * fatores['residencia']

        # Distribuição de trabalhadores para áreas industriais
        if 'est_trabalhadores' in uso_terra_gdf.columns:
            mask_ind = uso_terra_gdf['class'] == 'industrial'
            uso_terra_gdf.loc[mask_ind, f'trab_atual_{hora_str}'] = (
                uso_terra_gdf.loc[mask_ind, 'est_trabalhadores'] *
                (fatores['trabalho_industria'] or 1)
            )
        # Distribuição para comércio
        if 'est_trabalhadores_comercio' in uso_terra_gdf.columns:
            mask_urb = uso_terra_gdf['class'] == 'urban'
            uso_terra_gdf.loc[mask_urb, f'trab_comercio_atual_{hora_str}'] = (
                uso_terra_gdf.loc[mask_urb, 'est_trabalhadores_comercio'] *
                (fatores['trabalho_comercio'] or 1)
            )
        # Distribuição para serviços
        if 'est_trabalhadores_servicos' in uso_terra_gdf.columns:
            uso_terra_gdf.loc[mask_urb, f'trab_servicos_atual_{hora_str}'] = (
                uso_terra_gdf.loc[mask_urb, 'est_trabalhadores_servicos'] *
                (fatores['trabalho_servicos'] or 1)
            )
        # Distribuição de alunos nas edificações
        if 'est_alunos' in edificacoes_gdf.columns and 'nivel_ensino' in edificacoes_gdf.columns:
            # Fundamental
            mask_fund = edificacoes_gdf['nivel_ensino'] == 'fundamental'
            edificacoes_gdf.loc[mask_fund, f'alunos_atual_{hora_str}'] = (
                edificacoes_gdf.loc[mask_fund, 'est_alunos'] * (fatores['escola_fundamental'] or 1)
            )
            # Médio
            mask_medio = edificacoes_gdf['nivel_ensino'] == 'medio'
            edificacoes_gdf.loc[mask_medio, f'alunos_atual_{hora_str}'] = (
                edificacoes_gdf.loc[mask_medio, 'est_alunos'] * (fatores['escola_medio'] or 1)
            )
            # Superior
            mask_sup = edificacoes_gdf['nivel_ensino'] == 'superior'
            edificacoes_gdf.loc[mask_sup, f'alunos_atual_{hora_str}'] = (
                edificacoes_gdf.loc[mask_sup, 'est_alunos'] * (fatores['universidade'] or 1)
            )

        # Garantir que não existam valores NaN
        for col in [f'pop_atual_{hora_str}', f'trab_atual_{hora_str}',
                    f'trab_comercio_atual_{hora_str}', f'trab_servicos_atual_{hora_str}',
                    f'alunos_atual_{hora_str}']:
            if col in setores_gdf.columns:
                setores_gdf[col] = setores_gdf[col].fillna(0)
            if col in uso_terra_gdf.columns:
                uso_terra_gdf[col] = uso_terra_gdf[col].fillna(0)
            if col in edificacoes_gdf.columns:
                edificacoes_gdf[col] = edificacoes_gdf[col].fillna(0)

        # Salvar as atualizações
        setores_gdf.to_file(gpkg_path, layer='setores_censitarios', driver="GPKG")
        uso_terra_gdf.to_file(gpkg_path, layer='uso_terra_ocupacao', driver="GPKG")
        edificacoes_gdf.to_file(gpkg_path, layer='edificacoes', driver="GPKG")

        logging.info(f"Distribuição temporal para {hora.strftime('%H:%M')} calculada com sucesso.")
        return {'setores': setores_gdf, 'uso_terra': uso_terra_gdf, 'edificacoes': edificacoes_gdf, 'hora': hora}
    except Exception as e:
        logging.error(f"Erro ao calcular distribuição temporal: {e}")
        logging.debug(traceback.format_exc())
        return None

def visualizar_distribuicao_populacional(camadas_temporais, output_dir):
    """
    Cria e salva visualizações da distribuição populacional e da ocupação em edificações educacionais.
    """
    logging.info("Criando visualizações da distribuição populacional...")
    try:
        setores = camadas_temporais['setores']
        hora = camadas_temporais['hora']
        hora_str = hora.strftime('%H%M')
        col_pop = f'pop_atual_{hora_str}'

        if col_pop not in setores.columns:
            logging.error(f"Coluna {col_pop} não encontrada. Visualização não pode ser criada.")
            return

        fig, ax = plt.subplots(figsize=(12, 10))
        setores.plot(column=col_pop, ax=ax, cmap='viridis', legend=True,
                     legend_kwds={'label': f"População (hora: {hora.strftime('%H:%M')})"})
        ax.set_title(f"Distribuição Populacional - {hora.strftime('%H:%M')}")
        output_path = os.path.join(output_dir, f"distribuicao_pop_{hora_str}.png")
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        logging.info(f"Visualização populacional salva em {output_path}")

        # Visualização para edificações educacionais, se disponíveis
        edificacoes = camadas_temporais['edificacoes']
        col_alunos = f'alunos_atual_{hora_str}'
        if col_alunos in edificacoes.columns:
            fig, ax = plt.subplots(figsize=(12, 10))
            educ_edificacoes = edificacoes[edificacoes[col_alunos] > 0]
            if not educ_edificacoes.empty:
                educ_edificacoes.plot(column=col_alunos, ax=ax, cmap='Reds', markersize=30,
                                      legend=True, legend_kwds={'label': "Alunos presentes"})
                setores.plot(ax=ax, color='lightgrey', alpha=0.5)
                ax.set_title(f"Ocupação Escolar - {hora.strftime('%H:%M')}")
                output_path = os.path.join(output_dir, f"ocupacao_escolar_{hora_str}.png")
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                plt.close()
                logging.info(f"Visualização de ocupação escolar salva em {output_path}")
            else:
                logging.info(f"Nenhuma edificação com alunos encontrada para {hora.strftime('%H:%M')}")
    except Exception as e:
        logging.error(f"Erro ao criar visualizações: {e}")
        logging.debug(traceback.format_exc())

def visualizar_areas_trabalho(camadas_temporais, output_dir):
    """
    Cria e salva visualizações para áreas de trabalho, diferenciando indústria e comércio/serviços.
    """
    logging.info("Criando visualizações das áreas de trabalho...")
    try:
        uso_terra = camadas_temporais['uso_terra']
        setores = camadas_temporais['setores']
        hora = camadas_temporais['hora']
        hora_str = hora.strftime('%H%M')

        col_ind = f'trab_atual_{hora_str}'
        col_com = f'trab_comercio_atual_{hora_str}'
        col_serv = f'trab_servicos_atual_{hora_str}'

        if col_ind not in uso_terra.columns and col_com not in uso_terra.columns and col_serv not in uso_terra.columns:
            logging.error(f"Nenhuma coluna de trabalho encontrada para {hora.strftime('%H:%M')}.")
            return

        # Visualização para indústria
        if col_ind in uso_terra.columns:
            fig, ax = plt.subplots(figsize=(12, 10))
            setores.plot(ax=ax, color='lightgrey', alpha=0.3)
            industrias_ativas = uso_terra[(uso_terra['class'] == 'industrial') & (uso_terra[col_ind] > 0)]
            if not industrias_ativas.empty:
                industrias_ativas.plot(column=col_ind, ax=ax, cmap='Blues', alpha=0.7,
                                       legend=True, legend_kwds={'label': "Trabalhadores na indústria"})
                ax.set_title(f"Trabalhadores na Indústria - {hora.strftime('%H:%M')}")
                output_path = os.path.join(output_dir, f"trabalhadores_industria_{hora_str}.png")
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                plt.close()
                logging.info(f"Visualização de trabalho industrial salva em {output_path}")

        # Visualização para comércio e serviços
        if col_com in uso_terra.columns or col_serv in uso_terra.columns:
            fig, ax = plt.subplots(figsize=(12, 10))
            setores.plot(ax=ax, color='lightgrey', alpha=0.3)
            # Criar coluna combinada sem modificar o DataFrame original
            trab_comb = uso_terra.get(col_com, 0) + uso_terra.get(col_serv, 0)
            uso_terra = uso_terra.copy()
            uso_terra['trab_comercio_servicos'] = trab_comb
            urbano_ativo = uso_terra[(uso_terra['class'] == 'urban') & (uso_terra['trab_comercio_servicos'] > 0)]
            if not urbano_ativo.empty:
                urbano_ativo.plot(column='trab_comercio_servicos', ax=ax, cmap='Greens', alpha=0.7,
                                   legend=True, legend_kwds={'label': "Trabalhadores no comércio e serviços"})
                ax.set_title(f"Trabalhadores no Comércio e Serviços - {hora.strftime('%H:%M')}")
                output_path = os.path.join(output_dir, f"trabalhadores_comercio_servicos_{hora_str}.png")
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                plt.close()
                logging.info(f"Visualização de comércio/serviços salva em {output_path}")
    except Exception as e:
        logging.error(f"Erro ao criar visualizações de áreas de trabalho: {e}")
        logging.debug(traceback.format_exc())

def main():
    logging.info("Iniciando processo de enriquecimento de dados socioeconômicos no GeoPackage...")

    layers = listar_camadas_geopackage()
    if not layers:
        logging.error("Nenhuma camada disponível no GeoPackage. Processo encerrado.")
        return

    setores_gdf = enriquecer_setores_censitarios()
    if setores_gdf is None:
        logging.error("Falha ao enriquecer setores censitários. Processo encerrado.")
        return

    uso_terra_gdf = enriquecer_areas_economicas(setores_gdf)
    edificacoes_gdf = enriquecer_edificacoes_educacionais(setores_gdf)

    # Lista de horários para análise
    horarios = [
        datetime.strptime("08:00", "%H:%M"),
        datetime.strptime("12:00", "%H:%M"),
        datetime.strptime("15:00", "%H:%M"),
        datetime.strptime("19:00", "%H:%M"),
        datetime.strptime("23:00", "%H:%M")
    ]

    for hora in horarios:
        camadas_temporais = calcular_distribuicao_temporal(hora)
        if camadas_temporais is not None:
            visualizar_distribuicao_populacional(camadas_temporais, output_dir)
            visualizar_areas_trabalho(camadas_temporais, output_dir)

    logging.info("Processo de enriquecimento concluído com sucesso!")
    logging.info(f"Visualizações salvas em: {output_dir}")

if __name__ == "__main__":
    main()

import geopandas as gpd
import fiona
import pandas as pd
import os

# Caminho para o GeoPackage atualizado
gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_integrados_alt.gpkg'

# Listar todas as camadas disponíveis
layers = fiona.listlayers(gpkg_path)
print(f"Camadas encontradas no GeoPackage: {layers}")

# Função para analisar e descrever uma camada
def analyze_layer(layer_name):
    print(f"\n{'='*80}\nANÁLISE DETALHADA DA CAMADA: {layer_name}\n{'='*80}")

    # Carregar a camada
    gdf = gpd.read_file(gpkg_path, layer=layer_name)

    # Informações básicas
    print(f"Número de feições: {len(gdf)}")
    print(f"Sistema de coordenadas: {gdf.crs}")
    print(f"Tipos de geometria: {gdf.geometry.geom_type.unique()}")

    # Lista de colunas
    print(f"\nColunas disponíveis ({len(gdf.columns)}):")
    for col in gdf.columns:
        if col != 'geometry':
            dtype = gdf[col].dtype
            non_null = gdf[col].count()
            pct_filled = (non_null / len(gdf)) * 100
            print(f"  - {col}: {dtype} ({non_null}/{len(gdf)} preenchidos, {pct_filled:.1f}%)")

    # Estatísticas para colunas numéricas
    numeric_cols = gdf.select_dtypes(include=['number']).columns
    if len(numeric_cols) > 0:
        print("\nEstatísticas para colunas numéricas:")
        desc = gdf[numeric_cols].describe().transpose()
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', 120)
        print(desc)

    # Amostra de dados
    print("\nAmostra dos primeiros registros (excluindo geometria):")
    if 'geometry' in gdf.columns:
        sample = gdf.drop(columns='geometry').head(3)
    else:
        sample = gdf.head(3)
    print(sample)

    # Análise de valores únicos para colunas categóricas
    categorical_cols = [col for col in gdf.columns if col != 'geometry' and
                        (gdf[col].dtype == 'object' or
                         len(gdf[col].unique()) < 20 if gdf[col].dtype != 'object' else False)]

    if categorical_cols:
        print("\nAnálise de valores categóricos:")
        for col in categorical_cols[:5]:  # Limitar a 5 colunas para não sobrecarregar
            value_counts = gdf[col].value_counts().head(5)
            print(f"  Top 5 valores para '{col}':")
            for val, count in value_counts.items():
                pct = (count / len(gdf)) * 100
                print(f"    - {val}: {count} ({pct:.1f}%)")

    # Informações de enriquecimento socioeconômico
    socio_cols = [col for col in gdf.columns if col.startswith('est_') or
                 'atual' in col or 'alunos' in col or 'trab' in col]

    if socio_cols:
        print("\nInformações de enriquecimento socioeconômico:")
        for col in socio_cols:
            if col in gdf.columns:
                total = gdf[col].sum()
                print(f"  - {col}: Total = {total:.1f}")

    print(f"\nTamanho estimado da camada: {gdf.memory_usage(deep=True).sum() / (1024*1024):.2f} MB")

# Analisar cada camada
for layer in layers:
    try:
        analyze_layer(layer)
    except Exception as e:
        print(f"\nErro ao analisar camada {layer}: {e}")

print("\nAnálise completa do GeoPackage concluída.")

import os
import logging
import pandas as pd
import numpy as np
import geopandas as gpd
import fiona
from scipy.interpolate import griddata
from shapely.geometry import Point, LineString
import matplotlib.pyplot as plt
from datetime import datetime
import warnings
import traceback

warnings.filterwarnings('ignore')

# =============================================================================
# Configuração do Logging
# =============================================================================
log_file = '/content/drive/MyDrive/GrafosGeoespaciais/output/atmosfera/processamento.log'
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# =============================================================================
# Funções Auxiliares
# =============================================================================
def listar_camadas_gpkg(gpkg_path: str) -> list:
    """
    Lista todas as camadas disponíveis em um arquivo GeoPackage.
    """
    try:
        layers = fiona.listlayers(gpkg_path)
        logging.info(f"Camadas disponíveis em {os.path.basename(gpkg_path)}:")
        for layer in layers:
            logging.info(f" - {layer}")
        return layers
    except Exception as e:
        logging.error(f"Erro ao listar camadas: {e}")
        return []


def extrair_topografia(gpkg_path: str, layer_name: str = 'curvas_nivel_com_elevacao') -> gpd.GeoDataFrame:
    """
    Extrai dados de elevação do terreno a partir de uma camada de curvas de nível.
    """
    try:
        curvas = gpd.read_file(gpkg_path, layer=layer_name)
        if 'elevation' not in curvas.columns:
            raise ValueError("A coluna 'elevation' não foi encontrada na camada.")
        logging.info(f"Topografia: {len(curvas)} curvas de nível carregadas")
        logging.info(f"Intervalo de elevação: {curvas['elevation'].min()} a {curvas['elevation'].max()} metros")
        return curvas
    except Exception as e:
        logging.error(f"Erro ao extrair topografia da camada '{layer_name}': {e}")
        return None


def extrair_limites(gpkg_path: str, layer_name: str = 'uso_terra_ocupacao') -> tuple:
    """
    Extrai os limites geográficos da área de estudo a partir de uma camada de uso do solo.
    """
    try:
        uso_terra = gpd.read_file(gpkg_path, layer=layer_name)
        bounds = uso_terra.total_bounds
        logging.info(f"Limites geográficos extraídos: {bounds}")
        return tuple(bounds)
    except Exception as e:
        logging.error(f"Erro ao extrair limites: {e}")
        return (-47.7, -23.7, -47.3, -23.3)


def processar_dados_inmet(arquivo_inmet: str, encoding: str = 'cp1252') -> dict:
    """
    Processa os dados meteorológicos do INMET, calculando as médias diárias para todas as colunas numéricas.
    Como o arquivo INMET não contém coordenadas (apenas dados meteorológicos), essa função ignora a extração de coordenadas.
    Os valores médios das variáveis (incluindo precipitação) serão replicados para cada nó da grade.
    """
    try:
        logging.info(f"Lendo arquivo INMET: {os.path.basename(arquivo_inmet)}")
        encodings = [encoding, 'utf-8', 'latin1', 'iso-8859-1']
        df = None
        for enc in encodings:
            try:
                df = pd.read_csv(arquivo_inmet, encoding=enc, skiprows=8, sep=';', low_memory=False)
                logging.info(f"Arquivo lido com encoding: {enc}")
                break
            except Exception as e:
                logging.warning(f"Falha ao ler com encoding {enc}: {e}")
        if df is None:
            raise ValueError("Não foi possível ler o arquivo com nenhum encoding")

        logging.info(f"Colunas encontradas: {df.columns.tolist()}")

        # Processamento de data e hora
        cols_data = [col for col in df.columns if 'DATA' in col.upper()]
        cols_hora = [col for col in df.columns if 'HORA' in col.upper()]
        if cols_data and cols_hora:
            df['DATA_HORA'] = pd.to_datetime(df[cols_data[0]] + ' ' + df[cols_hora[0]], errors='coerce')
        else:
            raise ValueError("Colunas de data/hora não encontradas")

        # Processar todas as colunas numéricas (exceto as de data/hora)
        colunas_ignorar = ['DATA', 'HORA', 'DATA_HORA']
        colunas_numericas = []
        for col in df.columns:
            if any(ignore in col.upper() for ignore in colunas_ignorar):
                continue
            try:
                df[col] = df[col].astype(str).str.replace(',', '.').replace('-', 'NaN')
                df[col] = pd.to_numeric(df[col], errors='coerce')
                colunas_numericas.append(col)
            except:
                continue

        logging.info(f"Colunas numéricas identificadas: {colunas_numericas}")

        # Calcular médias diárias para as colunas numéricas
        df_daily = df.groupby(df['DATA_HORA'].dt.date)[colunas_numericas].mean().reset_index()

        # Extrair informações da estação a partir do nome do arquivo
        nome_arquivo = os.path.basename(arquivo_inmet)
        partes = nome_arquivo.split('_')
        if len(partes) >= 3:
            codigo_estacao = partes[2]
            nome_estacao = partes[3] if len(partes) > 3 else 'DESCONHECIDA'
        else:
            codigo_estacao = 'UNKNOWN'
            nome_estacao = 'DESCONHECIDA'

        # NÃO é necessário extrair coordenadas do INMET, pois a grade já possui seus próprios pontos.
        logging.info("Arquivo INMET não possui coordenadas; os dados meteorológicos serão replicados na grade.")

        # Calcular os valores médios para as variáveis meteorológicas
        medias = {}
        for col in colunas_numericas:
            valor_medio = df_daily[col].mean()
            if np.isnan(valor_medio):
                valor_medio = 0
            medias[col] = valor_medio
            logging.info(f"Média de {col}: {valor_medio}")

        # Seleciona a precipitação com base em uma coluna apropriada
        precipitacao_media = 0
        col_precip = [col for col in colunas_numericas if any(term in col.upper() for term in
                     ['PRECIPITACAO', 'CHUVA', 'PRECIP', 'PREC', 'PLUVIO', 'RAIN'])]
        if col_precip:
            precipitacao_media = medias[col_precip[0]]

        return {
            'codigo': codigo_estacao,
            'nome': nome_estacao,
            'precipitacao_media': precipitacao_media,
            'dados_diarios': df_daily,
            'variaveis_meteorologicas': medias
        }
    except Exception as e:
        logging.error(f"Erro ao processar arquivo INMET: {e}")
        logging.error(traceback.format_exc())
        return None


def exportar_para_gpkg(pontos_gdf: gpd.GeoDataFrame, arestas_gdf: gpd.GeoDataFrame,
                       caminho_saida: str, layer_pontos: str = 'pontos_grade', layer_arestas: str = 'arestas_grade'):
    """
    Exporta os GeoDataFrames de pontos e arestas para um arquivo GeoPackage.
    """
    try:
        diretorio_saida = os.path.dirname(caminho_saida)
        if diretorio_saida and not os.path.exists(diretorio_saida):
            os.makedirs(diretorio_saida)
            logging.info(f"Diretório de saída criado: {diretorio_saida}")
        if pontos_gdf is None or pontos_gdf.empty:
            logging.error("GeoDataFrame de pontos vazio ou nulo.")
            return False
        pontos_gdf.to_file(caminho_saida, layer=layer_pontos, driver="GPKG")
        logging.info(f"Pontos exportados para {caminho_saida}, camada '{layer_pontos}'")
        if arestas_gdf is not None and not arestas_gdf.empty:
            arestas_gdf.to_file(caminho_saida, layer=layer_arestas, driver="GPKG")
            logging.info(f"Arestas exportadas para {caminho_saida}, camada '{layer_arestas}'")
        return True
    except Exception as e:
        logging.error(f"Erro ao exportar para GeoPackage: {e}")
        logging.error(traceback.format_exc())
        return False


# =============================================================================
# Criação da Grade Atmosférica 3D e Interpolação
# =============================================================================
def criar_grade_atmosferica_3d(limites: tuple, topografia: gpd.GeoDataFrame = None,
                               dados_inmet: dict = None, dados_locais: pd.DataFrame = None,
                               resolucao_horizontal: float = 500, niveis_verticais: int = 11,
                               altura_maxima: float = 5000, crs: str = 'EPSG:4674'):
    """
    Cria uma grade atmosférica 3D considerando topografia e dados meteorológicos.
    Neste cenário, NÃO são utilizados dados de estação de solo.
    A grade é construída com espaçamentos de 500m horizontalmente e 500m verticalmente,
    e os dados do INMET são replicados em cada nó da grade.
    """
    xmin, ymin, xmax, ymax = limites
    logging.info(f"Criando grade atmosférica 3D nos limites: {limites}")

    num_cells_x = int(((xmax - xmin) * 111000) / resolucao_horizontal)
    num_cells_y = int(((ymax - ymin) * 111000) / resolucao_horizontal)

    max_cells = 10000
    if num_cells_x * num_cells_y > max_cells:
        fator_ajuste = np.sqrt(max_cells / (num_cells_x * num_cells_y))
        num_cells_x = int(num_cells_x * fator_ajuste)
        num_cells_y = int(num_cells_y * fator_ajuste)
        logging.info(f"Resolução ajustada: Nova grade {num_cells_x} x {num_cells_y} células")

    x_range = np.linspace(xmin, xmax, num_cells_x)
    y_range = np.linspace(ymin, ymax, num_cells_y)
    xx, yy = np.meshgrid(x_range, y_range)

    # Interpolação de elevação (se houver topografia; caso contrário, assume elevação zero)
    if topografia is not None and 'elevation' in topografia.columns:
        topo_points = []
        topo_values = []
        for _, row in topografia.iterrows():
            geom = row.geometry
            if geom is None:
                continue
            if geom.geom_type == 'LineString':
                for coord in geom.coords:
                    topo_points.append(coord)
                    topo_values.append(row['elevation'])
            elif geom.geom_type == 'MultiLineString':
                for line in geom.geoms:
                    for coord in line.coords:
                        topo_points.append(coord)
                        topo_values.append(row['elevation'])
        if len(topo_points) < 3:
            logging.warning("Poucos pontos de topografia para interpolação. Usando elevação zero.")
            elevacao_terreno = np.zeros_like(xx)
        else:
            topo_points = np.array(topo_points)
            topo_values = np.array(topo_values)
            logging.info(f"Interpolando elevação com {len(topo_points)} pontos")
            elevacao_terreno = griddata(topo_points, topo_values, (xx, yy), method='nearest')
            elevacao_min = max(0, np.min(topo_values))
            elevacao_terreno = np.maximum(elevacao_terreno, elevacao_min)
            logging.info(f"Elevação interpolada: min={np.min(elevacao_terreno)}, max={np.max(elevacao_terreno)}")
    else:
        logging.warning("Dados de topografia não disponíveis. Usando elevação zero.")
        elevacao_terreno = np.zeros_like(xx)

    alturas = np.linspace(0, 1, niveis_verticais)

    # Preparar os dados meteorológicos do INMET para replicação
    dados_meteo_inmet = {}
    if dados_inmet is not None:
        if 'variaveis_meteorologicas' in dados_inmet:
            dados_meteo_inmet = dados_inmet['variaveis_meteorologicas']
            logging.info(f"Dados INMET disponíveis: {list(dados_meteo_inmet.keys())}")
        dados_meteo_inmet['precipitacao'] = dados_inmet['precipitacao_media']
    else:
        logging.warning("Nenhum dado INMET disponível. Usando valores zero para meteorologia.")
        dados_meteo_inmet['precipitacao'] = 0

    # Nesta versão, NÃO adicionamos dados de estações locais.
    pontos_list = []
    logging.info(f"Gerando grade 3D: {xx.shape[0]} x {xx.shape[1]} x {niveis_verticais} pontos")
    pontos_id = 0
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            x_pt = xx[i, j]
            y_pt = yy[i, j]
            elev_base = elevacao_terreno[i, j]
            for k, alt_norm in enumerate(alturas):
                z = elev_base + alt_norm * (altura_maxima - elev_base)
                ponto = {
                    'geometry': Point(x_pt, y_pt),
                    'x': x_pt,
                    'y': y_pt,
                    'z': z,
                    'nivel_vertical': k,
                    'altura_normalizada': alt_norm,
                    'ponto_id': pontos_id,
                    'i': i,
                    'j': j,
                    'elevacao_base': elev_base,
                    'tipo': 'grade',
                    'is_estacao': False
                }
                # Replicar os dados meteorológicos do INMET para cada ponto
                for var_name, value in dados_meteo_inmet.items():
                    ponto[var_name] = value if k == 0 else value * np.exp(-3 * alt_norm)
                pontos_list.append(ponto)
                pontos_id += 1
    pontos_gdf = gpd.GeoDataFrame(pontos_list, crs=crs)
    logging.info(f"Grade criada com {len(pontos_gdf)} pontos")

    # Criar as conexões (arestas) entre os pontos da grade
    logging.info("Criando conexões entre pontos da grade...")
    arestas_list = []
    aresta_id = 0
    pontos_dict = {}
    for idx, row in pontos_gdf[pontos_gdf['tipo'] == 'grade'].iterrows():
        pontos_dict[(row['i'], row['j'], row['nivel_vertical'])] = (idx, row)

    # Conexões horizontais em Y
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1] - 1):
            for k in range(niveis_verticais):
                key1 = (i, j, k)
                key2 = (i, j+1, k)
                if key1 in pontos_dict and key2 in pontos_dict:
                    idx1, row1 = pontos_dict[key1]
                    idx2, row2 = pontos_dict[key2]
                    linha = LineString([(row1['x'], row1['y']), (row2['x'], row2['y'])])
                    aresta = {
                        'geometry': linha,
                        'tipo': 'horizontal',
                        'nivel_vertical': k,
                        'z_medio': (row1['z'] + row2['z']) / 2,
                        'from_id': idx1,
                        'to_id': idx2,
                        'aresta_id': aresta_id
                    }
                    for var_name in dados_meteo_inmet.keys():
                        if var_name in row1 and var_name in row2:
                            aresta[f"{var_name}_media"] = (row1[var_name] + row2[var_name]) / 2
                    arestas_list.append(aresta)
                    aresta_id += 1

    # Conexões horizontais em X
    for i in range(xx.shape[0] - 1):
        for j in range(xx.shape[1]):
            for k in range(niveis_verticais):
                key1 = (i, j, k)
                key2 = (i+1, j, k)
                if key1 in pontos_dict and key2 in pontos_dict:
                    idx1, row1 = pontos_dict[key1]
                    idx2, row2 = pontos_dict[key2]
                    linha = LineString([(row1['x'], row1['y']), (row2['x'], row2['y'])])
                    aresta = {
                        'geometry': linha,
                        'tipo': 'horizontal',
                        'nivel_vertical': k,
                        'z_medio': (row1['z'] + row2['z']) / 2,
                        'from_id': idx1,
                        'to_id': idx2,
                        'aresta_id': aresta_id
                    }
                    for var_name in dados_meteo_inmet.keys():
                        if var_name in row1 and var_name in row2:
                            aresta[f"{var_name}_media"] = (row1[var_name] + row2[var_name]) / 2
                    arestas_list.append(aresta)
                    aresta_id += 1

    # Conexões verticais
    for i in range(xx.shape[0]):
        for j in range(xx.shape[1]):
            for k in range(niveis_verticais - 1):
                key1 = (i, j, k)
                key2 = (i, j, k+1)
                if key1 in pontos_dict and key2 in pontos_dict:
                    idx1, row1 = pontos_dict[key1]
                    idx2, row2 = pontos_dict[key2]
                    linha = LineString([(row1['x'], row1['y']), (row2['x'], row2['y'])])
                    aresta = {
                        'geometry': linha,
                        'tipo': 'vertical',
                        'nivel_inferior': k,
                        'nivel_superior': k+1,
                        'z_medio': (row1['z'] + row2['z']) / 2,
                        'from_id': idx1,
                        'to_id': idx2,
                        'aresta_id': aresta_id
                    }
                    for var_name in dados_meteo_inmet.keys():
                        if var_name in row1 and var_name in row2:
                            aresta[f"{var_name}_media"] = (row1[var_name] + row2[var_name]) / 2
                    arestas_list.append(aresta)
                    aresta_id += 1

    arestas_gdf = gpd.GeoDataFrame(arestas_list, crs=crs)
    logging.info(f"Conexões criadas: {len(arestas_gdf)} arestas")

    return pontos_gdf, arestas_gdf


# =============================================================================
# Visualização e Exportação
# =============================================================================
def visualizar_grade_3d(pontos_gdf: gpd.GeoDataFrame, arestas_gdf: gpd.GeoDataFrame = None,
                        caminho_saida: str = 'visualizacao_grade_atmosferica_3d.png',
                        var_visualizacao: str = 'precipitacao'):
    """
    Gera visualizações 3D e 2D da grade atmosférica com base na variável especificada.
    """
    try:
        from mpl_toolkits.mplot3d import Axes3D

        if var_visualizacao not in pontos_gdf.columns:
            logging.warning(f"Variável '{var_visualizacao}' não encontrada. Usando 'precipitacao' como padrão.")
            var_visualizacao = 'precipitacao'
            if 'precipitacao' not in pontos_gdf.columns:
                for col in pontos_gdf.columns:
                    if pontos_gdf[col].dtype.kind in 'if' and not pontos_gdf[col].isna().all():
                        var_visualizacao = col
                        logging.info(f"Usando variável '{var_visualizacao}' para visualização.")
                        break

        os.makedirs(os.path.dirname(caminho_saida), exist_ok=True)

        if len(pontos_gdf) > 5000:
            logging.info("Amostrando 5000 pontos para visualização 3D.")
            pontos_vis = pontos_gdf.sample(5000, random_state=42)
        else:
            pontos_vis = pontos_gdf

        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        sc = ax.scatter(
            pontos_vis['x'], pontos_vis['y'], pontos_vis['z'],
            c=pontos_vis[var_visualizacao], cmap='Blues', s=5, alpha=0.6
        )
        cbar = plt.colorbar(sc, ax=ax, pad=0.1)
        cbar.set_label(f'{var_visualizacao}')

        estacoes = pontos_gdf[pontos_gdf['is_estacao'] == True]
        if not estacoes.empty:
            ax.scatter(
                estacoes['x'], estacoes['y'], estacoes['z'],
                c='red', s=50, marker='^', label='Estações'
            )
            ax.legend()

        ax.set_xlabel('Longitude')
        ax.set_ylabel('Latitude')
        ax.set_zlabel('Altitude (m)')
        ax.set_title(f'Grade Atmosférica 3D - {var_visualizacao}')
        ax.view_init(elev=30, azim=225)
        plt.tight_layout()
        plt.savefig(caminho_saida, dpi=300, bbox_inches='tight')
        plt.close()
        logging.info(f"Visualização 3D salva em: {caminho_saida}")

        if 'nivel_vertical' in pontos_gdf.columns:
            niveis = sorted(pontos_gdf['nivel_vertical'].unique())
            niveis_vis = [niveis[0]] + [niveis[i] for i in np.linspace(1, len(niveis)-1, min(4, len(niveis)-1)).astype(int)]
            for nivel in niveis_vis:
                fig, ax = plt.subplots(figsize=(10, 8))
                pontos_nivel = pontos_gdf[pontos_gdf['nivel_vertical'] == nivel]
                pontos_nivel.plot(
                    column=var_visualizacao, ax=ax, cmap='Blues',
                    markersize=20, alpha=0.7, legend=True,
                    legend_kwds={'label': f'{var_visualizacao}'}
                )
                ax.set_title(f'{var_visualizacao} - Nível {nivel} (Alt. Média: {pontos_nivel["z"].mean():.0f}m)')
                nivel_path = caminho_saida.replace('.png', f'_nivel{nivel}.png')
                plt.tight_layout()
                plt.savefig(nivel_path, dpi=300, bbox_inches='tight')
                plt.close()
                logging.info(f"Visualização 2D do nível {nivel} salva em: {nivel_path}")

        variaveis_meteo = [col for col in pontos_gdf.columns if col not in [var_visualizacao, 'x', 'y', 'z', 'nivel_vertical', 'i', 'j', 'ponto_id'] and pontos_gdf[col].dtype.kind in 'if' and not pontos_gdf[col].isna().all()]
        for var in variaveis_meteo[:3]:
            var_path = caminho_saida.replace('.png', f'_{var}.png')
            fig, ax = plt.subplots(figsize=(10, 8))
            pontos_gdf.plot(
                column=var, ax=ax, cmap='viridis',
                markersize=10, alpha=0.7, legend=True,
                legend_kwds={'label': f'{var}'}
            )
            ax.set_title(f'Grade Atmosférica - {var}')
            plt.tight_layout()
            plt.savefig(var_path, dpi=300, bbox_inches='tight')
            plt.close()
            logging.info(f"Visualização da variável '{var}' salva em: {var_path}")

    except Exception as e:
        logging.error(f"Erro ao criar visualizações 3D/2D: {e}")
        logging.error(traceback.format_exc())


# =============================================================================
# Função Principal
# =============================================================================
def processar_grade_atmosferica(gpkg_path: str, arquivos_inmet: list = None, arquivos_local: list = None,
                               caminho_saida: str = '/content/drive/MyDrive/GrafosGeoespaciais/output/atmosfera',
                               resolucao_horizontal: float = 500, niveis_verticais: int = 11,
                               altura_maxima: float = 5000, visualizar: bool = True):
    logging.info("Iniciando processamento da grade atmosférica...")

    camadas = listar_camadas_gpkg(gpkg_path)
    if not camadas:
        logging.error(f"Nenhuma camada encontrada em {gpkg_path}")
        return None, None

    topografia = extrair_topografia(gpkg_path)
    limites = extrair_limites(gpkg_path)

    # Processar somente os dados INMET (não utilizamos dados de estação local)
    dados_inmet = None
    if arquivos_inmet:
        for arquivo in arquivos_inmet:
            if os.path.exists(arquivo):
                dados_inmet = processar_dados_inmet(arquivo)
                if dados_inmet:
                    logging.info(f"Usando dados INMET: {dados_inmet['nome']} ({dados_inmet['codigo']})")
                    break
                else:
                    logging.warning(f"Não foi possível processar o arquivo INMET: {arquivo}")

    if dados_inmet is None:
        logging.warning("Nenhum dado meteorológico INMET disponível. A grade terá valores zero.")

    # Neste cenário, não utilizamos dados de estações locais.
    pontos_gdf, arestas_gdf = criar_grade_atmosferica_3d(
        limites=limites,
        topografia=topografia,
        dados_inmet=dados_inmet,
        dados_locais=None,
        resolucao_horizontal=resolucao_horizontal,
        niveis_verticais=niveis_verticais,
        altura_maxima=altura_maxima
    )

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    gpkg_saida = os.path.join(caminho_saida, f'grade_atmosferica_{timestamp}.gpkg')
    exportar_para_gpkg(pontos_gdf, arestas_gdf, gpkg_saida)

    if visualizar:
        vis_path = os.path.join(caminho_saida, f'visualizacao_grade_{timestamp}.png')
        visualizar_grade_3d(pontos_gdf, arestas_gdf, vis_path, 'precipitacao')

    logging.info("Processamento da grade atmosférica concluído com sucesso!")
    return pontos_gdf, arestas_gdf


# =============================================================================
# Bloco Principal (Exemplo de Execução)
# =============================================================================
if __name__ == "__main__":
    # Caminhos de exemplo (ajuste conforme necessário)
    gpkg_path = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_integrados_alt.gpkg'
    caminho_saida = '/content/drive/MyDrive/GrafosGeoespaciais/data/clima/processados'
    arquivo_inmet = '/content/drive/MyDrive/GrafosGeoespaciais/data/clima/INMET_SE_SP_A707_PRESIDENTE PRUDENTE_01-01-2024_A_31-12-2024.CSV'
    arquivos_inmet = [arquivo_inmet] if os.path.exists(arquivo_inmet) else []

    # Neste exemplo, NÃO utilizamos dados de estação local.
    pontos_gdf, arestas_gdf = processar_grade_atmosferica(
        gpkg_path=gpkg_path,
        arquivos_inmet=arquivos_inmet,
        arquivos_local=None,
        caminho_saida=caminho_saida,
        resolucao_horizontal=500,  # 500 m de espaçamento horizontal
        niveis_verticais=11,        # 11 níveis (0 a 5000 m, com 500 m de espaçamento vertical)
        altura_maxima=5000,
        visualizar=True
    )

    logging.info("Execução completa!")

import geopandas as gpd
import fiona
import os
from shapely.geometry import box

# Definir caminhos dos arquivos
arquivo_clima = '/content/drive/MyDrive/GrafosGeoespaciais/data/clima/processados/grade_atmosferica_20250316_222610.gpkg'
arquivo_base = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_integrados_alt.gpkg'

# Listar camadas em ambos os arquivos
print("Camadas no arquivo de clima:")
camadas_clima = fiona.listlayers(arquivo_clima)
for c in camadas_clima:
    print(f" - {c}")

print("\nCamadas no arquivo base:")
camadas_base = fiona.listlayers(arquivo_base)
for c in camadas_base:
    print(f" - {c}")

# Obter os limites de Sorocaba
# Usaremos a camada uso_terra_ocupacao como delimitadora
sorocaba_limites = gpd.read_file(arquivo_base, layer='uso_terra_ocupacao')
bounds = sorocaba_limites.total_bounds

# Função para filtrar geometrias dentro dos limites
def filtrar_por_limites(gdf, total_bounds):
    # Criar um polígono retangular a partir dos limites
    minx, miny, maxx, maxy = total_bounds
    bbox = box(minx, miny, maxx, maxy)

    # Recortar o GeoDataFrame pelo polígono
    return gdf[gdf.geometry.intersects(bbox)]

# Processar cada camada da grade atmosférica
camadas_adicionadas = []
for camada_clima in camadas_clima:
    try:
        # Ler a camada da grade atmosférica
        clima_gdf = gpd.read_file(arquivo_clima, layer=camada_clima)

        # Filtrar para os limites de Sorocaba
        clima_filtrado = filtrar_por_limites(clima_gdf, bounds)

        # Definir nome da nova camada
        novo_nome = f"clima_{camada_clima}"

        # Verificar se já existe no arquivo de destino
        if novo_nome in camadas_base:
            print(f"Aviso: Camada {novo_nome} já existe e será substituída.")

        # Salvar a camada filtrada no arquivo base
        clima_filtrado.to_file(arquivo_base, layer=novo_nome, driver="GPKG")

        # Registrar sucesso
        print(f"Camada {camada_clima} processada e adicionada como {novo_nome}")
        print(f" - Registros originais: {len(clima_gdf)}, Após filtro: {len(clima_filtrado)}")
        camadas_adicionadas.append(novo_nome)

    except Exception as e:
        print(f"Erro ao processar camada {camada_clima}: {e}")

print("\nProcessamento concluído!")
print(f"Total de {len(camadas_adicionadas)} camadas adicionadas:")
for camada in camadas_adicionadas:
    print(f" - {camada}")

# Verificar camadas finais
print("\nCamadas atualizadas no arquivo base:")
camadas_finais = fiona.listlayers(arquivo_base)
for c in camadas_finais:
    print(f" - {c}")

import geopandas as gpd
import fiona
import os
from shapely.ops import unary_union

# Definir caminhos dos arquivos
arquivo_base = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_integrados_alt.gpkg'
arquivo_saida = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_corrigidos.gpkg'

# Verificar se já existe arquivo de saída para evitar sobrescrever
if os.path.exists(arquivo_saida):
    backup_path = arquivo_saida.replace('.gpkg', '_backup.gpkg')
    os.rename(arquivo_saida, backup_path)
    print(f"Arquivo existente movido para: {backup_path}")

# Listar todas as camadas no arquivo
print("Camadas no arquivo original:")
camadas_originais = fiona.listlayers(arquivo_base)
for c in camadas_originais:
    print(f" - {c}")

# Carregar a camada de setores censitários como máscara
print("\nCarregando camada de setores censitários para usar como máscara...")
try:
    setores = gpd.read_file(arquivo_base, layer='setores_censitarios')
    # Criar um único polígono unindo todos os setores para usar como máscara
    mask_polygon = unary_union(setores.geometry)
    print(f"Máscara criada a partir de {len(setores)} setores censitários")
except Exception as e:
    print(f"Erro ao carregar setores censitários: {e}")
    print("Tentando usar outra camada como referência...")
    # Tentar uso_terra_ocupacao como alternativa
    try:
        uso_terra = gpd.read_file(arquivo_base, layer='uso_terra_ocupacao')
        mask_polygon = unary_union(uso_terra.geometry)
        print(f"Máscara alternativa criada a partir de {len(uso_terra)} feições de uso da terra")
    except Exception as e2:
        print(f"Erro ao criar máscara alternativa: {e2}")
        print("Não foi possível criar máscara para recorte. Encerrando.")
        exit(1)

# Identificar camadas problemáticas (atmosfera e curvas de nível)
camadas_problematicas = [c for c in camadas_originais if
                         any(termo in c.lower() for termo in
                            ['atmos', 'clima', 'curv', 'nivel', 'elevation'])]

print(f"\nCamadas identificadas para correção: {camadas_problematicas}")

# Processar cada camada
camadas_processadas = []
for camada in camadas_originais:
    try:
        # Carregar a camada
        gdf = gpd.read_file(arquivo_base, layer=camada)

        # Verificar se é uma camada problemática
        if camada in camadas_problematicas:
            print(f"\nProcessando camada problemática: {camada}")
            print(f" - Feições originais: {len(gdf)}")

            # Recortar a camada pelo polígono da máscara
            gdf_recortado = gpd.clip(gdf, mask_polygon)

            print(f" - Feições após recorte: {len(gdf_recortado)}")

            # Usar a camada recortada
            gdf_final = gdf_recortado
        else:
            print(f"Mantendo camada original: {camada}")
            gdf_final = gdf

        # Salvar a camada processada (recortada ou original) no arquivo de saída
        gdf_final.to_file(arquivo_saida, layer=camada, driver="GPKG")
        camadas_processadas.append(camada)

    except Exception as e:
        print(f"Erro ao processar camada {camada}: {e}")

print("\nProcessamento concluído!")
print(f"Total de {len(camadas_processadas)}/{len(camadas_originais)} camadas processadas")
print(f"Arquivo corrigido salvo em: {arquivo_saida}")

# Verificar camadas no arquivo de saída
print("\nCamadas no arquivo corrigido:")
camadas_finais = fiona.listlayers(arquivo_saida)
for c in camadas_finais:
    print(f" - {c}")

import geopandas as gpd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import fiona
import matplotlib.colors as mcolors
import matplotlib.patches as mpatches
from matplotlib.collections import PathCollection
import contextily as ctx

# Definir caminho do arquivo
arquivo_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_corrigidos.gpkg'

# Listar as camadas disponíveis
camadas = fiona.listlayers(arquivo_gpkg)
print("Camadas disponíveis:")
for camada in camadas:
    print(f" - {camada}")

# Definir cores para cada tipo de camada
cores = {
    # Elementos naturais
    'curvas_nivel_com_elevacao': '#8C510A',  # Marrom (topografia)
    'hidrografia': '#0571B0',                # Azul (água)
    'hidrografia_com_elevacao': '#0571B0',   # Azul (água)

    # Uso da terra
    'uso_terra_ocupacao': '#41B6C4',         # Azul-verde (uso da terra)
    'uso_terra_ocupacao_com_elevacao': '#41B6C4',

    # Infraestrutura
    'rodovias': '#FF7F00',                   # Laranja (rodovias)
    'rodovias_com_elevacao': '#FF7F00',
    'ferrovias': '#762A83',                  # Roxo (ferrovias)
    'ferrovias_com_elevacao': '#762A83',

    # Edificações
    'edificacoes': '#D73027',                # Vermelho (edifícios)
    'edificacoes_com_elevacao': '#D73027',
    'edificacoes_prioritarias': '#FC8D59',    # Vermelho claro (edifícios prioritários)
    'edificacoes_prioritarias_com_elevacao': '#FC8D59',

    # Divisões administrativas
    'setores_censitarios': '#FFFFBF',         # Amarelo claro (divisões administrativas)
    'setores_com_edificacoes': '#FFFFBF',

    # Dados atmosféricos
    'clima_pontos_grade': '#4575B4',          # Azul escuro (pontos atmosféricos)
    'clima_arestas_grade': '#91BFDB'          # Azul claro (conexões atmosféricas)
}

# Definir grupos de camadas para visualização seletiva
grupos_camadas = {
    'base': ['setores_censitarios', 'uso_terra_ocupacao_com_elevacao'],
    'hidrografia': ['hidrografia_com_elevacao'],
    'transportes': ['rodovias_com_elevacao', 'ferrovias_com_elevacao'],
    'edificacoes': ['edificacoes_prioritarias_com_elevacao'],
    'topografia': ['curvas_nivel_com_elevacao'],
    'clima': ['clima_pontos_grade'],
}

# Definir função para criar visualização 3D
def visualizar_3d(camadas_selecionadas, titulo="Visualização 3D - Sorocaba",
                 azimuth=225, elevation=45, amostrar=True, tamanho_figura=(14, 12)):
    """
    Cria visualização 3D das camadas selecionadas do GeoPackage.

    Parameters:
    -----------
    camadas_selecionadas : list
        Lista de nomes das camadas a serem visualizadas.
    titulo : str
        Título do gráfico.
    azimuth : float
        Ângulo azimutal da visualização 3D em graus.
    elevation : float
        Ângulo de elevação da visualização 3D em graus.
    amostrar : bool
        Se deve reduzir o número de pontos para melhorar o desempenho.
    tamanho_figura : tuple
        Tamanho da figura (largura, altura) em polegadas.
    """
    # Criar figura 3D
    fig = plt.figure(figsize=tamanho_figura)
    ax = fig.add_subplot(111, projection='3d')

    # Dicionário para armazenar as legendas
    handles = []

    # Ajustar labels conforme as camadas que estamos visualizando
    labels_grupos = {
        'base': 'Uso da Terra',
        'hidrografia': 'Hidrografia',
        'transportes': 'Infraestrutura de Transporte',
        'edificacoes': 'Edificações Prioritárias',
        'topografia': 'Curvas de Nível',
        'clima': 'Grade Atmosférica'
    }

    # Parâmetros para escala vertical
    exagero_vertical = 5  # Fator de exagero da escala vertical
    z_min, z_max = float('inf'), float('-inf')

    # Carregar e plotar cada camada
    for nome_camada in camadas_selecionadas:
        if nome_camada not in camadas:
            print(f"Aviso: Camada '{nome_camada}' não encontrada no arquivo.")
            continue

        print(f"Carregando camada: {nome_camada}")
        try:
            gdf = gpd.read_file(arquivo_gpkg, layer=nome_camada)

            # Verificar se a camada está vazia
            if gdf.empty:
                print(f"Aviso: Camada '{nome_camada}' está vazia.")
                continue

            # Se for uma camada muito grande, amostrar para melhorar desempenho
            if amostrar and len(gdf) > 1000:
                if 'clima' in nome_camada:
                    gdf = gdf.sample(min(1000, len(gdf)), random_state=42)
                else:
                    gdf = gdf.sample(min(500, len(gdf)), random_state=42)

            # Definir cor da camada
            cor = cores.get(nome_camada, '#333333')  # Cinza escuro como padrão

            # Verificar se a camada tem dados de elevação
            if 'elevation' in gdf.columns:
                # Ajustar escala vertical
                z = gdf['elevation'] * exagero_vertical
                z_min = min(z_min, z.min())
                z_max = max(z_max, z.max())
            else:
                # Atribuir elevação zero se não houver dados
                print(f"A camada '{nome_camada}' não tem dados de elevação. Atribuindo z=0.")
                z = np.zeros(len(gdf))

            # Extrair coordenadas dos centroides
            x = gdf.geometry.centroid.x
            y = gdf.geometry.centroid.y

            # Plotar pontos 3D
            if 'clima_pontos' in nome_camada:
                # Pontos atmosféricos podem ter escala vertical diferente
                scatter = ax.scatter(x, y, z, color=cor, s=10, alpha=0.4, label=nome_camada)
            elif 'arestas' in nome_camada:
                # Para arestas, vamos plotar como linhas
                # Como isso é complexo em 3D, vamos simplificar usando pontos
                scatter = ax.scatter(x, y, z, color=cor, s=1, alpha=0.2, label=nome_camada)
            else:
                scatter = ax.scatter(x, y, z, color=cor, s=20, alpha=0.7, label=nome_camada)

            # Adicionar à legenda
            # Identificar a qual grupo pertence esta camada
            grupo_encontrado = None
            for grupo, lista_camadas in grupos_camadas.items():
                if nome_camada in lista_camadas:
                    grupo_encontrado = grupo
                    break

            # Se for parte de um grupo conhecido, usar o nome do grupo para a legenda
            if grupo_encontrado:
                # Verificar se o grupo já está na legenda
                if not any(item.get_label() == labels_grupos[grupo_encontrado] for item in handles):
                    patch = mpatches.Patch(color=cor, label=labels_grupos[grupo_encontrado])
                    handles.append(patch)
            else:
                # Caso contrário, usar o nome da camada
                patch = mpatches.Patch(color=cor, label=nome_camada)
                handles.append(patch)

            print(f"Camada '{nome_camada}' carregada com sucesso: {len(gdf)} feições.")

        except Exception as e:
            print(f"Erro ao carregar camada '{nome_camada}': {e}")

    # Configurar eixos e legendas
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')
    ax.set_zlabel('Elevação (m)')
    ax.view_init(elev=elevation, azim=azimuth)

    # Definir a escala vertical adequada
    if z_min != float('inf') and z_max != float('-inf'):
        ax.set_zlim(z_min, z_max)

    # Adicionar título
    ax.set_title(titulo, fontsize=16, pad=20)

    # Adicionar legenda
    if handles:
        ax.legend(handles=handles, loc='upper right', frameon=True,
                 framealpha=0.9, fontsize=10)

    plt.tight_layout()
    return fig, ax

# Definir algumas visualizações específicas

# 1. Visualização de base com topografia e edificações prioritárias
print("\nCriando visualização base...")
camadas_base = grupos_camadas['base'] + grupos_camadas['topografia'] + grupos_camadas['edificacoes']
fig1, ax1 = visualizar_3d(camadas_base, "Sorocaba - Topografia e Edificações Prioritárias")
plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_base_3d.png',
            dpi=300, bbox_inches='tight')
plt.close(fig1)

# 2. Visualização da rede de transportes
print("\nCriando visualização de transportes...")
camadas_transporte = grupos_camadas['base'] + grupos_camadas['transportes']
fig2, ax2 = visualizar_3d(camadas_transporte, "Sorocaba - Rede de Transportes",
                         azimuth=200, elevation=35)
plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_transportes_3d.png',
            dpi=300, bbox_inches='tight')
plt.close(fig2)

# 3. Visualização da hidrografia
print("\nCriando visualização de hidrografia...")
camadas_hidro = grupos_camadas['base'] + grupos_camadas['hidrografia'] + grupos_camadas['topografia']
fig3, ax3 = visualizar_3d(camadas_hidro, "Sorocaba - Hidrografia e Topografia",
                         azimuth=240, elevation=40)
plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_hidrografia_3d.png',
            dpi=300, bbox_inches='tight')
plt.close(fig3)

# 4. Visualização da grade atmosférica
print("\nCriando visualização da grade atmosférica...")
camadas_clima = grupos_camadas['base'] + grupos_camadas['clima']
fig4, ax4 = visualizar_3d(camadas_clima, "Sorocaba - Grade Atmosférica",
                         azimuth=210, elevation=30)
plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_clima_3d.png',
            dpi=300, bbox_inches='tight')
plt.close(fig4)

# 5. Visualização integrada completa (pode ser pesada para renderizar)
print("\nCriando visualização integrada completa (modo leve)...")
camadas_completas = []
for grupo in grupos_camadas.values():
    camadas_completas.extend(grupo)

fig5, ax5 = visualizar_3d(camadas_completas, "Sorocaba - Ambiente Integrado 3D",
                         azimuth=235, elevation=35, amostrar=True)
plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_completo_3d.png',
            dpi=300, bbox_inches='tight')
plt.close(fig5)

print("\nVisualização concluída. Imagens salvas no diretório 'plots'.")

import geopandas as gpd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import fiona
from matplotlib.colors import LightSource
import matplotlib.colors as mcolors
from matplotlib.tri import Triangulation
from matplotlib import cm
from scipy.interpolate import griddata
import matplotlib.patches as mpatches
from scipy.spatial import Delaunay
import contextily as ctx
from shapely.geometry import Point, LineString, mapping, shape
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.tri as mtri

# Definir caminho do arquivo
arquivo_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_corrigidos.gpkg'

# Listar as camadas disponíveis
camadas = fiona.listlayers(arquivo_gpkg)
print("Camadas disponíveis:")
for camada in camadas:
    print(f" - {camada}")

# Definir cores para cada tipo de camada
cores = {
    # Elementos naturais
    'curvas_nivel_com_elevacao': '#8C510A',    # Marrom (topografia)
    'hidrografia': '#0571B0',                  # Azul (água)
    'hidrografia_com_elevacao': '#0571B0',     # Azul (água)

    # Uso da terra
    'uso_terra_ocupacao': '#41B6C4',           # Azul-verde (uso da terra)
    'uso_terra_ocupacao_com_elevacao': '#41B6C4',

    # Infraestrutura
    'rodovias': '#FF7F00',                     # Laranja (rodovias)
    'rodovias_com_elevacao': '#FF7F00',
    'ferrovias': '#762A83',                    # Roxo (ferrovias)
    'ferrovias_com_elevacao': '#762A83',

    # Edificações
    'edificacoes': '#D73027',                  # Vermelho (edifícios)
    'edificacoes_com_elevacao': '#D73027',
    'edificacoes_prioritarias': '#FC8D59',     # Vermelho claro (edifícios prioritários)
    'edificacoes_prioritarias_com_elevacao': '#FC8D59',

    # Divisões administrativas
    'setores_censitarios': '#FFFFBF',          # Amarelo claro (setores)
    'setores_com_edificacoes': '#FFFFBF',

    # Dados atmosféricos
    'clima_pontos_grade': '#4575B4',           # Azul escuro (pontos atmosféricos)
    'clima_arestas_grade': '#91BFDB'           # Azul claro (conexões atmosféricas)
}

# Inicialização de funções para a visualização 3D aprimorada
def criar_superficie_3d(gdf_curvas, resolucao=100, metodo='linear'):
    """
    Cria uma superfície 3D a partir de curvas de nível.

    Parameters:
    -----------
    gdf_curvas : GeoDataFrame
        GeoDataFrame contendo curvas de nível com coluna 'elevation'
    resolucao : int
        Resolução da grade (número de células em cada direção)
    metodo : str
        Método de interpolação ('linear', 'cubic', 'nearest')

    Returns:
    --------
    X, Y : arrays 2D de pontos da grade
    Z : array 2D de elevações
    """
    # Extrair pontos e valores de elevação das curvas de nível
    pontos = []
    valores = []

    # Verificar se temos curvas suficientes
    if len(gdf_curvas) < 3:
        print("Aviso: Poucas curvas de nível disponíveis para interpolação")
        return None, None, None

    # Extrair pontos e valores das curvas
    for _, curva in gdf_curvas.iterrows():
        if curva.geometry is None:
            continue

        z = curva['elevation']

        if curva.geometry.geom_type == 'LineString':
            for x, y in curva.geometry.coords:
                pontos.append([x, y])
                valores.append(z)
        elif curva.geometry.geom_type == 'MultiLineString':
            for linha in curva.geometry.geoms:
                for x, y in linha.coords:
                    pontos.append([x, y])
                    valores.append(z)

    if not pontos:
        print("Aviso: Nenhum ponto extraído das curvas de nível")
        return None, None, None

    # Converter para arrays numpy
    pontos = np.array(pontos)
    valores = np.array(valores)

    # Definir os limites da grade
    x_min, y_min = pontos.min(axis=0)
    x_max, y_max = pontos.max(axis=0)

    print(f"Limites da grade: X={x_min:.2f} a {x_max:.2f}, Y={y_min:.2f} a {y_max:.2f}")
    print(f"Elevação: {valores.min():.1f}m a {valores.max():.1f}m")

    # Criar grade regular
    x = np.linspace(x_min, x_max, resolucao)
    y = np.linspace(y_min, y_max, resolucao)
    X, Y = np.meshgrid(x, y)

    # Interpolar valores na grade
    Z = griddata(pontos, valores, (X, Y), method=metodo, fill_value=valores.min())

    return X, Y, Z

def visualizar_terreno_3d(X, Y, Z, camadas_selecionadas, titulo="Modelo 3D do Terreno - Sorocaba",
                        cmap='terrain', exagero_vertical=5, alpha_superficie=0.8,
                        azimuth=225, elevation=45, tamanho_figura=(14, 12)):
    """
    Cria visualização 3D do terreno com as camadas selecionadas.

    Parameters:
    -----------
    X, Y, Z : arrays numpy
        Grade de pontos e elevações para a superfície
    camadas_selecionadas : dict
        Dicionário com camadas a serem plotadas {nome_camada: cor}
    titulo : str
        Título do gráfico
    cmap : str ou Colormap
        Esquema de cores para a superfície
    exagero_vertical : float
        Fator de exagero para a escala vertical
    alpha_superficie : float
        Transparência da superfície (0 a 1)
    azimuth, elevation : float
        Ângulos de visualização em graus
    tamanho_figura : tuple
        Tamanho da figura (largura, altura) em polegadas
    """
    fig = plt.figure(figsize=tamanho_figura)
    ax = fig.add_subplot(111, projection='3d')

    # Lista para armazenar elementos da legenda
    handles_legenda = []

    # Plotar superfície do terreno
    if X is not None and Y is not None and Z is not None:
        # Aplicar exagero vertical
        Z_exag = Z * exagero_vertical

        # Criar iluminação para efeito de relevo
        ls = LightSource(azdeg=315, altdeg=45)
        rgb = ls.shade(Z, cmap=cm.get_cmap(cmap), vert_exag=0.1, blend_mode='soft')

        # Plotar superfície
        surf = ax.plot_surface(X, Y, Z_exag, facecolors=rgb,
                            rstride=1, cstride=1,
                            linewidth=0, antialiased=True,
                            alpha=alpha_superficie)

        # Armazenar elevação mínima e máxima
        z_min, z_max = Z_exag.min(), Z_exag.max()

        # Adicionar à legenda
        terreno_patch = mpatches.Patch(color=cm.get_cmap(cmap)(0.5),
                                    label='Terreno')
        handles_legenda.append(terreno_patch)

        print(f"Superfície do terreno plotada com elevação de {Z.min():.1f}m a {Z.max():.1f}m")
        print(f"Após exagero vertical: {z_min:.1f}m a {z_max:.1f}m")
    else:
        print("Aviso: Dados insuficientes para criar superfície do terreno")
        # Definir valores padrão para os limites dos eixos
        z_min, z_max = 0, 1000

    # Carregar e plotar as camadas selecionadas
    for nome_camada, cor in camadas_selecionadas.items():
        try:
            # Carregar a camada
            gdf = gpd.read_file(arquivo_gpkg, layer=nome_camada)

            # Verificar se a camada está vazia
            if gdf.empty:
                print(f"Aviso: Camada '{nome_camada}' está vazia.")
                continue

            # Verificar se a camada tem dados de elevação
            if 'elevation' not in gdf.columns:
                print(f"Aviso: A camada '{nome_camada}' não tem dados de elevação. Atribuindo valor estimado.")
                # Usar Z da superfície interpolada para estimar elevação
                if X is not None and Y is not None and Z is not None:
                    # Obter centroides de cada geometria
                    gdf['centroid_x'] = gdf.geometry.centroid.x
                    gdf['centroid_y'] = gdf.geometry.centroid.y

                    # Estimar elevação para cada centroide
                    gdf['elevation'] = np.nan
                    for idx, row in gdf.iterrows():
                        # Encontrar o ponto mais próximo na grade
                        x_diff = X[0,:] - row['centroid_x']
                        y_diff = Y[:,0] - row['centroid_y']
                        x_idx = np.argmin(np.abs(x_diff))
                        y_idx = np.argmin(np.abs(y_diff))

                        # Atribuir elevação do ponto mais próximo
                        gdf.at[idx, 'elevation'] = Z[y_idx, x_idx]
                else:
                    # Se não temos a superfície, usar um valor padrão
                    gdf['elevation'] = 0

            # Aplicar exagero vertical
            z = gdf['elevation'] * exagero_vertical

            # Extrair coordenadas dos centroides
            x = gdf.geometry.centroid.x
            y = gdf.geometry.centroid.y

            # Determinar o estilo com base no tipo da camada
            if 'hidrografia' in nome_camada:
                # Hidrografia em azul
                scatter = ax.scatter(x, y, z, color=cor, s=10, alpha=0.9,
                                  label=nome_camada.replace('_com_elevacao', ''))

                # Adicionar à legenda
                if 'Hidrografia' not in [h.get_label() for h in handles_legenda]:
                    hidro_patch = mpatches.Patch(color=cor, label='Hidrografia')
                    handles_legenda.append(hidro_patch)

            elif 'rodovias' in nome_camada:
                # Rodovias em laranja
                scatter = ax.scatter(x, y, z, color=cor, s=15, alpha=0.9,
                                  label=nome_camada.replace('_com_elevacao', ''))

                # Adicionar à legenda
                if 'Rodovias' not in [h.get_label() for h in handles_legenda]:
                    rod_patch = mpatches.Patch(color=cor, label='Rodovias')
                    handles_legenda.append(rod_patch)

            elif 'ferrovia' in nome_camada:
                # Ferrovias em roxo
                scatter = ax.scatter(x, y, z, color=cor, s=15, alpha=0.9,
                                  label=nome_camada.replace('_com_elevacao', ''))

                # Adicionar à legenda
                if 'Ferrovias' not in [h.get_label() for h in handles_legenda]:
                    ferro_patch = mpatches.Patch(color=cor, label='Ferrovias')
                    handles_legenda.append(ferro_patch)

            elif 'edificacoes' in nome_camada:
                # Edificações em vermelho
                scatter = ax.scatter(x, y, z, color=cor, s=20, alpha=0.9,
                                  label=nome_camada.replace('_com_elevacao', ''))

                # Adicionar à legenda
                if 'Edificações' not in [h.get_label() for h in handles_legenda]:
                    edificio_patch = mpatches.Patch(color=cor, label='Edificações')
                    handles_legenda.append(edificio_patch)

            elif 'clima' in nome_camada:
                # Pontos climáticos em azul escuro
                scatter = ax.scatter(x, y, z, color=cor, s=5, alpha=0.3,
                                  label=nome_camada)

                # Adicionar à legenda
                if 'Clima' not in [h.get_label() for h in handles_legenda]:
                    clima_patch = mpatches.Patch(color=cor, label='Grade Atmosférica')
                    handles_legenda.append(clima_patch)

            else:
                # Outros elementos
                scatter = ax.scatter(x, y, z, color=cor, s=15, alpha=0.7,
                                  label=nome_camada.replace('_com_elevacao', ''))

            print(f"Camada '{nome_camada}' plotada com {len(gdf)} feições.")

        except Exception as e:
            print(f"Erro ao processar camada '{nome_camada}': {e}")
            import traceback
            traceback.print_exc()

    # Configurar eixos e legendas
    ax.set_xlabel('X (Este)')
    ax.set_ylabel('Y (Norte)')
    ax.set_zlabel('Elevação (m)')

    # Ajustar os limites do eixo Z
    ax.set_zlim(z_min, z_max * 1.1)

    # Definir o ponto de vista
    ax.view_init(elev=elevation, azim=azimuth)

    # Adicionar título
    ax.set_title(titulo, fontsize=16, pad=20)

    # Adicionar legenda
    if handles_legenda:
        ax.legend(handles=handles_legenda, loc='upper right',
                frameon=True, framealpha=0.9)

    plt.tight_layout()
    return fig, ax

# Função principal para criar visualizações
def criar_visualizacoes(camadas_superficie=None, camadas_adicionais=None):
    """
    Cria visualizações 3D do terreno com diferentes configurações.

    Parameters:
    -----------
    camadas_superficie : list
        Lista de camadas a serem usadas para criar a superfície do terreno
    camadas_adicionais : dict
        Dicionário de camadas adicionais e suas cores
    """
    # Se não forem especificadas, usar curvas de nível para a superfície
    if camadas_superficie is None:
        camadas_superficie = ['curvas_nivel_com_elevacao']

    # Camadas adicionais padrão se não especificadas
    if camadas_adicionais is None:
        camadas_adicionais = {
            'hidrografia_com_elevacao': cores['hidrografia_com_elevacao'],
            'rodovias_com_elevacao': cores['rodovias_com_elevacao'],
            'edificacoes_prioritarias_com_elevacao': cores['edificacoes_prioritarias_com_elevacao']
        }

    # Carregar dados para criar a superfície
    superficie_gdf = None
    for camada in camadas_superficie:
        try:
            temp_gdf = gpd.read_file(arquivo_gpkg, layer=camada)
            if superficie_gdf is None:
                superficie_gdf = temp_gdf
            else:
                superficie_gdf = pd.concat([superficie_gdf, temp_gdf])
        except Exception as e:
            print(f"Erro ao carregar camada '{camada}' para superfície: {e}")

    if superficie_gdf is None or len(superficie_gdf) < 3:
        print("Erro: Dados insuficientes para criar superfície do terreno")
        return

    # Criar grade para a superfície
    X, Y, Z = criar_superficie_3d(superficie_gdf, resolucao=150, metodo='linear')

    # 1. Visualização base: terreno + hidrografia + rodovias + edificações
    print("\nCriando visualização base...")
    fig1, ax1 = visualizar_terreno_3d(
        X, Y, Z,
        camadas_adicionais,
        "Sorocaba - Modelo Digital do Terreno com Camadas Integradas",
        exagero_vertical=10,
        elevation=35
    )
    plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_terreno_3d.png',
                dpi=300, bbox_inches='tight')
    print("Visualização salva como 'sorocaba_terreno_3d.png'")
    plt.close(fig1)

    # 2. Visualização de hidrografia com terreno
    print("\nCriando visualização de hidrografia...")
    fig2, ax2 = visualizar_terreno_3d(
        X, Y, Z,
        {'hidrografia_com_elevacao': cores['hidrografia_com_elevacao']},
        "Sorocaba - Hidrografia no Contexto do Terreno",
        cmap='Blues',
        exagero_vertical=10,
        alpha_superficie=0.7,
        elevation=40
    )
    plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_hidrografia_terreno_3d.png',
                dpi=300, bbox_inches='tight')
    print("Visualização salva como 'sorocaba_hidrografia_terreno_3d.png'")
    plt.close(fig2)

    # 3. Visualização de transportes com terreno
    print("\nCriando visualização de transportes...")
    fig3, ax3 = visualizar_terreno_3d(
        X, Y, Z,
        {
            'rodovias_com_elevacao': cores['rodovias_com_elevacao'],
            'ferrovias_com_elevacao': cores['ferrovias_com_elevacao']
        },
        "Sorocaba - Rede de Transportes no Contexto do Terreno",
        cmap='gist_earth',
        exagero_vertical=10,
        alpha_superficie=0.7,
        elevation=35
    )
    plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_transportes_terreno_3d.png',
                dpi=300, bbox_inches='tight')
    print("Visualização salva como 'sorocaba_transportes_terreno_3d.png'")
    plt.close(fig3)

    # 4. Visualização integrada para PITCIC
    print("\nCriando visualização integrada para PITCIC...")
    fig4, ax4 = visualizar_terreno_3d(
        X, Y, Z,
        {
            'hidrografia_com_elevacao': cores['hidrografia_com_elevacao'],
            'rodovias_com_elevacao': cores['rodovias_com_elevacao'],
            'edificacoes_prioritarias_com_elevacao': cores['edificacoes_prioritarias_com_elevacao'],
            'clima_pontos_grade': cores['clima_pontos_grade']
        },
        "Sorocaba - Visualização Integrada para PITCIC",
        cmap='terrain',
        exagero_vertical=10,
        alpha_superficie=0.7,
        elevation=40
    )
    plt.savefig('/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/plots/sorocaba_pitcic_3d.png',
                dpi=300, bbox_inches='tight')
    print("Visualização salva como 'sorocaba_pitcic_3d.png'")
    plt.close(fig4)

# Executar a função principal
criar_visualizacoes()

print("\nProcessamento completo. Visualizações salvas no diretório 'plots'.")

import geopandas as gpd
import pandas as pd
import numpy as np
import fiona
import os
import json
from shapely.geometry import Point, LineString, Polygon, mapping
from collections import defaultdict

# Definir caminho do arquivo GeoPackage integrado
arquivo_gpkg = '/content/drive/MyDrive/GrafosGeoespaciais/data/terreno/processados/sorocaba_dados_corrigidos.gpkg'
diretorio_saida = '/content/drive/MyDrive/GrafosGeoespaciais/data/analise'

# Criar diretório de saída se não existir
os.makedirs(diretorio_saida, exist_ok=True)

# Função para analisar detalhadamente uma camada
def analisar_camada_detalhada(nome_camada, arquivo_gpkg):
    """
    Realiza uma análise detalhada de uma camada do GeoPackage.

    Parâmetros:
    -----------
    nome_camada : str
        Nome da camada a ser analisada
    arquivo_gpkg : str
        Caminho para o arquivo GeoPackage

    Retorna:
    --------
    dict
        Dicionário com as informações da análise
    """
    try:
        # Carregar a camada
        gdf = gpd.read_file(arquivo_gpkg, layer=nome_camada)

        # Informações básicas
        analise = {
            "nome_camada": nome_camada,
            "num_feicoes": len(gdf),
            "sistema_coordenadas": str(gdf.crs),
            "tipos_geometria": gdf.geometry.geom_type.value_counts().to_dict(),
            "atributos": {},
            "estatisticas_numericas": {},
            "valores_categoricos": {},
            "estatisticas_espaciais": {},
            "amostra_dados": []
        }

        # Analisar cada atributo
        for col in gdf.columns:
            if col != 'geometry':
                # Informações do atributo
                tipo = str(gdf[col].dtype)
                n_validos = gdf[col].count()
                n_nulos = gdf[col].isna().sum()
                pct_validos = (n_validos / len(gdf)) * 100 if len(gdf) > 0 else 0

                analise["atributos"][col] = {
                    "tipo": tipo,
                    "valores_validos": int(n_validos),
                    "valores_nulos": int(n_nulos),
                    "porcentagem_validos": float(pct_validos)
                }

                # Estatísticas para atributos numéricos
                if pd.api.types.is_numeric_dtype(gdf[col]):
                    try:
                        estatisticas = {
                            "minimo": float(gdf[col].min()) if not pd.isna(gdf[col].min()) else None,
                            "maximo": float(gdf[col].max()) if not pd.isna(gdf[col].max()) else None,
                            "media": float(gdf[col].mean()) if not pd.isna(gdf[col].mean()) else None,
                            "mediana": float(gdf[col].median()) if not pd.isna(gdf[col].median()) else None,
                            "desvio_padrao": float(gdf[col].std()) if not pd.isna(gdf[col].std()) else None,
                            "quartis": {
                                "q1": float(gdf[col].quantile(0.25)) if not pd.isna(gdf[col].quantile(0.25)) else None,
                                "q3": float(gdf[col].quantile(0.75)) if not pd.isna(gdf[col].quantile(0.75)) else None
                            }
                        }
                        analise["estatisticas_numericas"][col] = estatisticas
                    except Exception as e:
                        analise["estatisticas_numericas"][col] = {"erro": str(e)}

                # Valores categóricos (para colunas não numéricas ou numéricas com poucos valores distintos)
                if not pd.api.types.is_numeric_dtype(gdf[col]) or gdf[col].nunique() < 20:
                    try:
                        valores = gdf[col].value_counts().head(20).to_dict()  # Limitar a 20 valores mais comuns

                        # Converter tipos não serializáveis para string
                        valores_formatados = {}
                        for k, v in valores.items():
                            if pd.isna(k):
                                key = "NA"
                            elif isinstance(k, (np.integer, np.floating)):
                                key = float(k)
                            else:
                                key = str(k)
                            valores_formatados[key] = int(v)

                        analise["valores_categoricos"][col] = valores_formatados
                    except Exception as e:
                        analise["valores_categoricos"][col] = {"erro": str(e)}

        # Estatísticas espaciais
        try:
            bounds = gdf.total_bounds
            centroide = gdf.unary_union.centroid

            analise["estatisticas_espaciais"] = {
                "extensao": {
                    "min_x": float(bounds[0]),
                    "min_y": float(bounds[1]),
                    "max_x": float(bounds[2]),
                    "max_y": float(bounds[3])
                },
                "area_total": float(gdf.unary_union.area) if hasattr(gdf.unary_union, 'area') else None,
                "perimetro_total": float(gdf.unary_union.length) if hasattr(gdf.unary_union, 'length') else None,
                "centroide": {
                    "x": float(centroide.x),
                    "y": float(centroide.y)
                } if centroide else None
            }
        except Exception as e:
            analise["estatisticas_espaciais"] = {"erro": str(e)}

        # Coletar amostra de dados (primeiras 5 feições sem geometria)
        try:
            amostra = gdf.head(5).drop(columns='geometry').to_dict('records')
            analise["amostra_dados"] = amostra
        except Exception as e:
            analise["amostra_dados"] = [{"erro": str(e)}]

        return analise

    except Exception as e:
        return {
            "nome_camada": nome_camada,
            "erro": str(e)
        }

# Listar todas as camadas disponíveis no GeoPackage
print("Listando camadas do GeoPackage...")
camadas = fiona.listlayers(arquivo_gpkg)
print(f"Total de camadas encontradas: {len(camadas)}")
for i, camada in enumerate(camadas):
    print(f"{i+1}. {camada}")

# Analisar cada camada e armazenar resultados
print("\nIniciando análise detalhada de cada camada...")
resultados_analise = {}

for camada in camadas:
    print(f"Analisando camada: {camada}")
    analise = analisar_camada_detalhada(camada, arquivo_gpkg)
    resultados_analise[camada] = analise
    print(f"  - {analise['num_feicoes'] if 'num_feicoes' in analise else 'ERRO'} feições processadas")

# Salvar resultados em formato JSON
caminho_resultados = os.path.join(diretorio_saida, "analise_detalhada_gpkg.json")
with open(caminho_resultados, 'w', encoding='utf-8') as f:
    json.dump(resultados_analise, f, indent=2, ensure_ascii=False)
print(f"\nResultados completos salvos em: {caminho_resultados}")

# Gerar relatório resumido em formato tabular
print("\nCriando relatório resumido em formato CSV...")

# Resumo por camada
resumo_camadas = []
for nome_camada, analise in resultados_analise.items():
    if 'erro' in analise:
        resumo_camadas.append({
            'camada': nome_camada,
            'num_feicoes': 'ERRO',
            'tipos_geometria': 'ERRO',
            'num_atributos': 'ERRO',
            'erro': analise['erro']
        })
    else:
        tipos_geom = ', '.join(analise['tipos_geometria'].keys())
        resumo_camadas.append({
            'camada': nome_camada,
            'num_feicoes': analise['num_feicoes'],
            'tipos_geometria': tipos_geom,
            'num_atributos': len(analise['atributos']),
            'sistema_coordenadas': analise['sistema_coordenadas']
        })

df_resumo_camadas = pd.DataFrame(resumo_camadas)
caminho_resumo = os.path.join(diretorio_saida, "resumo_camadas.csv")
df_resumo_camadas.to_csv(caminho_resumo, index=False, encoding='utf-8')
print(f"Resumo das camadas salvo em: {caminho_resumo}")

# Resumo de atributos
atributos_resumo = []
for nome_camada, analise in resultados_analise.items():
    if 'atributos' in analise:
        for atributo, info in analise['atributos'].items():
            row = {
                'camada': nome_camada,
                'atributo': atributo,
                'tipo': info['tipo'],
                'valores_validos': info['valores_validos'],
                'porcentagem_validos': info['porcentagem_validos']
            }

            # Adicionar estatísticas se disponíveis
            if 'estatisticas_numericas' in analise and atributo in analise['estatisticas_numericas']:
                est = analise['estatisticas_numericas'][atributo]
                if 'erro' not in est:
                    row.update({
                        'minimo': est['minimo'],
                        'maximo': est['maximo'],
                        'media': est['media'],
                        'desvio_padrao': est['desvio_padrao']
                    })

            atributos_resumo.append(row)

df_atributos = pd.DataFrame(atributos_resumo)
caminho_atributos = os.path.join(diretorio_saida, "resumo_atributos.csv")
df_atributos.to_csv(caminho_atributos, index=False, encoding='utf-8')
print(f"Resumo dos atributos salvo em: {caminho_atributos}")

# Informações para construção de grafos
print("\nInformações relevantes para construção de grafos:")

# Exibir número de feições por camada
print("\nNúmero de feições por camada:")
for camada, analise in resultados_analise.items():
    if 'num_feicoes' in analise:
        print(f"  - {camada}: {analise['num_feicoes']}")

# Identificar atributos importantes para matriz de features
print("\nAtributos candidatos para a matriz de features por camada:")
for camada, analise in resultados_analise.items():
    if 'atributos' in analise:
        atributos_relevantes = []
        for atributo, info in analise['atributos'].items():
            # Considerar atributos com boa cobertura
            if info['porcentagem_validos'] > 80:
                atributos_relevantes.append(atributo)

        print(f"  - {camada}: {', '.join(atributos_relevantes) if atributos_relevantes else 'Nenhum atributo com boa cobertura'}")

print("\nAnálise completa concluída. Os resultados detalhados estão disponíveis nos arquivos CSV e JSON gerados.")

import pandas as pd
import geopandas as gpd
import numpy as np
import json
import torch
from torch_geometric.data import Data, HeteroData
import networkx as nx
from scipy.spatial import Delaunay
from shapely.geometry import Point, LineString, Polygon, MultiPolygon
import matplotlib.pyplot as plt

# ===============================================
# 1. Funções de carregamento de dados
# ===============================================

def carregar_dados_json(filepath):
    """Carrega dados do geopackage a partir de uma análise detalhada em JSON"""
    with open(filepath, 'r', encoding='utf-8') as f:
        dados = json.load(f)
    return dados

def extrair_camadas(dados_json):
    """Extrai informações das camadas do JSON"""
    camadas = {}
    for nome_camada, info in dados_json.items():
        camadas[nome_camada] = {
            'num_feicoes': info['num_feicoes'],
            'sistema_coordenadas': info['sistema_coordenadas'],
            'tipos_geometria': info['tipos_geometria'],
            'atributos': info['atributos'],
            'estatisticas_numericas': info.get('estatisticas_numericas', {}),
            'valores_categoricos': info.get('valores_categoricos', {}),
            'estatisticas_espaciais': info.get('estatisticas_espaciais', {})
        }
    return camadas

# ===============================================
# 2. Funções de construção da camada de terreno
# ===============================================

def criar_grade_terreno(bbox, cell_size=100):
    """Cria uma grade regular para representar o terreno"""
    xmin, ymin, xmax, ymax = bbox
    x_points = np.arange(xmin, xmax + cell_size, cell_size)
    y_points = np.arange(ymin, ymax + cell_size, cell_size)

    nodes = []
    for i, x in enumerate(x_points[:-1]):
        for j, y in enumerate(y_points[:-1]):
            centroid_x = x + cell_size / 2
            centroid_y = y + cell_size / 2
            nodes.append({
                'x': centroid_x,
                'y': centroid_y,
                'cell_i': i,
                'cell_j': j
            })

    return pd.DataFrame(nodes)

def extrair_atributos_terreno(grade_terreno, dados_json):
    """Extrai atributos da camada de terreno para cada célula da grade"""
    # Em um caso real, seria necessário cruzar espacialmente com os dados vetoriais
    # Simulação de atributos com base nas estatísticas
    uso_terra = dados_json['uso_terra_ocupacao']
    hidrografia = dados_json['hidrografia']

    # Atribuição aleatória baseada nas estatísticas
    n_cells = len(grade_terreno)
    grade_terreno['elevacao'] = np.random.uniform(
        uso_terra['estatisticas_numericas']['elevation']['minimo'],
        uso_terra['estatisticas_numericas']['elevation']['maximo'],
        n_cells
    )

    # Classes de uso do solo baseadas nas estatísticas
    classes_uso = list(uso_terra['valores_categoricos']['class'].keys())
    probs_uso = list(uso_terra['valores_categoricos']['class'].values())
    probs_uso = [p/sum(probs_uso) for p in probs_uso]
    grade_terreno['uso_solo'] = np.random.choice(classes_uso, n_cells, p=probs_uso)

    # Indicador de presença de hidrografia (1-sim, 0-não)
    grade_terreno['hidrografia'] = np.random.binomial(1, 0.1, n_cells)

    return grade_terreno

def criar_arestas_terreno(grade_terreno, max_dist=150):
    """Cria arestas entre células adjacentes da grade de terreno"""
    points = grade_terreno[['x', 'y']].values
    # Usar triangulação de Delaunay para criar conexões
    if len(points) > 3:  # Delaunay requer pelo menos 4 pontos
        tri = Delaunay(points)
        edges = set()

        for simplex in tri.simplices:
            for i in range(3):
                for j in range(i+1, 3):
                    idx_i, idx_j = simplex[i], simplex[j]
                    # Calcular distância
                    dist = np.sqrt(((points[idx_i] - points[idx_j])**2).sum())
                    if dist <= max_dist:
                        # Adicionar aresta (menor índice primeiro para evitar duplicatas)
                        if idx_i < idx_j:
                            edges.add((idx_i, idx_j, dist))
                        else:
                            edges.add((idx_j, idx_i, dist))

        # Converter para DataFrame
        edges_df = pd.DataFrame(list(edges), columns=['source', 'target', 'distance'])

        # Adicionar atributos das arestas
        edges_df['weight'] = edges_df['distance']
        edges_df['tipo'] = 'espacial'

        return edges_df
    else:
        return pd.DataFrame(columns=['source', 'target', 'distance', 'weight', 'tipo'])

# ===============================================
# 3. Funções de construção da camada climática
# ===============================================

def extrair_camada_climatica(dados_json):
    """Extrai pontos e arestas da camada climática"""
    clima_pontos = dados_json['clima_pontos_grade']

    # Selecionar apenas um nível vertical para simplificar (nível 5 - intermediário)
    # Em um caso real, poderia incluir todos os níveis ou selecionar conforme necessidade
    nivel_selecionado = 5

    # Simular extração dos pontos climáticos (em um caso real, seria necessário carregar os dados reais)
    n_pontos = 1000  # Número reduzido para simplificar

    pontos_clima = pd.DataFrame({
        'x': np.random.uniform(
            clima_pontos['estatisticas_espaciais']['extensao']['min_x'],
            clima_pontos['estatisticas_espaciais']['extensao']['max_x'],
            n_pontos
        ),
        'y': np.random.uniform(
            clima_pontos['estatisticas_espaciais']['extensao']['min_y'],
            clima_pontos['estatisticas_espaciais']['extensao']['max_y'],
            n_pontos
        ),
        'nivel_vertical': nivel_selecionado,
        'precipitacao': np.random.uniform(
            clima_pontos['estatisticas_numericas']['precipitacao']['minimo'],
            clima_pontos['estatisticas_numericas']['precipitacao']['maximo'],
            n_pontos
        ),
        'temperatura': np.random.uniform(
            clima_pontos['estatisticas_numericas']['TEMPERATURA DO PONTO DE ORVALHO (°C)']['minimo'],
            clima_pontos['estatisticas_numericas']['TEMPERATURA DO PONTO DE ORVALHO (°C)']['maximo'],
            n_pontos
        ),
        'vento': np.random.uniform(
            clima_pontos['estatisticas_numericas']['VENTO, RAJADA MAXIMA (m/s)']['minimo'],
            clima_pontos['estatisticas_numericas']['VENTO, RAJADA MAXIMA (m/s)']['maximo'],
            n_pontos
        ),
        'elevacao': np.random.uniform(
            clima_pontos['estatisticas_numericas']['elevacao_base']['minimo'],
            clima_pontos['estatisticas_numericas']['elevacao_base']['maximo'],
            n_pontos
        )
    })

    return pontos_clima

def criar_arestas_clima(pontos_clima, max_dist=1000):
    """Cria arestas entre pontos próximos da grade climática"""
    points = pontos_clima[['x', 'y']].values

    if len(points) > 3:
        tri = Delaunay(points)
        edges = set()

        for simplex in tri.simplices:
            for i in range(3):
                for j in range(i+1, 3):
                    idx_i, idx_j = simplex[i], simplex[j]
                    # Calcular distância
                    dist = np.sqrt(((points[idx_i] - points[idx_j])**2).sum())
                    if dist <= max_dist:
                        # Adicionar aresta
                        if idx_i < idx_j:
                            edges.add((idx_i, idx_j, dist))
                        else:
                            edges.add((idx_j, idx_i, dist))

        # Converter para DataFrame
        edges_df = pd.DataFrame(list(edges), columns=['source', 'target', 'distance'])

        # Adicionar atributos das arestas
        edges_df['weight'] = edges_df['distance']
        edges_df['tipo'] = 'clima'

        # Adicionar média de atributos climáticos entre os nós conectados
        edges_df['precipitacao_media'] = edges_df.apply(
            lambda row: (pontos_clima.iloc[row['source']]['precipitacao'] +
                         pontos_clima.iloc[row['target']]['precipitacao']) / 2,
            axis=1
        )

        edges_df['vento_medio'] = edges_df.apply(
            lambda row: (pontos_clima.iloc[row['source']]['vento'] +
                         pontos_clima.iloc[row['target']]['vento']) / 2,
            axis=1
        )

        return edges_df
    else:
        return pd.DataFrame(columns=['source', 'target', 'distance', 'weight', 'tipo',
                                    'precipitacao_media', 'vento_medio'])

# ===============================================
# 4. Funções de construção da camada civil
# ===============================================

def extrair_camada_civil(dados_json):
    """Extrai elementos da camada civil (edificações prioritárias e setores)"""
    edificacoes = dados_json['edificacoes_prioritarias']
    setores = dados_json['setores_censitarios']

    # Número de edificações prioritárias
    n_edificacoes = edificacoes['num_feicoes']

    # Simulação de pontos para edificações prioritárias
    edificacoes_df = pd.DataFrame({
        'x': np.random.uniform(
            edificacoes['estatisticas_espaciais']['extensao']['min_x'],
            edificacoes['estatisticas_espaciais']['extensao']['max_x'],
            n_edificacoes
        ),
        'y': np.random.uniform(
            edificacoes['estatisticas_espaciais']['extensao']['min_y'],
            edificacoes['estatisticas_espaciais']['extensao']['max_y'],
            n_edificacoes
        ),
        'tipo': np.random.choice(
            list(edificacoes['valores_categoricos']['building_type'].keys()),
            n_edificacoes,
            p=[v/sum(edificacoes['valores_categoricos']['building_type'].values())
               for v in edificacoes['valores_categoricos']['building_type'].values()]
        ),
        'prioridade': np.random.choice(
            list(map(float, edificacoes['valores_categoricos']['priority'].keys())),
            n_edificacoes,
            p=[v/sum(edificacoes['valores_categoricos']['priority'].values())
               for v in edificacoes['valores_categoricos']['priority'].values()]
        ),
        'elevacao': np.random.uniform(
            edificacoes['estatisticas_numericas']['elevation']['minimo'],
            edificacoes['estatisticas_numericas']['elevation']['maximo'],
            n_edificacoes
        ),
        'is_edificacao': 1  # Flag para identificar como edificação prioritária
    })

    # Simulação reduzida de centroides para setores censitários
    n_setores = 500  # Reduzido para simplificar

    setores_df = pd.DataFrame({
        'x': np.random.uniform(
            setores['estatisticas_espaciais']['extensao']['min_x'],
            setores['estatisticas_espaciais']['extensao']['max_x'],
            n_setores
        ),
        'y': np.random.uniform(
            setores['estatisticas_espaciais']['extensao']['min_y'],
            setores['estatisticas_espaciais']['extensao']['max_y'],
            n_setores
        ),
        'populacao': np.random.exponential(
            setores['estatisticas_numericas']['est_populacao']['media'],
            n_setores
        ),
        'area_km2': np.random.lognormal(
            np.log(setores['estatisticas_numericas']['area_km2']['mediana']),
            setores['estatisticas_numericas']['area_km2']['desvio_padrao'] /
            setores['estatisticas_numericas']['area_km2']['media'],
            n_setores
        ),
        'is_urban': np.random.choice(
            [0, 1],
            n_setores,
            p=[1-setores['estatisticas_numericas']['is_urban']['media'],
               setores['estatisticas_numericas']['is_urban']['media']]
        ),
        'elevacao': np.random.uniform(550, 900, n_setores),
        'is_edificacao': 0  # Flag para identificar como setor censitário
    })

    # Combinar edificações e setores
    civil_df = pd.concat([edificacoes_df, setores_df], ignore_index=True)

    return civil_df

def criar_arestas_civil(civil_df, max_dist=5000):
    """
    Cria arestas para a camada civil:
    1. Entre edificações próximas
    2. Entre edificações e setores censitários que as contêm
    """
    # Separar edificações e setores
    edificacoes = civil_df[civil_df['is_edificacao'] == 1]
    setores = civil_df[civil_df['is_edificacao'] == 0]

    # Índice para mapeamento global
    edificacoes_idx = np.where(civil_df['is_edificacao'] == 1)[0]
    setores_idx = np.where(civil_df['is_edificacao'] == 0)[0]

    # 1. Conexões entre edificações
    edges_edificacoes = []

    if len(edificacoes) > 3:
        points = edificacoes[['x', 'y']].values
        tri = Delaunay(points)

        for simplex in tri.simplices:
            for i in range(3):
                for j in range(i+1, 3):
                    local_idx_i, local_idx_j = simplex[i], simplex[j]
                    # Mapear para índice global
                    global_idx_i = edificacoes_idx[local_idx_i]
                    global_idx_j = edificacoes_idx[local_idx_j]

                    # Calcular distância
                    dist = np.sqrt(((points[local_idx_i] - points[local_idx_j])**2).sum())
                    if dist <= max_dist:
                        edges_edificacoes.append((global_idx_i, global_idx_j, dist, 'edificacao'))

    # 2. Conexões entre edificações e setores mais próximos
    edges_edif_setor = []

    for i, edificacao in enumerate(edificacoes.itertuples()):
        global_idx_edif = edificacoes_idx[i]

        # Encontrar os setores mais próximos
        for j, setor in enumerate(setores.itertuples()):
            global_idx_setor = setores_idx[j]

            # Distância entre edificação e setor
            dist = np.sqrt((edificacao.x - setor.x)**2 + (edificacao.y - setor.y)**2)

            if dist <= max_dist:
                edges_edif_setor.append((global_idx_edif, global_idx_setor, dist, 'edif_setor'))

    # Combinar todas as arestas
    edges = edges_edificacoes + edges_edif_setor

    if edges:
        edges_df = pd.DataFrame(edges, columns=['source', 'target', 'distance', 'tipo'])
        edges_df['weight'] = edges_df['distance']
        return edges_df
    else:
        return pd.DataFrame(columns=['source', 'target', 'distance', 'tipo', 'weight'])

# ===============================================
# 5. Funções de conexão inter-camadas
# ===============================================

def criar_conexoes_intercamadas(terreno_df, clima_df, civil_df, max_dist=1000):
    """Cria conexões entre nós de diferentes camadas com base na proximidade espacial"""
    # Mapear índices com deslocamento para cada camada
    offset_clima = len(terreno_df)
    offset_civil = offset_clima + len(clima_df)

    # Criar conexões terreno-clima
    edges_terreno_clima = []

    for i, terreno in enumerate(terreno_df.itertuples()):
        for j, clima in enumerate(clima_df.itertuples()):
            dist = np.sqrt((terreno.x - clima.x)**2 + (terreno.y - clima.y)**2)
            if dist <= max_dist:
                # Índice i para terreno, j+offset_clima para clima
                edges_terreno_clima.append((i, j+offset_clima, dist, 'terreno_clima'))

    # Criar conexões terreno-civil
    edges_terreno_civil = []

    for i, terreno in enumerate(terreno_df.itertuples()):
        for j, civil in enumerate(civil_df.itertuples()):
            dist = np.sqrt((terreno.x - civil.x)**2 + (terreno.y - civil.y)**2)
            if dist <= max_dist:
                # Índice i para terreno, j+offset_civil para civil
                edges_terreno_civil.append((i, j+offset_civil, dist, 'terreno_civil'))

    # Criar conexões clima-civil
    edges_clima_civil = []

    for i, clima in enumerate(clima_df.itertuples()):
        for j, civil in enumerate(civil_df.itertuples()):
            dist = np.sqrt((clima.x - civil.x)**2 + (clima.y - civil.y)**2)
            if dist <= max_dist:
                # Índice i+offset_clima para clima, j+offset_civil para civil
                edges_clima_civil.append((i+offset_clima, j+offset_civil, dist, 'clima_civil'))

    # Combinar todas as arestas inter-camadas
    edges = edges_terreno_clima + edges_terreno_civil + edges_clima_civil

    if edges:
        edges_df = pd.DataFrame(edges, columns=['source', 'target', 'distance', 'tipo'])
        edges_df['weight'] = edges_df['distance']
        return edges_df
    else:
        return pd.DataFrame(columns=['source', 'target', 'distance', 'tipo', 'weight'])

# ===============================================
# 6. Funções para PyTorch Geometric
# ===============================================

def construir_pyg_data(terreno_df, clima_df, civil_df,
                      edges_terreno, edges_clima, edges_civil, edges_inter):
    """
    Constrói o objeto Data do PyTorch Geometric a partir dos DataFrames
    """
    # Criar matriz de atributos dos nós
    # Selecionar e normalizar atributos relevantes

    # Atributos da camada de terreno
    terreno_attrs = terreno_df[['elevacao']].values
    # One-hot encoding para uso do solo
    uso_solo_categories = pd.get_dummies(terreno_df['uso_solo'])
    terreno_attrs = np.hstack([terreno_attrs, terreno_df[['hidrografia']].values, uso_solo_categories.values])

    # Atributos da camada climática
    clima_attrs = clima_df[['precipitacao', 'temperatura', 'vento', 'elevacao']].values

    # Atributos da camada civil
    # Separar edificações e setores para tratamento específico
    edificacoes = civil_df[civil_df['is_edificacao'] == 1]
    setores = civil_df[civil_df['is_edificacao'] == 0]

    # One-hot encoding para tipo de edificação
    tipo_edificacao = pd.get_dummies(edificacoes['tipo'])

    # Combinar atributos para edificações
    edificacoes_attrs = np.hstack([
        edificacoes[['elevacao', 'prioridade']].values,
        tipo_edificacao.values,
        np.ones((len(edificacoes), 1)),  # Indicador de edificação
        np.zeros((len(edificacoes), 1))  # Não é setor
    ])

    # Combinar atributos para setores
    setores_attrs = np.hstack([
        setores[['elevacao']].values,
        np.zeros((len(setores), 1)),  # Placeholder para prioridade
        np.zeros((len(setores), tipo_edificacao.shape[1])),  # Placeholder para tipo
        np.zeros((len(setores), 1)),  # Não é edificação
        np.ones((len(setores), 1))  # É setor
    ])

    # Normalização de atributos numéricos
    # Aqui usamos apenas um exemplo simples de normalização min-max
    # Em um caso real, seria necessário aplicar técnicas mais adequadas

    # Combinar todos os atributos dos nós
    civil_attrs = np.vstack([edificacoes_attrs, setores_attrs])
    all_attrs = np.vstack([terreno_attrs, clima_attrs, civil_attrs])

    # Min-max scaling para normalização
    def min_max_scale(data):
        min_vals = np.min(data, axis=0)
        max_vals = np.max(data, axis=0)
        range_vals = max_vals - min_vals
        # Evitar divisão por zero
        range_vals[range_vals == 0] = 1
        return (data - min_vals) / range_vals

    # Normalizar atributos (não aplicar a colunas categóricas/binárias)
    x = torch.tensor(all_attrs, dtype=torch.float)

    # Combinar todas as arestas
    all_edges = pd.concat([edges_terreno, edges_clima, edges_civil, edges_inter], ignore_index=True)

    # Criar edge_index
    edge_index = torch.tensor([all_edges['source'].values, all_edges['target'].values], dtype=torch.long)

    # Criar atributos das arestas
    edge_attr = torch.tensor(all_edges[['weight']].values, dtype=torch.float)

    # Tipos categóricos de nós (0: terreno, 1: clima, 2: civil)
    node_type = torch.zeros(len(terreno_df) + len(clima_df) + len(civil_df), dtype=torch.long)
    node_type[len(terreno_df):len(terreno_df)+len(clima_df)] = 1
    node_type[len(terreno_df)+len(clima_df):] = 2

    # Criar objeto Data do PyG
    data = Data(
        x=x,
        edge_index=edge_index,
        edge_attr=edge_attr,
        node_type=node_type,
        num_nodes=len(node_type)
    )

    return data

def construir_grafo_heterogeneo(terreno_df, clima_df, civil_df,
                              edges_terreno, edges_clima, edges_civil, edges_inter):
    """
    Constrói um grafo heterogêneo usando PyTorch Geometric HeteroData
    """
    # Criar objeto HeteroData
    data = HeteroData()

    # Atributos dos nós para cada tipo
    # Terreno
    data['terreno'].x = torch.tensor(terreno_df[['elevacao', 'hidrografia']].values, dtype=torch.float)
    data['terreno'].pos = torch.tensor(terreno_df[['x', 'y']].values, dtype=torch.float)

    # Clima
    data['clima'].x = torch.tensor(clima_df[['precipitacao', 'temperatura', 'vento']].values, dtype=torch.float)
    data['clima'].pos = torch.tensor(clima_df[['x', 'y']].values, dtype=torch.float)

    # Civil - separar edificações e setores
    edificacoes = civil_df[civil_df['is_edificacao'] == 1]
    setores = civil_df[civil_df['is_edificacao'] == 0]

    # Edificações
    data

import os
path = "/content/drive/MyDrive/GrafosGeoespaciais"
os.listdir(path)